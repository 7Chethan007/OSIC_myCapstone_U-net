{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":20604,"databundleVersionId":1357052,"sourceType":"competition"},{"sourceId":13905913,"sourceType":"datasetVersion","datasetId":8852072},{"sourceId":662983,"sourceType":"modelInstanceVersion","modelInstanceId":501638,"modelId":516804}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# AI-Driven Early Prediction of Pulmonary Fibrosis Using Deep Learning\n","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport lightgbm as lgb\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n# ==========================================\n# CONFIGURATION\n# ==========================================\nCONFIG = {\n    # Neural Net Config\n    \"nn_lr\": 2e-3,\n    \"nn_weight_decay\": 3e-5,\n    \"nn_batch_size\": 64,\n    \"nn_epochs\": 300,\n    \"nn_patience\": 50,\n    \n    # LightGBM Config\n    \"lgb_params\": {\n        'objective': 'quantile',\n        'metric': 'quantile',\n        'alpha': 0.5,  # Will train 3 models for q20, q50, q80\n        'boosting_type': 'gbdt',\n        'num_leaves': 31,\n        'learning_rate': 0.05,\n        'feature_fraction': 0.9,\n        'bagging_fraction': 0.8,\n        'bagging_freq': 5,\n        'verbose': -1,\n        'max_depth': 6,\n        'min_child_samples': 20,\n        'reg_alpha': 0.1,\n        'reg_lambda': 0.1\n    },\n    \"lgb_rounds\": 500,\n    \"lgb_early_stopping\": 50,\n    \n    # General\n    \"n_folds\": 5,\n    \"quantiles\": [0.2, 0.5, 0.8],\n    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n    \"data_dir\": \"../input/osic-pulmonary-fibrosis-progression\",\n    \"biomarker_path\": \"../input/feature-extraction-u-net-segmentation/master_dataset.csv\",\n    \"seed\": 42,\n    \n    # Ensemble weights (tuned during validation)\n    \"ensemble_nn_weight\": 0.5,  # Will be optimized\n    \"ensemble_lgb_weight\": 0.5\n}\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(CONFIG['seed'])\n\n# ==========================================\n# DATA PREPROCESSING (Same as before)\n# ==========================================\ndef preprocess_data_no_leakage(config):\n    \"\"\"Zero look-ahead bias preprocessing\"\"\"\n    clinical_df = pd.read_csv(f\"{config['data_dir']}/train.csv\")\n    biomarkers_df = pd.read_csv(config['biomarker_path'])\n    \n    image_features = [\n        'lung_vol_ml', 'hu_mean', 'hu_std', 'hu_skew', 'hu_kurt',\n        'glcm_contrast', 'glcm_homogeneity', 'glcm_energy', 'glcm_correlation'\n    ]\n    \n    cols_to_keep = ['Patient'] + [c for c in image_features if c in biomarkers_df.columns]\n    biomarkers_clean = biomarkers_df[cols_to_keep].drop_duplicates(subset=['Patient'])\n    \n    train = clinical_df.merge(biomarkers_clean, on='Patient', how='inner')\n    train['Weeks'] = train['Weeks'].astype(int)\n    train.sort_values(['Patient', 'Weeks'], inplace=True)\n    \n    baseline = train.groupby('Patient').first().reset_index()\n    baseline = baseline[['Patient', 'FVC', 'Percent']].rename(\n        columns={'FVC': 'Base_FVC', 'Percent': 'Base_Percent'}\n    )\n    train = train.merge(baseline, on='Patient', how='left')\n    \n    base_weeks = train.groupby('Patient')['Weeks'].min().reset_index().rename(\n        columns={'Weeks': 'Base_Week'}\n    )\n    train = train.merge(base_weeks, on='Patient', how='left')\n    train['Relative_Weeks'] = train['Weeks'] - train['Base_Week']\n    \n    train['FVC_Ratio'] = train['FVC'] / train['Base_FVC']\n    \n    train['Weeks_squared'] = train['Relative_Weeks'] ** 2\n    train['Weeks_cubed'] = train['Relative_Weeks'] ** 3\n    train['Week_Decay'] = 1 - np.exp(-train['Relative_Weeks'] / 52.0)\n    \n    train['FVC_Week_Interaction'] = train['Base_FVC'] * train['Relative_Weeks']\n    train['Age_Week_Interaction'] = train['Age'] * train['Relative_Weeks']\n    train['Percent_Week_Interaction'] = train['Base_Percent'] * train['Relative_Weeks']\n    \n    available_img_feats = [c for c in image_features if c in train.columns]\n    \n    if 'lung_vol_ml' in train.columns:\n        train['LungVol_FVC_Ratio'] = train['lung_vol_ml'] / (train['Base_FVC'] + 1e-6)\n        train['LungVol_Week_Interaction'] = train['lung_vol_ml'] * train['Relative_Weeks']\n    \n    if 'hu_mean' in train.columns:\n        train['HU_Week_Interaction'] = train['hu_mean'] * train['Relative_Weeks']\n    \n    train['Base_FVC_Raw'] = train['Base_FVC']\n    \n    train['Sex'] = train['Sex'].apply(lambda x: 1 if x == 'Male' else 0)\n    train['Smk_Ex'] = train['SmokingStatus'].apply(lambda x: 1 if x == 'Ex-smoker' else 0)\n    train['Smk_Cur'] = train['SmokingStatus'].apply(lambda x: 1 if x == 'Currently smokes' else 0)\n    \n    num_cols = ['Age', 'Base_Percent', 'Relative_Weeks', 'Weeks_squared', \n                'Weeks_cubed', 'Week_Decay'] + available_img_feats\n    \n    interaction_cols = ['FVC_Week_Interaction', 'Age_Week_Interaction', \n                       'Percent_Week_Interaction']\n    \n    if 'LungVol_FVC_Ratio' in train.columns:\n        interaction_cols.extend(['LungVol_FVC_Ratio', 'LungVol_Week_Interaction'])\n    if 'HU_Week_Interaction' in train.columns:\n        interaction_cols.append('HU_Week_Interaction')\n    \n    feature_cols = num_cols + interaction_cols + ['Sex', 'Smk_Ex', 'Smk_Cur']\n    \n    print(f\"âœ… Preprocessing: {train.shape[0]} samples, {len(feature_cols)} features\")\n    \n    return train, feature_cols, num_cols + interaction_cols\n\n# ==========================================\n# NEURAL NETWORK MODEL\n# ==========================================\nclass EnhancedQuantileMLP(nn.Module):\n    def __init__(self, input_dim, quantiles, dropout=0.25):\n        super().__init__()\n        h1, h2, h3 = 256, 128, 64\n        \n        self.net = nn.Sequential(\n            nn.Linear(input_dim, h1), nn.BatchNorm1d(h1), nn.LeakyReLU(0.1), nn.Dropout(dropout),\n            nn.Linear(h1, h2), nn.BatchNorm1d(h2), nn.LeakyReLU(0.1), nn.Dropout(dropout),\n            nn.Linear(h2, h3), nn.BatchNorm1d(h3), nn.LeakyReLU(0.1), nn.Dropout(dropout),\n            nn.Linear(h3, len(quantiles))\n        )\n    \n    def forward(self, x):\n        return self.net(x)\n\ndef quantile_loss(preds, target, quantiles):\n    losses = []\n    for i, q in enumerate(quantiles):\n        errors = target - preds[:, i].unsqueeze(1)\n        loss = torch.max((q-1) * errors, q * errors)\n        losses.append(loss)\n    return torch.mean(torch.sum(torch.cat(losses, dim=1), dim=1))\n\n# ==========================================\n# METRICS\n# ==========================================\ndef calculate_metrics(y_true_fvc, q_preds_ratio, baseline_fvc):\n    q20 = q_preds_ratio[:, 0] * baseline_fvc\n    q50 = q_preds_ratio[:, 1] * baseline_fvc\n    q80 = q_preds_ratio[:, 2] * baseline_fvc\n    \n    sigma = q80 - q20\n    sigma_clipped = np.maximum(sigma, 70)\n    \n    delta = np.minimum(np.abs(y_true_fvc - q50), 1000)\n    lll = - (np.sqrt(2) * delta / sigma_clipped) - np.log(np.sqrt(2) * sigma_clipped)\n    \n    mse = mean_squared_error(y_true_fvc, q50)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(y_true_fvc, q50)\n    r2 = r2_score(y_true_fvc, q50)\n    \n    return {'lll': np.mean(lll), 'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2}\n\n# ==========================================\n# NEURAL NETWORK TRAINING\n# ==========================================\ndef train_nn_fold(train_data, val_data, features, scale_cols, fold):\n    \"\"\"Train single fold of neural network\"\"\"\n    feature_scaler = StandardScaler()\n    ratio_scaler = StandardScaler()\n    \n    train_data_scaled = train_data.copy()\n    val_data_scaled = val_data.copy()\n    \n    train_data_scaled[scale_cols] = feature_scaler.fit_transform(train_data[scale_cols])\n    train_data_scaled['FVC_Ratio_Scaled'] = ratio_scaler.fit_transform(train_data[['FVC_Ratio']])\n    \n    val_data_scaled[scale_cols] = feature_scaler.transform(val_data[scale_cols])\n    val_data_scaled['FVC_Ratio_Scaled'] = ratio_scaler.transform(val_data[['FVC_Ratio']])\n    \n    X_train = torch.tensor(train_data_scaled[features].values, dtype=torch.float32).to(CONFIG['device'])\n    y_train = torch.tensor(train_data_scaled['FVC_Ratio_Scaled'].values, dtype=torch.float32).unsqueeze(1).to(CONFIG['device'])\n    \n    X_val = torch.tensor(val_data_scaled[features].values, dtype=torch.float32).to(CONFIG['device'])\n    \n    model = EnhancedQuantileMLP(len(features), CONFIG['quantiles']).to(CONFIG['device'])\n    optimizer = optim.AdamW(model.parameters(), lr=CONFIG['nn_lr'], weight_decay=CONFIG['nn_weight_decay'])\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=20, verbose=False)\n    \n    best_lll = -float('inf')\n    best_preds = None\n    patience = 0\n    \n    for epoch in range(CONFIG['nn_epochs']):\n        model.train()\n        optimizer.zero_grad()\n        preds = model(X_train)\n        loss = quantile_loss(preds, y_train, CONFIG['quantiles'])\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        \n        model.eval()\n        with torch.no_grad():\n            val_preds_scaled = model(X_val)\n            val_preds_ratio = ratio_scaler.inverse_transform(val_preds_scaled.cpu().numpy())\n            metrics = calculate_metrics(val_data['FVC'].values, val_preds_ratio, val_data['Base_FVC_Raw'].values)\n            \n        scheduler.step(metrics['lll'])\n        \n        if metrics['lll'] > best_lll:\n            best_lll = metrics['lll']\n            best_preds = val_preds_ratio.copy()\n            patience = 0\n            torch.save({'model': model.state_dict(), 'feature_scaler': feature_scaler, \n                       'ratio_scaler': ratio_scaler}, f\"nn_fold{fold}.pth\")\n        else:\n            patience += 1\n            \n        if patience >= CONFIG['nn_patience']:\n            break\n    \n    return best_preds, best_lll\n\n# ==========================================\n# LIGHTGBM TRAINING\n# ==========================================\ndef train_lgb_fold(train_data, val_data, features, fold):\n    \"\"\"Train LightGBM for all 3 quantiles\"\"\"\n    lgb_preds = []\n    \n    for q_idx, quantile in enumerate(CONFIG['quantiles']):\n        params = CONFIG['lgb_params'].copy()\n        params['alpha'] = quantile\n        \n        dtrain = lgb.Dataset(train_data[features], label=train_data['FVC_Ratio'])\n        dval = lgb.Dataset(val_data[features], label=val_data['FVC_Ratio'], reference=dtrain)\n        \n        model = lgb.train(\n            params,\n            dtrain,\n            num_boost_round=CONFIG['lgb_rounds'],\n            valid_sets=[dval],\n            callbacks=[lgb.early_stopping(CONFIG['lgb_early_stopping'], verbose=False)]\n        )\n        \n        preds = model.predict(val_data[features])\n        lgb_preds.append(preds)\n        \n        model.save_model(f\"lgb_q{int(quantile*100)}_fold{fold}.txt\")\n    \n    lgb_preds = np.column_stack(lgb_preds)\n    lll = calculate_metrics(val_data['FVC'].values, lgb_preds, val_data['Base_FVC_Raw'].values)['lll']\n    \n    return lgb_preds, lll\n\n# ==========================================\n# ENSEMBLE TRAINING\n# ==========================================\ndef train_ensemble():\n    df, feature_cols, scale_cols = preprocess_data_no_leakage(CONFIG)\n    patients = df['Patient'].unique()\n    kf = KFold(n_splits=CONFIG['n_folds'], shuffle=True, random_state=CONFIG['seed'])\n    \n    oof_nn_preds = []\n    oof_lgb_preds = []\n    oof_trues = []\n    oof_baselines = []\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"ğŸš€ PROFESSIONAL ENSEMBLE: NEURAL NET + LIGHTGBM\")\n    print(f\"{'='*80}\\n\")\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(patients)):\n        print(f\"{'='*80}\")\n        print(f\"FOLD {fold+1}/5\")\n        print(f\"{'='*80}\")\n        \n        train_p, val_p = patients[train_idx], patients[val_idx]\n        train_data = df[df['Patient'].isin(train_p)].copy()\n        val_data = df[df['Patient'].isin(val_p)].copy()\n        \n        print(f\"\\nğŸ§  Training Neural Network...\")\n        nn_preds, nn_lll = train_nn_fold(train_data, val_data, feature_cols, scale_cols, fold+1)\n        print(f\"   NN  - LLL: {nn_lll:.4f}\")\n        \n        print(f\"\\nğŸŒ³ Training LightGBM...\")\n        lgb_preds, lgb_lll = train_lgb_fold(train_data, val_data, feature_cols, fold+1)\n        print(f\"   LGB - LLL: {lgb_lll:.4f}\")\n        \n        # Optimize ensemble weights for this fold\n        best_lll = -float('inf')\n        best_weight = 0.5\n        \n        for w_nn in np.arange(0.3, 0.8, 0.05):\n            w_lgb = 1 - w_nn\n            ensemble_preds = w_nn * nn_preds + w_lgb * lgb_preds\n            lll = calculate_metrics(val_data['FVC'].values, ensemble_preds, val_data['Base_FVC_Raw'].values)['lll']\n            \n            if lll > best_lll:\n                best_lll = lll\n                best_weight = w_nn\n        \n        final_preds = best_weight * nn_preds + (1 - best_weight) * lgb_preds\n        metrics = calculate_metrics(val_data['FVC'].values, final_preds, val_data['Base_FVC_Raw'].values)\n        \n        print(f\"\\nğŸ”€ Ensemble (NN:{best_weight:.2f}, LGB:{1-best_weight:.2f})\")\n        print(f\"   LLL:  {metrics['lll']:.4f}  {'âœ…' if metrics['lll'] > -6.64 else 'âŒ'}\")\n        print(f\"   RÂ²:   {metrics['r2']:.4f}\")\n        print(f\"   RMSE: {metrics['rmse']:.2f} mL\")\n        print(f\"   MAE:  {metrics['mae']:.2f} mL\\n\")\n        \n        oof_nn_preds.append(nn_preds)\n        oof_lgb_preds.append(lgb_preds)\n        oof_trues.extend(val_data['FVC'].values)\n        oof_baselines.extend(val_data['Base_FVC_Raw'].values)\n    \n    # Final ensemble optimization\n    oof_nn_preds = np.vstack(oof_nn_preds)\n    oof_lgb_preds = np.vstack(oof_lgb_preds)\n    oof_trues = np.array(oof_trues)\n    oof_baselines = np.array(oof_baselines)\n    \n    print(f\"{'='*80}\")\n    print(f\"ğŸ¯ FINAL ENSEMBLE OPTIMIZATION\")\n    print(f\"{'='*80}\\n\")\n    \n    best_final_lll = -float('inf')\n    best_final_weight = 0.5\n    \n    for w_nn in np.arange(0.3, 0.8, 0.05):\n        w_lgb = 1 - w_nn\n        ensemble_preds = w_nn * oof_nn_preds + w_lgb * oof_lgb_preds\n        metrics = calculate_metrics(oof_trues, ensemble_preds, oof_baselines)\n        \n        print(f\"NN:{w_nn:.2f} LGB:{w_lgb:.2f} â†’ LLL: {metrics['lll']:.4f}, RMSE: {metrics['rmse']:.2f}\")\n        \n        if metrics['lll'] > best_final_lll:\n            best_final_lll = metrics['lll']\n            best_final_weight = w_nn\n    \n    final_ensemble_preds = best_final_weight * oof_nn_preds + (1 - best_final_weight) * oof_lgb_preds\n    final_metrics = calculate_metrics(oof_trues, final_ensemble_preds, oof_baselines)\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"ğŸ† FINAL ENSEMBLE RESULTS\")\n    print(f\"{'='*80}\")\n    print(f\"Optimal Weights: NN={best_final_weight:.2f}, LGB={1-best_final_weight:.2f}\\n\")\n    print(f\"LLL:  {final_metrics['lll']:.4f}   {'âœ…' if final_metrics['lll'] > -6.64 else 'âŒ'} (Target > -6.64)\")\n    print(f\"RÂ²:   {final_metrics['r2']:.4f}   {'âœ…' if final_metrics['r2'] > 0.88 else 'âŒ'} (Target > 0.88)\")\n    print(f\"RMSE: {final_metrics['rmse']:.2f} mL\")\n    print(f\"MAE:  {final_metrics['mae']:.2f} mL\")\n    print(f\"MSE:  {final_metrics['mse']:.2f}\")\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"ğŸ’¾ MODELS SAVED\")\n    print(f\"{'='*80}\")\n    print(f\"Neural Networks: nn_fold1.pth ... nn_fold5.pth\")\n    print(f\"LightGBM: lgb_q20_fold1.txt ... lgb_q80_fold5.txt\")\n    print(f\"{'='*80}\\n\")\n    \n    return final_metrics\n\nif __name__ == \"__main__\":\n    print(\"=\"*80)\n    print(\"ğŸ›¡ï¸ LEAK-FREE PROFESSIONAL ENSEMBLE\")\n    print(\"=\"*80)\n    print(\"âœ… Neural Network: Deep MLP with quantile regression\")\n    print(\"âœ… LightGBM: Gradient boosting for non-linear patterns\")\n    print(\"âœ… Dynamic weight optimization per fold\")\n    print(\"âœ… Zero data leakage guarantee\")\n    print(\"=\"*80 + \"\\n\")\n    \n    results = train_ensemble()\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"ğŸ¯ TARGET STATUS\")\n    print(\"=\"*80)\n    \n    if results['rmse'] < 170 and results['lll'] > -6.64:\n        print(\"ğŸ‰ SUCCESS! Both targets met!\")\n    elif results['rmse'] < 180:\n        print(\"âš ï¸ Very close! Consider multi-seed ensemble.\")\n    else:\n        print(\"ğŸ“Š Honest baseline established. Further improvements needed.\")\n    \n    print(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T06:13:44.350092Z","iopub.execute_input":"2025-11-29T06:13:44.350857Z","iopub.status.idle":"2025-11-29T06:13:51.176540Z","shell.execute_reply.started":"2025-11-29T06:13:44.350826Z","shell.execute_reply":"2025-11-29T06:13:51.175325Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nğŸ›¡ï¸ LEAK-FREE PROFESSIONAL ENSEMBLE\n================================================================================\nâœ… Neural Network: Deep MLP with quantile regression\nâœ… LightGBM: Gradient boosting for non-linear patterns\nâœ… Dynamic weight optimization per fold\nâœ… Zero data leakage guarantee\n================================================================================\n\nâœ… Preprocessing: 1549 samples, 24 features\n\n================================================================================\nğŸš€ PROFESSIONAL ENSEMBLE: NEURAL NET + LIGHTGBM\n================================================================================\n\n================================================================================\nFOLD 1/5\n================================================================================\n\nğŸ§  Training Neural Network...\n   NN  - LLL: -6.4964\n\nğŸŒ³ Training LightGBM...\n   LGB - LLL: -6.4175\n\nğŸ”€ Ensemble (NN:0.30, LGB:0.70)\n   LLL:  -6.4121  âœ…\n   RÂ²:   0.9388\n   RMSE: 194.48 mL\n   MAE:  127.17 mL\n\n================================================================================\nFOLD 2/5\n================================================================================\n\nğŸ§  Training Neural Network...\n   NN  - LLL: -6.7391\n\nğŸŒ³ Training LightGBM...\n   LGB - LLL: -6.6054\n\nğŸ”€ Ensemble (NN:0.30, LGB:0.70)\n   LLL:  -6.6098  âœ…\n   RÂ²:   0.9309\n   RMSE: 234.28 mL\n   MAE:  155.60 mL\n\n================================================================================\nFOLD 3/5\n================================================================================\n\nğŸ§  Training Neural Network...\n   NN  - LLL: -6.7747\n\nğŸŒ³ Training LightGBM...\n   LGB - LLL: -6.5784\n\nğŸ”€ Ensemble (NN:0.30, LGB:0.70)\n   LLL:  -6.6083  âœ…\n   RÂ²:   0.9121\n   RMSE: 222.96 mL\n   MAE:  151.09 mL\n\n================================================================================\nFOLD 4/5\n================================================================================\n\nğŸ§  Training Neural Network...\n   NN  - LLL: -6.6895\n\nğŸŒ³ Training LightGBM...\n   LGB - LLL: -6.4814\n\nğŸ”€ Ensemble (NN:0.30, LGB:0.70)\n   LLL:  -6.5063  âœ…\n   RÂ²:   0.9215\n   RMSE: 214.47 mL\n   MAE:  144.72 mL\n\n================================================================================\nFOLD 5/5\n================================================================================\n\nğŸ§  Training Neural Network...\n   NN  - LLL: -6.5673\n\nğŸŒ³ Training LightGBM...\n   LGB - LLL: -6.5676\n\nğŸ”€ Ensemble (NN:0.50, LGB:0.50)\n   LLL:  -6.5231  âœ…\n   RÂ²:   0.9559\n   RMSE: 190.41 mL\n   MAE:  129.51 mL\n\n================================================================================\nğŸ¯ FINAL ENSEMBLE OPTIMIZATION\n================================================================================\n\nNN:0.30 LGB:0.70 â†’ LLL: -6.5331, RMSE: 211.74\nNN:0.35 LGB:0.65 â†’ LLL: -6.5362, RMSE: 211.86\nNN:0.40 LGB:0.60 â†’ LLL: -6.5398, RMSE: 212.01\nNN:0.45 LGB:0.55 â†’ LLL: -6.5440, RMSE: 212.21\nNN:0.50 LGB:0.50 â†’ LLL: -6.5489, RMSE: 212.44\nNN:0.55 LGB:0.45 â†’ LLL: -6.5548, RMSE: 212.72\nNN:0.60 LGB:0.40 â†’ LLL: -6.5615, RMSE: 213.04\nNN:0.65 LGB:0.35 â†’ LLL: -6.5689, RMSE: 213.39\nNN:0.70 LGB:0.30 â†’ LLL: -6.5773, RMSE: 213.79\nNN:0.75 LGB:0.25 â†’ LLL: -6.5866, RMSE: 214.23\n\n================================================================================\nğŸ† FINAL ENSEMBLE RESULTS\n================================================================================\nOptimal Weights: NN=0.30, LGB=0.70\n\nLLL:  -6.5331   âœ… (Target > -6.64)\nRÂ²:   0.9353   âœ… (Target > 0.88)\nRMSE: 211.74 mL\nMAE:  141.26 mL\nMSE:  44834.98\n\n================================================================================\nğŸ’¾ MODELS SAVED\n================================================================================\nNeural Networks: nn_fold1.pth ... nn_fold5.pth\nLightGBM: lgb_q20_fold1.txt ... lgb_q80_fold5.txt\n================================================================================\n\n\n================================================================================\nğŸ¯ TARGET STATUS\n================================================================================\nğŸ“Š Honest baseline established. Further improvements needed.\n================================================================================\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
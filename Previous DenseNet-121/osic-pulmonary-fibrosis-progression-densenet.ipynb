{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":20604,"databundleVersionId":1357052,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Oct 27","metadata":{}},{"cell_type":"markdown","source":"ðŸ” What's Missing From Our Original Plan (OSIC-Only)\nLooking back at our discussion, here's what you haven't done yet that we agreed was feasible:\n* .Proper data splitting - You only did simple train/val split. We discussed 5-fold cross-validation OR holding out a proper test set (20%) that you evaluate ONCE at the end. Right now you have no true held-out evaluation.\\n\n* .Clinical baseline comparison - You need to train a clinical-only model (Age, Sex, Smoking, baseline FVC) and show that adding CT scans improves predictions. This proves CT adds value beyond simple demographics.\\n\n* .Comprehensive evaluation metrics - You only reported RÂ², MAE, LLL. Missing: classification metrics (accuracy, sensitivity, specificity for fast/moderate/slow progressors), confusion matrix, calibration plots, and prediction scatter plots.\\n\n* .Interpretability/Visualization - No attention maps showing which lung regions drive predictions, no feature importance analysis, no training curves saved as figures.\\n\n* .Error analysis - Which patients did the model fail on? Are there patterns in failures? This builds trust and shows limitations honestly.\\n\nDo these 5 things with your current code and OSIC dataset, and you'll have a complete, publishable story showing that baseline CT scans can predict fibrosis progression! ðŸŽ¯\n\n","metadata":{}},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"# ============================================================================\n#\n#  OSIC Paper Review Script - Reproducing the v1.txt Result (RÂ²=0.46)\n#  - FIX: Corrected IndexError on targets[p][2] -> targets[p][1]\n#\n# ============================================================================\n\nimport os\nimport cv2\nimport pydicom\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport random\nfrom tqdm import tqdm \nfrom sklearn.model_selection import train_test_split # Use single split\nfrom sklearn.metrics import (r2_score, mean_absolute_error, mean_squared_error)\nimport xgboost as xgb\nfrom scipy import stats\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau \nfrom pathlib import Path\nimport albumentations as albu\nfrom albumentations.pytorch import ToTensorV2\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n# ============================================================================\n# 1. CONFIGURATION (Matching v1.txt)\n# ============================================================================\n\nclass Config:\n    SEED = 42\n    DATA_DIR = Path(\"/kaggle/input/osic-pulmonary-fibrosis-progression\")\n    TRAIN_DIR = DATA_DIR / \"train\" \n    OUTPUT_DIR = Path(\"./\")\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # --- Split from v1.txt ---\n    TEST_SIZE = 0.15 \n    \n    IMG_SIZE = 384\n    BATCH_SIZE = 8\n    NUM_WORKERS = 2\n    \n    EPOCHS = 50 # v1.txt ran for 50\n    LR = 1e-4 # v1.txt LR\n    WEIGHT_DECAY = 1e-4\n    EARLY_STOP_PATIENCE = 10 # v1.txt used 10\n    SCHEDULER_PATIENCE = 5 # v1.txt used 5\n    \n    # Dropout from v1.txt\n    FUSION_DROPOUT = 0.2 \n\nconfig = Config()\n\ndef seed_everything(seed=Config.SEED):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nseed_everything()\n\nprint(f\"ðŸš€ REPRODUCING v1.txt (RÂ²={0.46}, MAE={3.27})\")\nprint(\"=\" * 60)\nprint(f\"ðŸ“± Device: {config.DEVICE}\")\nprint(f\"ðŸ“ Image Size: {config.IMG_SIZE}x{config.IMG_SIZE}\")\n\n\n# ============================================================================\n# 2. DATA PREPARATION (from v1.txt)\n# ============================================================================\n\ndef get_optimized_tab_features(df_row):\n    vector = []\n    age = df_row['Age']\n    vector.extend([(age - 50) / 30, age / 100])\n    vector.append(1.0 if df_row['Sex'] == 'Male' else 0.0)\n    \n    smoking_status = df_row['SmokingStatus']\n    if smoking_status == 'Never smoked': vector.extend([1, 0, 0])\n    elif smoking_status == 'Ex-smoker': vector.extend([0, 1, 0])\n    elif smoking_status == 'Currently smokes': vector.extend([0, 0, 1])\n    else: vector.extend([0, 0, 0])\n    \n    if 'FVC' in df_row:\n        fvc = df_row['FVC']\n        vector.extend([fvc / 3000, (fvc - 2500) / 1000])\n    \n    if 'FVC' in df_row and 'Age' in df_row:\n        fvc, age, sex = df_row['FVC'], df_row['Age'], df_row['Sex']\n        if sex == 'Male': pp_fvc = fvc / (27.63 - 0.112 * age) if age > 0 else 0.8\n        else: pp_fvc = fvc / (21.78 - 0.101 * age) if age > 0 else 0.8\n        vector.append(min(pp_fvc, 2.0))\n    \n    expected_dim = 9 \n    while len(vector) < expected_dim:\n        vector.append(0.0) \n\n    return np.array(vector[:expected_dim]) \n\ndef get_progression_class(slope):\n    \"\"\"Categorizes slope for stratification.\"\"\"\n    if slope > -5: return 0  # Slow\n    elif slope > -15: return 1  # Moderate\n    else: return 2  # Fast\n\ndef prepare_data():\n    print(\"Loading data and calculating targets...\")\n    df = pd.read_csv(config.DATA_DIR / 'train.csv')\n    baseline_df = df.loc[df.groupby('Patient')['Weeks'].idxmin()]\n    \n    targets = {}\n    tabular_dim = 0 \n    \n    for patient in tqdm(df['Patient'].unique()):\n        sub = df[df['Patient'] == patient].copy().sort_values('Weeks')\n        fvc, weeks = sub['FVC'].values, sub['Weeks'].values\n        \n        if len(weeks) >= 2:\n            try:\n                slopes = []\n                for i in range(len(weeks)):\n                    for j in range(i + 1, len(weeks)):\n                        if weeks[j] != weeks[i]:\n                            slopes.append((fvc[j] - fvc[i]) / (weeks[j] - weeks[i]))\n                slope = np.median(slopes) if slopes else 0.0\n            except Exception:\n                slope = 0.0\n        else:\n            slope = 0.0\n            \n        tabular_features = get_optimized_tab_features(sub.iloc[0])\n        if tabular_dim == 0:\n            tabular_dim = len(tabular_features)\n            \n        # Store as 2-element tuple: (slope, features)\n        targets[patient] = (slope, tabular_features)\n\n    # Create bins for stratification (4 bins)\n    slopes = [targets[p][0] for p in baseline_df['Patient'].values if p in targets]\n    # Ensure baseline_df only contains patients we have targets for\n    baseline_df = baseline_df[baseline_df['Patient'].isin(targets.keys())]\n    baseline_df['decay_bins'] = pd.cut(slopes, bins=4, labels=False)\n    \n    print(f\"Processed {len(targets)} patients.\")\n    print(f\"Detected Tabular Dimension: {tabular_dim}\")\n    \n    return df, targets, baseline_df, tabular_dim\n\n# ============================================================================\n# 3. DATASET & AUGMENTATIONS (from v1.txt)\n# ============================================================================\n\nclass OptimizedAugmentation:\n    def __init__(self, augment=True):\n        if augment:\n            self.transform = albu.Compose([\n                albu.Rotate(limit=10, p=0.5),\n                albu.HorizontalFlip(p=0.4),\n                albu.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=10, p=0.6),\n                albu.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.4),\n                albu.GaussNoise(var_limit=(5.0, 20.0), p=0.3),\n                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ToTensorV2()\n            ])\n        else:\n            self.transform = albu.Compose([\n                albu.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ToTensorV2()\n            ])\n    \n    def __call__(self, image):\n        return self.transform(image=image)['image']\n\nclass OptimizedOSICDataset(Dataset):\n    def __init__(self, patients, targets_dict, data_dir, split='train'):\n        self.patients = [p for p in patients if p not in ['ID00011637202177653955184', 'ID00052637202186188008618']]\n        self.targets_dict = targets_dict\n        self.data_dir = Path(data_dir)\n        self.split = split\n        self.augmentor = OptimizedAugmentation(augment=(split=='train'))\n        \n        self.patient_images = {}\n        for patient in self.patients:\n            patient_dir = self.data_dir / patient\n            if patient_dir.exists():\n                self.patient_images[patient] = [f for f in patient_dir.iterdir() if f.suffix.lower() == '.dcm']\n        \n        self.valid_patients = [p for p in self.patients if p in self.patient_images and self.patient_images[p]]\n        \n        print(f\"Dataset split '{split}': {len(self.valid_patients)} valid patients with images.\")\n    \n    def __len__(self):\n        return len(self.valid_patients) * 8 if self.split == 'train' else len(self.valid_patients)\n    \n    def __getitem__(self, idx):\n        patient_idx = idx % len(self.valid_patients) if self.split == 'train' else idx\n        patient = self.valid_patients[patient_idx]\n        \n        available_images = self.patient_images[patient]\n        selected_image = random.choice(available_images)\n        \n        img = self.load_dicom(selected_image)\n        img_tensor = self.augmentor(img)\n        \n        slope, tab_features = self.targets_dict[patient]\n        \n        return {\n            \"image\": img_tensor,\n            \"tabular\": torch.tensor(tab_features, dtype=torch.float32),\n            \"slope_target\": torch.tensor(slope, dtype=torch.float32),\n        }\n    \n    def load_dicom(self, path):\n        try:\n            dcm = pydicom.dcmread(str(path))\n            img = dcm.pixel_array.astype(np.float32)\n            \n            img = cv2.resize(img, (config.IMG_SIZE, config.IMG_SIZE))\n            \n            img_min, img_max = img.min(), img.max()\n            if img_max > img_min:\n                img = (img - img_min) / (img_max - img_min) * 255\n            else:\n                img = np.zeros_like(img)\n            \n            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n            img = clahe.apply(img.astype(np.uint8))\n            img = np.stack([img, img, img], axis=2).astype(np.uint8)\n            return img\n        except Exception as e:\n            print(f\"Error loading {path}: {e}\")\n            return np.zeros((config.IMG_SIZE, config.IMG_SIZE, 3), dtype=np.uint8)\n\n# ============================================================================\n# 4. MODEL (from v1.txt)\n# ============================================================================\n\nclass OptimizedDenseNetModel(nn.Module):\n    def __init__(self, tabular_dim):\n        super(OptimizedDenseNetModel, self).__init__()\n        densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n        self.features = densenet.features\n        \n        # --- Partial freezing as in v1.txt ---\n        print(\"Applying partial freezing (like v1.txt): only first 100 layers frozen.\")\n        for i, param in enumerate(self.features.parameters()):\n            param.requires_grad = i > 100 \n        \n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        \n        self.tabular_processor = nn.Sequential(\n            nn.Linear(tabular_dim, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Dropout(0.2), \n            nn.Linear(128, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n        )\n        \n        self.fusion_layer = nn.Sequential(\n            nn.Linear(1024 + 256, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            # --- Dropout from v1.txt ---\n            nn.Dropout(config.FUSION_DROPOUT), \n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n        )\n        \n        self.mean_head = nn.Sequential(\n            nn.Linear(256, 128), nn.ReLU(),\n            nn.Linear(128, 64), nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n        self.log_var_head = nn.Sequential(\n            nn.Linear(256, 32), nn.ReLU(),\n            nn.Linear(32, 1),\n            nn.Tanh()\n        )\n        \n        self._initialize_weights()\n    \n    def _initialize_weights(self):\n        for m in [self.tabular_processor, self.fusion_layer, self.mean_head, self.log_var_head]:\n            for layer in m:\n                if isinstance(layer, nn.Linear):\n                    nn.init.normal_(layer.weight, mean=0.0, std=0.01)\n                    if layer.bias is not None:\n                        nn.init.constant_(layer.bias, 0.0)\n    \n    def forward(self, images, tabular):\n        img_features = self.features(images) \n        img_features = self.global_pool(img_features).view(images.size(0), -1)\n        tab_features = self.tabular_processor(tabular)\n        combined_features = torch.cat([img_features, tab_features], dim=1)\n        fused_features = self.fusion_layer(combined_features)\n        \n        mean_pred = self.mean_head(fused_features).squeeze(-1)\n        log_var = self.log_var_head(fused_features).squeeze(-1)\n        \n        return mean_pred, log_var\n\n# ============================================================================\n# 5. LOSS & METRICS (from v1.txt)\n# ============================================================================\n\nclass RegressionLoss(nn.Module):\n    \"\"\"Laplace NLL loss from v1.txt.\"\"\"\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, mean_pred, log_var, targets):\n        # --- This is the v1.txt loss logic ---\n        var = torch.exp(log_var)\n        mse_loss_term = 0.5 * (mean_pred - targets) ** 2 / var\n        unc_loss_term = 0.5 * log_var\n        \n        loss = mse_loss_term + unc_loss_term\n        \n        loss = torch.where(torch.isinf(loss) | torch.isnan(loss), torch.zeros_like(loss), loss)\n        return loss.mean()\n\n\ndef calculate_laplace_lll(actual, predicted, sigma):\n    sigma = np.maximum(sigma, 1e-6)\n    delta = np.abs(actual - predicted)\n    lll = -np.sqrt(2) * delta / sigma - np.log(sigma * np.sqrt(2))\n    finite_lll = lll[np.isfinite(lll)]\n    return np.mean(finite_lll) if len(finite_lll) > 0 else -np.inf\n\n\ndef calculate_all_metrics(results):\n    metrics = {}\n    slope_true = results['slope_targets']\n    slope_pred = results['slope_preds']\n\n    finite_mask = np.isfinite(slope_pred) & np.isfinite(slope_true)\n    if np.sum(finite_mask) < 2:\n         return {'R2': 0.0, 'MAE': np.inf, 'MSE': np.inf, 'RMSE': np.inf, 'LaplaceLL': -np.inf}\n        \n    slope_true_f = slope_true[finite_mask]\n    slope_pred_f = slope_pred[finite_mask]\n\n    metrics['R2'] = r2_score(slope_true_f, slope_pred_f)\n    metrics['MAE'] = mean_absolute_error(slope_true_f, slope_pred_f)\n    metrics['MSE'] = mean_squared_error(slope_true_f, slope_pred_f)\n    metrics['RMSE'] = np.sqrt(metrics['MSE'])\n    \n    log_var_preds_f = results['log_var_preds'][finite_mask]\n    sigma_pred_f = np.exp(log_var_preds_f / 2)\n    metrics['LaplaceLL'] = calculate_laplace_lll(slope_true_f, slope_pred_f, sigma_pred_f)\n        \n    return metrics\n\n# ============================================================================\n# 6. TRAINING & VALIDATION LOOPS\n# ============================================================================\n\ndef train_one_epoch(model, loader, optimizer, criterion, scaler):\n    model.train()\n    total_loss = 0\n    batches_processed = 0\n    pbar = tqdm(loader, desc=\"Training\", leave=False)\n\n    for batch in pbar:\n        optimizer.zero_grad(set_to_none=True)\n\n        with autocast(enabled=True):\n            mean_pred, log_var = model(\n                batch['image'].to(config.DEVICE),\n                batch['tabular'].to(config.DEVICE)\n            )\n            loss = criterion(\n                mean_pred, log_var,\n                batch['slope_target'].to(config.DEVICE)\n            )\n\n        if torch.isnan(loss) or torch.isinf(loss):\n            print(f\"Warning: NaN or Inf loss detected in training batch. Skipping batch.\")\n            continue\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        total_loss += loss.item()\n        batches_processed += 1\n        pbar.set_postfix(loss=loss.item())\n\n    return total_loss / batches_processed if batches_processed > 0 else 0.0\n\n\ndef validate_one_epoch(model, loader):\n    model.eval()\n    results = {'slope_preds': [], 'log_var_preds': [], 'slope_targets': []}\n\n    with torch.no_grad():\n        for batch in tqdm(loader, desc=\"Validating\", leave=False):\n            with autocast(enabled=True):\n                mean_pred, log_var = model(\n                    batch['image'].to(config.DEVICE),\n                    batch['tabular'].to(config.DEVICE)\n                )\n\n            if mean_pred.ndim == 0:\n                results['slope_preds'].append(mean_pred.cpu().item())\n                results['log_var_preds'].append(log_var.cpu().item())\n            else:\n                results['slope_preds'].extend(mean_pred.cpu().tolist())\n                results['log_var_preds'].extend(log_var.cpu().tolist())\n\n            results['slope_targets'].append(batch['slope_target'].numpy())\n\n    results['slope_preds'] = np.array(results['slope_preds'])\n    results['log_var_preds'] = np.array(results['log_var_preds'])\n    results['slope_targets'] = np.concatenate(results['slope_targets'])\n\n    metrics = calculate_all_metrics(results)\n    return metrics\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T16:14:43.444132Z","iopub.execute_input":"2025-10-30T16:14:43.444375Z","iopub.status.idle":"2025-10-30T16:14:43.550769Z","shell.execute_reply.started":"2025-10-30T16:14:43.444358Z","shell.execute_reply":"2025-10-30T16:14:43.549925Z"}},"outputs":[{"name":"stdout","text":"ðŸš€ REPRODUCING v1.txt (RÂ²=0.46, MAE=3.27)\n============================================================\nðŸ“± Device: cuda\nðŸ“ Image Size: 384x384\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ============================================================================\n# 7. MAIN EXECUTION (Single Split)\n# ============================================================================\n\ndef run():\n    df, targets, baseline_df, tabular_dim = prepare_data()\n    \n    patients = baseline_df['Patient'].values\n    bins = baseline_df['decay_bins'].values\n    \n    # --- Create the v1.txt split ---\n    train_p, val_p = train_test_split(\n        patients, \n        test_size=config.TEST_SIZE, \n        random_state=config.SEED, \n        stratify=bins\n    )\n    \n    print(f\"\\n--- Single Split (v1.txt replication) ---\")\n    print(f\"Train: {len(train_p)} patients, Val: {len(val_p)} patients\")\n    \n    # --- 1. Clinical Baseline (XGBoost) ---\n    print(\"\\n--- Training Clinical Baseline (XGBoost) ---\")\n    \n    # --- *** FIX HERE *** ---\n    # `targets[p][1]` is tabular_features\n    # `targets[p][0]` is slope\n    X_train = np.array([targets[p][1] for p in train_p])\n    y_train = np.array([targets[p][0] for p in train_p])\n    X_val = np.array([targets[p][1] for p in val_p])\n    y_val = np.array([targets[p][0] for p in val_p])\n    \n    xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, \n                                 early_stopping_rounds=10, random_state=config.SEED)\n    xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n    y_pred = xgb_model.predict(X_val)\n    \n    baseline_r2 = r2_score(y_val, y_pred)\n    baseline_mae = mean_absolute_error(y_val, y_pred)\n    print(f\"Baseline XGBoost RÂ²: {baseline_r2:.4f}, MAE: {baseline_mae:.4f}\")\n    \n    # --- 2. Deep Learning Model ---\n    print(\"\\n--- Training Full Multimodal Model (v1.txt replication) ---\")\n    \n    train_ds = OptimizedOSICDataset(train_p, targets, config.TRAIN_DIR, 'train')\n    val_ds = OptimizedOSICDataset(val_p, targets, config.TRAIN_DIR, 'val')\n    \n    train_loader = DataLoader(train_ds, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=config.NUM_WORKERS, pin_memory=True, drop_last=True)\n    val_loader = DataLoader(val_ds, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=config.NUM_WORKERS)\n    \n    model = OptimizedDenseNetModel(tabular_dim=tabular_dim).to(config.DEVICE)\n    criterion = RegressionLoss()\n    \n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), \n                                  lr=config.LR, weight_decay=config.WEIGHT_DECAY)\n    \n    # --- Use ReduceLROnPlateau as in v1.txt ---\n    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, \n                                  patience=config.SCHEDULER_PATIENCE, verbose=True)\n    scaler = GradScaler()\n    \n    best_r2 = -float('inf')\n    best_mae = float('inf')\n    best_lll = -float('inf')\n    patience_counter = 0\n    \n    for epoch in range(config.EPOCHS):\n        print(f\"\\nEpoch {epoch+1}/{config.EPOCHS}\")\n        \n        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, scaler)\n        print(f\"Train Loss: {train_loss:.4f}\")\n        \n        val_metrics = validate_one_epoch(model, val_loader)\n        \n        print(\"--- Validation Metrics ---\")\n        print(f\"  RÂ²: {val_metrics['R2']:.4f}  |  MAE: {val_metrics['MAE']:.4f}  |  RMSE: {val_metrics['RMSE']:.4f}\")\n        print(f\"  LLL: {val_metrics['LaplaceLL']:.4f}\")\n        print(\"--------------------------\")\n        \n        scheduler.step(val_metrics['R2']) \n        \n        if val_metrics['R2'] > best_r2:\n            best_r2 = val_metrics['R2']\n            best_mae = val_metrics['MAE']\n            best_lll = val_metrics['LaplaceLL']\n            print(f\"ðŸŽ¯ New Best Model! RÂ²: {best_r2:.4f}, MAE: {best_mae:.4f}, LLL: {best_lll:.4f}. Saving...\")\n            torch.save(model.state_dict(), config.OUTPUT_DIR / 'best_v1_replication_model.pth')\n            patience_counter = 0\n        else:\n            patience_counter += 1\n        \n        if patience_counter >= config.EARLY_STOP_PATIENCE:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n            \n    print(f\"\\n{'='*25} FINAL RESULTS (v1.txt replication) {'='*25}\")\n    print(f\"ðŸ”¥ Best RÂ²: {best_r2:.4f}\")\n    print(f\"ðŸ”¥ Best MAE: {best_mae:.4f}\")\n    print(f\"ðŸ”¥ Best LLL: {best_lll:.4f}\")\n    \n    print(\"\\n--- Baseline Comparison ---\")\n    print(f\"Baseline RÂ²: {baseline_r2:.4f}\")\n    print(f\"Model RÂ²:    {best_r2:.4f}\")\n    print(f\"Improvement: {best_r2 - baseline_r2:.4f}\")\n\nif __name__ == \"__main__\":\n    run()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T16:14:43.552124Z","iopub.execute_input":"2025-10-30T16:14:43.552337Z","iopub.status.idle":"2025-10-30T16:20:47.102690Z","shell.execute_reply.started":"2025-10-30T16:14:43.552321Z","shell.execute_reply":"2025-10-30T16:20:47.101686Z"}},"outputs":[{"name":"stdout","text":"Loading data and calculating targets...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 176/176 [00:00<00:00, 1291.24it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 176 patients.\nDetected Tabular Dimension: 9\n\n--- Single Split (v1.txt replication) ---\nTrain: 149 patients, Val: 27 patients\n\n--- Training Clinical Baseline (XGBoost) ---\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Baseline XGBoost RÂ²: 0.0325, MAE: 4.9204\n\n--- Training Full Multimodal Model (v1.txt replication) ---\nDataset split 'train': 149 valid patients with images.\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n","output_type":"stream"},{"name":"stdout","text":"Dataset split 'val': 25 valid patients with images.\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30.8M/30.8M [00:00<00:00, 167MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Applying partial freezing (like v1.txt): only first 100 layers frozen.\n\nEpoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 26.7544\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: -0.0170  |  MAE: 4.6577  |  RMSE: 5.4883\n  LLL: -6.0911\n--------------------------\nðŸŽ¯ New Best Model! RÂ²: -0.0170, MAE: 4.6577, LLL: -6.0911. Saving...\n\nEpoch 2/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 11.5673\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: -0.1835  |  MAE: 4.8244  |  RMSE: 5.9205\n  LLL: -5.6089\n--------------------------\n\nEpoch 3/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 9.0508\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: -0.1781  |  MAE: 4.9423  |  RMSE: 5.9071\n  LLL: -5.7188\n--------------------------\n\nEpoch 4/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 7.4966\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: -0.2905  |  MAE: 4.8390  |  RMSE: 6.1824\n  LLL: -5.2331\n--------------------------\n\nEpoch 5/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 6.9884\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: -0.5807  |  MAE: 5.3871  |  RMSE: 6.8423\n  LLL: -5.6486\n--------------------------\n\nEpoch 6/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 6.1656\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: -0.0970  |  MAE: 4.8935  |  RMSE: 5.7001\n  LLL: -5.1609\n--------------------------\n\nEpoch 7/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 5.9415\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: -0.3942  |  MAE: 5.1400  |  RMSE: 6.4259\n  LLL: -5.3212\n--------------------------\n\nEpoch 8/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 5.7535\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: -0.2475  |  MAE: 4.9787  |  RMSE: 6.0783\n  LLL: -5.1919\n--------------------------\n\nEpoch 9/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 5.1206\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: -0.0099  |  MAE: 4.5763  |  RMSE: 5.4692\n  LLL: -4.8421\n--------------------------\nðŸŽ¯ New Best Model! RÂ²: -0.0099, MAE: 4.5763, LLL: -4.8421. Saving...\n\nEpoch 10/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 5.4280\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: 0.0083  |  MAE: 4.6539  |  RMSE: 5.4195\n  LLL: -4.9339\n--------------------------\nðŸŽ¯ New Best Model! RÂ²: 0.0083, MAE: 4.6539, LLL: -4.9339. Saving...\n\nEpoch 11/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 5.1294\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: -0.1122  |  MAE: 4.5246  |  RMSE: 5.7395\n  LLL: -4.8004\n--------------------------\n\nEpoch 12/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 4.8287\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: 0.2919  |  MAE: 4.0861  |  RMSE: 4.5795\n  LLL: -4.4163\n--------------------------\nðŸŽ¯ New Best Model! RÂ²: 0.2919, MAE: 4.0861, LLL: -4.4163. Saving...\n\nEpoch 13/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 4.8411\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: -0.0681  |  MAE: 4.4268  |  RMSE: 5.6243\n  LLL: -4.6562\n--------------------------\n\nEpoch 14/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 4.8667\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: -0.1801  |  MAE: 5.1030  |  RMSE: 5.9119\n  LLL: -5.3026\n--------------------------\n\nEpoch 15/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 4.5727\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: 0.2068  |  MAE: 3.9973  |  RMSE: 4.8470\n  LLL: -4.3070\n--------------------------\n\nEpoch 16/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 4.3101\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: 0.1643  |  MAE: 4.5164  |  RMSE: 4.9751\n  LLL: -4.7789\n--------------------------\n\nEpoch 17/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 3.9463\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: 0.0576  |  MAE: 4.3545  |  RMSE: 5.2830\n  LLL: -4.6368\n--------------------------\n\nEpoch 18/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 4.4322\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: 0.0211  |  MAE: 4.5440  |  RMSE: 5.3846\n  LLL: -4.7700\n--------------------------\n\nEpoch 19/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 3.9353\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: 0.0057  |  MAE: 4.3783  |  RMSE: 5.4267\n  LLL: -4.6289\n--------------------------\n\nEpoch 20/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 4.2426\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: 0.1101  |  MAE: 4.4249  |  RMSE: 5.1337\n  LLL: -4.6858\n--------------------------\n\nEpoch 21/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 3.8742\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: -0.0970  |  MAE: 4.6161  |  RMSE: 5.7000\n  LLL: -4.8311\n--------------------------\n\nEpoch 22/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 3.9107\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: -0.1374  |  MAE: 4.9907  |  RMSE: 5.8040\n  LLL: -5.1624\n--------------------------\nEarly stopping at epoch 22\n\n========================= FINAL RESULTS (v1.txt replication) =========================\nðŸ”¥ Best RÂ²: 0.2919\nðŸ”¥ Best MAE: 4.0861\nðŸ”¥ Best LLL: -4.4163\n\n--- Baseline Comparison ---\nBaseline RÂ²: 0.0325\nModel RÂ²:    0.2919\nImprovement: 0.2594\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ============================================================================\n# 7. MAIN EXECUTION (Single Split)\n# ============================================================================\n\ndef run():\n    df, targets, baseline_df, tabular_dim = prepare_data()\n    \n    patients = baseline_df['Patient'].values\n    bins = baseline_df['decay_bins'].values\n    \n    # --- Create the v1.txt split ---\n    train_p, val_p = train_test_split(\n        patients, \n        test_size=config.TEST_SIZE, \n        random_state=config.SEED, \n        stratify=bins\n    )\n    \n    print(f\"\\n--- Single Split (v1.txt replication) ---\")\n    print(f\"Train: {len(train_p)} patients, Val: {len(val_p)} patients\")\n    \n    # --- 1. Clinical Baseline (XGBoost) ---\n    print(\"\\n--- Training Clinical Baseline (XGBoost) ---\")\n    \n    # --- *** FIX HERE *** ---\n    # `targets[p][1]` is tabular_features\n    # `targets[p][0]` is slope\n    X_train = np.array([targets[p][1] for p in train_p])\n    y_train = np.array([targets[p][0] for p in train_p])\n    X_val = np.array([targets[p][1] for p in val_p])\n    y_val = np.array([targets[p][0] for p in val_p])\n    \n    xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, \n                                 early_stopping_rounds=10, random_state=config.SEED)\n    xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n    y_pred = xgb_model.predict(X_val)\n    \n    baseline_r2 = r2_score(y_val, y_pred)\n    baseline_mae = mean_absolute_error(y_val, y_pred)\n    print(f\"Baseline XGBoost RÂ²: {baseline_r2:.4f}, MAE: {baseline_mae:.4f}\")\n    \n    # --- 2. Deep Learning Model ---\n    print(\"\\n--- Training Full Multimodal Model (v1.txt replication) ---\")\n    \n    train_ds = OptimizedOSICDataset(train_p, targets, config.TRAIN_DIR, 'train')\n    val_ds = OptimizedOSICDataset(val_p, targets, config.TRAIN_DIR, 'val')\n    \n    train_loader = DataLoader(train_ds, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=config.NUM_WORKERS, pin_memory=True, drop_last=True)\n    val_loader = DataLoader(val_ds, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=config.NUM_WORKERS)\n    \n    model = OptimizedDenseNetModel(tabular_dim=tabular_dim).to(config.DEVICE)\n    criterion = RegressionLoss()\n    \n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), \n                                  lr=config.LR, weight_decay=config.WEIGHT_DECAY)\n    \n    # --- Use ReduceLROnPlateau as in v1.txt ---\n    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, \n                                  patience=config.SCHEDULER_PATIENCE, verbose=True)\n    scaler = GradScaler()\n    \n    best_r2 = -float('inf')\n    best_mae = float('inf')\n    best_lll = -float('inf')\n    patience_counter = 0\n    \n    for epoch in range(config.EPOCHS):\n        print(f\"\\nEpoch {epoch+1}/{config.EPOCHS}\")\n        \n        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, scaler)\n        print(f\"Train Loss: {train_loss:.4f}\")\n        \n        val_metrics = validate_one_epoch(model, val_loader)\n        \n        print(\"--- Validation Metrics ---\")\n        print(f\"  RÂ²: {val_metrics['R2']:.4f}  |  MAE: {val_metrics['MAE']:.4f}  |  RMSE: {val_metrics['RMSE']:.4f}\")\n        print(f\"  LLL: {val_metrics['LaplaceLL']:.4f}\")\n        print(\"--------------------------\")\n        \n        scheduler.step(val_metrics['R2']) \n        \n        if val_metrics['R2'] > best_r2:\n            best_r2 = val_metrics['R2']\n            best_mae = val_metrics['MAE']\n            best_lll = val_metrics['LaplaceLL']\n            print(f\"ðŸŽ¯ New Best Model! RÂ²: {best_r2:.4f}, MAE: {best_mae:.4f}, LLL: {best_lll:.4f}. Saving...\")\n            torch.save(model.state_dict(), config.OUTPUT_DIR / 'best_v2_replication_model.pth')\n            patience_counter = 0\n        else:\n            patience_counter += 1\n        \n        if patience_counter >= config.EARLY_STOP_PATIENCE:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n            \n    print(f\"\\n{'='*25} FINAL RESULTS (v1.txt replication) {'='*25}\")\n    print(f\"ðŸ”¥ Best RÂ²: {best_r2:.4f}\")\n    print(f\"ðŸ”¥ Best MAE: {best_mae:.4f}\")\n    print(f\"ðŸ”¥ Best LLL: {best_lll:.4f}\")\n    \n    print(\"\\n--- Baseline Comparison ---\")\n    print(f\"Baseline RÂ²: {baseline_r2:.4f}\")\n    print(f\"Model RÂ²:    {best_r2:.4f}\")\n    print(f\"Improvement: {best_r2 - baseline_r2:.4f}\")\n\nif __name__ == \"__main__\":\n    run()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T16:20:47.104306Z","iopub.execute_input":"2025-10-30T16:20:47.104584Z"}},"outputs":[{"name":"stdout","text":"Loading data and calculating targets...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 176/176 [00:00<00:00, 1294.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processed 176 patients.\nDetected Tabular Dimension: 9\n\n--- Single Split (v1.txt replication) ---\nTrain: 149 patients, Val: 27 patients\n\n--- Training Clinical Baseline (XGBoost) ---\nBaseline XGBoost RÂ²: 0.0325, MAE: 4.9204\n\n--- Training Full Multimodal Model (v1.txt replication) ---\nDataset split 'train': 149 valid patients with images.\nDataset split 'val': 25 valid patients with images.\nApplying partial freezing (like v1.txt): only first 100 layers frozen.\n\nEpoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 28.6541\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: -0.0791  |  MAE: 4.7034  |  RMSE: 5.6533\n  LLL: -6.1278\n--------------------------\nðŸŽ¯ New Best Model! RÂ²: -0.0791, MAE: 4.7034, LLL: -6.1278. Saving...\n\nEpoch 2/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 12.6471\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: 0.0276  |  MAE: 4.5247  |  RMSE: 5.3665\n  LLL: -5.4217\n--------------------------\nðŸŽ¯ New Best Model! RÂ²: 0.0276, MAE: 4.5247, LLL: -5.4217. Saving...\n\nEpoch 3/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 9.0944\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: -0.1033  |  MAE: 4.8070  |  RMSE: 5.7164\n  LLL: -5.3292\n--------------------------\n\nEpoch 4/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 7.9266\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: 0.0287  |  MAE: 4.6533  |  RMSE: 5.3636\n  LLL: -5.0782\n--------------------------\nðŸŽ¯ New Best Model! RÂ²: 0.0287, MAE: 4.6533, LLL: -5.0782. Saving...\n\nEpoch 5/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 7.5762\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: 0.0245  |  MAE: 4.4447  |  RMSE: 5.3751\n  LLL: -4.9160\n--------------------------\n\nEpoch 6/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 6.8280\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: 0.1529  |  MAE: 4.2803  |  RMSE: 5.0089\n  LLL: -4.6090\n--------------------------\nðŸŽ¯ New Best Model! RÂ²: 0.1529, MAE: 4.2803, LLL: -4.6090. Saving...\n\nEpoch 7/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 6.7655\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: -0.4316  |  MAE: 5.4810  |  RMSE: 6.5116\n  LLL: -5.7534\n--------------------------\n\nEpoch 8/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 6.1878\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: -0.0908  |  MAE: 4.7512  |  RMSE: 5.6839\n  LLL: -4.9858\n--------------------------\n\nEpoch 9/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 5.8884\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: -0.2352  |  MAE: 5.1904  |  RMSE: 6.0485\n  LLL: -5.4004\n--------------------------\n\nEpoch 10/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 5.8023\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: -0.4242  |  MAE: 5.1858  |  RMSE: 6.4948\n  LLL: -5.3483\n--------------------------\n\nEpoch 11/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 5.7883\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: -0.0068  |  MAE: 4.4584  |  RMSE: 5.4607\n  LLL: -4.7291\n--------------------------\n\nEpoch 12/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 5.2139\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: -0.0334  |  MAE: 4.7254  |  RMSE: 5.5324\n  LLL: -5.0046\n--------------------------\n\nEpoch 13/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 4.5452\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: 0.2246  |  MAE: 4.0682  |  RMSE: 4.7921\n  LLL: -4.3750\n--------------------------\nðŸŽ¯ New Best Model! RÂ²: 0.2246, MAE: 4.0682, LLL: -4.3750. Saving...\n\nEpoch 14/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 4.7240\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: 0.1460  |  MAE: 4.2228  |  RMSE: 5.0291\n  LLL: -4.5097\n--------------------------\n\nEpoch 15/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 4.5943\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: 0.0641  |  MAE: 4.2278  |  RMSE: 5.2648\n  LLL: -4.5566\n--------------------------\n\nEpoch 16/50\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 4.5109\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"--- Validation Metrics ---\n  RÂ²: -0.1047  |  MAE: 4.9025  |  RMSE: 5.7199\n  LLL: -5.0678\n--------------------------\n\nEpoch 17/50\n","output_type":"stream"},{"name":"stderr","text":"Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 121/149 [00:12<00:02,  9.77it/s, loss=3.33] ","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# 7. MAIN EXECUTION (Single Split)\n# ============================================================================\n\ndef run():\n    df, targets, baseline_df, tabular_dim = prepare_data()\n    \n    patients = baseline_df['Patient'].values\n    bins = baseline_df['decay_bins'].values\n    \n    # --- Create the v1.txt split ---\n    train_p, val_p = train_test_split(\n        patients, \n        test_size=config.TEST_SIZE, \n        random_state=config.SEED, \n        stratify=bins\n    )\n    \n    print(f\"\\n--- Single Split (v1.txt replication) ---\")\n    print(f\"Train: {len(train_p)} patients, Val: {len(val_p)} patients\")\n    \n    # --- 1. Clinical Baseline (XGBoost) ---\n    print(\"\\n--- Training Clinical Baseline (XGBoost) ---\")\n    \n    # --- *** FIX HERE *** ---\n    # `targets[p][1]` is tabular_features\n    # `targets[p][0]` is slope\n    X_train = np.array([targets[p][1] for p in train_p])\n    y_train = np.array([targets[p][0] for p in train_p])\n    X_val = np.array([targets[p][1] for p in val_p])\n    y_val = np.array([targets[p][0] for p in val_p])\n    \n    xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, \n                                 early_stopping_rounds=10, random_state=config.SEED)\n    xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n    y_pred = xgb_model.predict(X_val)\n    \n    baseline_r2 = r2_score(y_val, y_pred)\n    baseline_mae = mean_absolute_error(y_val, y_pred)\n    print(f\"Baseline XGBoost RÂ²: {baseline_r2:.4f}, MAE: {baseline_mae:.4f}\")\n    \n    # --- 2. Deep Learning Model ---\n    print(\"\\n--- Training Full Multimodal Model (v1.txt replication) ---\")\n    \n    train_ds = OptimizedOSICDataset(train_p, targets, config.TRAIN_DIR, 'train')\n    val_ds = OptimizedOSICDataset(val_p, targets, config.TRAIN_DIR, 'val')\n    \n    train_loader = DataLoader(train_ds, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=config.NUM_WORKERS, pin_memory=True, drop_last=True)\n    val_loader = DataLoader(val_ds, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=config.NUM_WORKERS)\n    \n    model = OptimizedDenseNetModel(tabular_dim=tabular_dim).to(config.DEVICE)\n    criterion = RegressionLoss()\n    \n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), \n                                  lr=config.LR, weight_decay=config.WEIGHT_DECAY)\n    \n    # --- Use ReduceLROnPlateau as in v1.txt ---\n    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, \n                                  patience=config.SCHEDULER_PATIENCE, verbose=True)\n    scaler = GradScaler()\n    \n    best_r2 = -float('inf')\n    best_mae = float('inf')\n    best_lll = -float('inf')\n    patience_counter = 0\n    \n    for epoch in range(config.EPOCHS):\n        print(f\"\\nEpoch {epoch+1}/{config.EPOCHS}\")\n        \n        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, scaler)\n        print(f\"Train Loss: {train_loss:.4f}\")\n        \n        val_metrics = validate_one_epoch(model, val_loader)\n        \n        print(\"--- Validation Metrics ---\")\n        print(f\"  RÂ²: {val_metrics['R2']:.4f}  |  MAE: {val_metrics['MAE']:.4f}  |  RMSE: {val_metrics['RMSE']:.4f}\")\n        print(f\"  LLL: {val_metrics['LaplaceLL']:.4f}\")\n        print(\"--------------------------\")\n        \n        scheduler.step(val_metrics['R2']) \n        \n        if val_metrics['R2'] > best_r2:\n            best_r2 = val_metrics['R2']\n            best_mae = val_metrics['MAE']\n            best_lll = val_metrics['LaplaceLL']\n            print(f\"ðŸŽ¯ New Best Model! RÂ²: {best_r2:.4f}, MAE: {best_mae:.4f}, LLL: {best_lll:.4f}. Saving...\")\n            torch.save(model.state_dict(), config.OUTPUT_DIR / 'best_v3_replication_model.pth')\n            patience_counter = 0\n        else:\n            patience_counter += 1\n        \n        if patience_counter >= config.EARLY_STOP_PATIENCE:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n            \n    print(f\"\\n{'='*25} FINAL RESULTS (v1.txt replication) {'='*25}\")\n    print(f\"ðŸ”¥ Best RÂ²: {best_r2:.4f}\")\n    print(f\"ðŸ”¥ Best MAE: {best_mae:.4f}\")\n    print(f\"ðŸ”¥ Best LLL: {best_lll:.4f}\")\n    \n    print(\"\\n--- Baseline Comparison ---\")\n    print(f\"Baseline RÂ²: {baseline_r2:.4f}\")\n    print(f\"Model RÂ²:    {best_r2:.4f}\")\n    print(f\"Improvement: {best_r2 - baseline_r2:.4f}\")\n\nif __name__ == \"__main__\":\n    run()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# 7. MAIN EXECUTION (Single Split)\n# ============================================================================\n\ndef run():\n    df, targets, baseline_df, tabular_dim = prepare_data()\n    \n    patients = baseline_df['Patient'].values\n    bins = baseline_df['decay_bins'].values\n    \n    # --- Create the v1.txt split ---\n    train_p, val_p = train_test_split(\n        patients, \n        test_size=config.TEST_SIZE, \n        random_state=config.SEED, \n        stratify=bins\n    )\n    \n    print(f\"\\n--- Single Split (v1.txt replication) ---\")\n    print(f\"Train: {len(train_p)} patients, Val: {len(val_p)} patients\")\n    \n    # --- 1. Clinical Baseline (XGBoost) ---\n    print(\"\\n--- Training Clinical Baseline (XGBoost) ---\")\n    \n    # --- *** FIX HERE *** ---\n    # `targets[p][1]` is tabular_features\n    # `targets[p][0]` is slope\n    X_train = np.array([targets[p][1] for p in train_p])\n    y_train = np.array([targets[p][0] for p in train_p])\n    X_val = np.array([targets[p][1] for p in val_p])\n    y_val = np.array([targets[p][0] for p in val_p])\n    \n    xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, \n                                 early_stopping_rounds=10, random_state=config.SEED)\n    xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n    y_pred = xgb_model.predict(X_val)\n    \n    baseline_r2 = r2_score(y_val, y_pred)\n    baseline_mae = mean_absolute_error(y_val, y_pred)\n    print(f\"Baseline XGBoost RÂ²: {baseline_r2:.4f}, MAE: {baseline_mae:.4f}\")\n    \n    # --- 2. Deep Learning Model ---\n    print(\"\\n--- Training Full Multimodal Model (v1.txt replication) ---\")\n    \n    train_ds = OptimizedOSICDataset(train_p, targets, config.TRAIN_DIR, 'train')\n    val_ds = OptimizedOSICDataset(val_p, targets, config.TRAIN_DIR, 'val')\n    \n    train_loader = DataLoader(train_ds, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=config.NUM_WORKERS, pin_memory=True, drop_last=True)\n    val_loader = DataLoader(val_ds, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=config.NUM_WORKERS)\n    \n    model = OptimizedDenseNetModel(tabular_dim=tabular_dim).to(config.DEVICE)\n    criterion = RegressionLoss()\n    \n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), \n                                  lr=config.LR, weight_decay=config.WEIGHT_DECAY)\n    \n    # --- Use ReduceLROnPlateau as in v1.txt ---\n    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, \n                                  patience=config.SCHEDULER_PATIENCE, verbose=True)\n    scaler = GradScaler()\n    \n    best_r2 = -float('inf')\n    best_mae = float('inf')\n    best_lll = -float('inf')\n    patience_counter = 0\n    \n    for epoch in range(config.EPOCHS):\n        print(f\"\\nEpoch {epoch+1}/{config.EPOCHS}\")\n        \n        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, scaler)\n        print(f\"Train Loss: {train_loss:.4f}\")\n        \n        val_metrics = validate_one_epoch(model, val_loader)\n        \n        print(\"--- Validation Metrics ---\")\n        print(f\"  RÂ²: {val_metrics['R2']:.4f}  |  MAE: {val_metrics['MAE']:.4f}  |  RMSE: {val_metrics['RMSE']:.4f}\")\n        print(f\"  LLL: {val_metrics['LaplaceLL']:.4f}\")\n        print(\"--------------------------\")\n        \n        scheduler.step(val_metrics['R2']) \n        \n        if val_metrics['R2'] > best_r2:\n            best_r2 = val_metrics['R2']\n            best_mae = val_metrics['MAE']\n            best_lll = val_metrics['LaplaceLL']\n            print(f\"ðŸŽ¯ New Best Model! RÂ²: {best_r2:.4f}, MAE: {best_mae:.4f}, LLL: {best_lll:.4f}. Saving...\")\n            torch.save(model.state_dict(), config.OUTPUT_DIR / 'best_v4_replication_model.pth')\n            patience_counter = 0\n        else:\n            patience_counter += 1\n        \n        if patience_counter >= config.EARLY_STOP_PATIENCE:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n            \n    print(f\"\\n{'='*25} FINAL RESULTS (v1.txt replication) {'='*25}\")\n    print(f\"ðŸ”¥ Best RÂ²: {best_r2:.4f}\")\n    print(f\"ðŸ”¥ Best MAE: {best_mae:.4f}\")\n    print(f\"ðŸ”¥ Best LLL: {best_lll:.4f}\")\n    \n    print(\"\\n--- Baseline Comparison ---\")\n    print(f\"Baseline RÂ²: {baseline_r2:.4f}\")\n    print(f\"Model RÂ²:    {best_r2:.4f}\")\n    print(f\"Improvement: {best_r2 - baseline_r2:.4f}\")\n\nif __name__ == \"__main__\":\n    run()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# 7. MAIN EXECUTION (Single Split)\n# ============================================================================\n\ndef run():\n    df, targets, baseline_df, tabular_dim = prepare_data()\n    \n    patients = baseline_df['Patient'].values\n    bins = baseline_df['decay_bins'].values\n    \n    # --- Create the v1.txt split ---\n    train_p, val_p = train_test_split(\n        patients, \n        test_size=config.TEST_SIZE, \n        random_state=config.SEED, \n        stratify=bins\n    )\n    \n    print(f\"\\n--- Single Split (v1.txt replication) ---\")\n    print(f\"Train: {len(train_p)} patients, Val: {len(val_p)} patients\")\n    \n    # --- 1. Clinical Baseline (XGBoost) ---\n    print(\"\\n--- Training Clinical Baseline (XGBoost) ---\")\n    \n    # --- *** FIX HERE *** ---\n    # `targets[p][1]` is tabular_features\n    # `targets[p][0]` is slope\n    X_train = np.array([targets[p][1] for p in train_p])\n    y_train = np.array([targets[p][0] for p in train_p])\n    X_val = np.array([targets[p][1] for p in val_p])\n    y_val = np.array([targets[p][0] for p in val_p])\n    \n    xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, \n                                 early_stopping_rounds=10, random_state=config.SEED)\n    xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n    y_pred = xgb_model.predict(X_val)\n    \n    baseline_r2 = r2_score(y_val, y_pred)\n    baseline_mae = mean_absolute_error(y_val, y_pred)\n    print(f\"Baseline XGBoost RÂ²: {baseline_r2:.4f}, MAE: {baseline_mae:.4f}\")\n    \n    # --- 2. Deep Learning Model ---\n    print(\"\\n--- Training Full Multimodal Model (v1.txt replication) ---\")\n    \n    train_ds = OptimizedOSICDataset(train_p, targets, config.TRAIN_DIR, 'train')\n    val_ds = OptimizedOSICDataset(val_p, targets, config.TRAIN_DIR, 'val')\n    \n    train_loader = DataLoader(train_ds, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=config.NUM_WORKERS, pin_memory=True, drop_last=True)\n    val_loader = DataLoader(val_ds, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=config.NUM_WORKERS)\n    \n    model = OptimizedDenseNetModel(tabular_dim=tabular_dim).to(config.DEVICE)\n    criterion = RegressionLoss()\n    \n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), \n                                  lr=config.LR, weight_decay=config.WEIGHT_DECAY)\n    \n    # --- Use ReduceLROnPlateau as in v1.txt ---\n    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, \n                                  patience=config.SCHEDULER_PATIENCE, verbose=True)\n    scaler = GradScaler()\n    \n    best_r2 = -float('inf')\n    best_mae = float('inf')\n    best_lll = -float('inf')\n    patience_counter = 0\n    \n    for epoch in range(config.EPOCHS):\n        print(f\"\\nEpoch {epoch+1}/{config.EPOCHS}\")\n        \n        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, scaler)\n        print(f\"Train Loss: {train_loss:.4f}\")\n        \n        val_metrics = validate_one_epoch(model, val_loader)\n        \n        print(\"--- Validation Metrics ---\")\n        print(f\"  RÂ²: {val_metrics['R2']:.4f}  |  MAE: {val_metrics['MAE']:.4f}  |  RMSE: {val_metrics['RMSE']:.4f}\")\n        print(f\"  LLL: {val_metrics['LaplaceLL']:.4f}\")\n        print(\"--------------------------\")\n        \n        scheduler.step(val_metrics['R2']) \n        \n        if val_metrics['R2'] > best_r2:\n            best_r2 = val_metrics['R2']\n            best_mae = val_metrics['MAE']\n            best_lll = val_metrics['LaplaceLL']\n            print(f\"ðŸŽ¯ New Best Model! RÂ²: {best_r2:.4f}, MAE: {best_mae:.4f}, LLL: {best_lll:.4f}. Saving...\")\n            torch.save(model.state_dict(), config.OUTPUT_DIR / 'best_v5_replication_model.pth')\n            patience_counter = 0\n        else:\n            patience_counter += 1\n        \n        if patience_counter >= config.EARLY_STOP_PATIENCE:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n            \n    print(f\"\\n{'='*25} FINAL RESULTS (v1.txt replication) {'='*25}\")\n    print(f\"ðŸ”¥ Best RÂ²: {best_r2:.4f}\")\n    print(f\"ðŸ”¥ Best MAE: {best_mae:.4f}\")\n    print(f\"ðŸ”¥ Best LLL: {best_lll:.4f}\")\n    \n    print(\"\\n--- Baseline Comparison ---\")\n    print(f\"Baseline RÂ²: {baseline_r2:.4f}\")\n    print(f\"Model RÂ²:    {best_r2:.4f}\")\n    print(f\"Improvement: {best_r2 - baseline_r2:.4f}\")\n\nif __name__ == \"__main__\":\n    run()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# 7. MAIN EXECUTION (Single Split)\n# ============================================================================\n\ndef run():\n    df, targets, baseline_df, tabular_dim = prepare_data()\n    \n    patients = baseline_df['Patient'].values\n    bins = baseline_df['decay_bins'].values\n    \n    # --- Create the v1.txt split ---\n    train_p, val_p = train_test_split(\n        patients, \n        test_size=config.TEST_SIZE, \n        random_state=config.SEED, \n        stratify=bins\n    )\n    \n    print(f\"\\n--- Single Split (v1.txt replication) ---\")\n    print(f\"Train: {len(train_p)} patients, Val: {len(val_p)} patients\")\n    \n    # --- 1. Clinical Baseline (XGBoost) ---\n    print(\"\\n--- Training Clinical Baseline (XGBoost) ---\")\n    \n    # --- *** FIX HERE *** ---\n    # `targets[p][1]` is tabular_features\n    # `targets[p][0]` is slope\n    X_train = np.array([targets[p][1] for p in train_p])\n    y_train = np.array([targets[p][0] for p in train_p])\n    X_val = np.array([targets[p][1] for p in val_p])\n    y_val = np.array([targets[p][0] for p in val_p])\n    \n    xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, \n                                 early_stopping_rounds=10, random_state=config.SEED)\n    xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n    y_pred = xgb_model.predict(X_val)\n    \n    baseline_r2 = r2_score(y_val, y_pred)\n    baseline_mae = mean_absolute_error(y_val, y_pred)\n    print(f\"Baseline XGBoost RÂ²: {baseline_r2:.4f}, MAE: {baseline_mae:.4f}\")\n    \n    # --- 2. Deep Learning Model ---\n    print(\"\\n--- Training Full Multimodal Model (v1.txt replication) ---\")\n    \n    train_ds = OptimizedOSICDataset(train_p, targets, config.TRAIN_DIR, 'train')\n    val_ds = OptimizedOSICDataset(val_p, targets, config.TRAIN_DIR, 'val')\n    \n    train_loader = DataLoader(train_ds, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=config.NUM_WORKERS, pin_memory=True, drop_last=True)\n    val_loader = DataLoader(val_ds, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=config.NUM_WORKERS)\n    \n    model = OptimizedDenseNetModel(tabular_dim=tabular_dim).to(config.DEVICE)\n    criterion = RegressionLoss()\n    \n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), \n                                  lr=config.LR, weight_decay=config.WEIGHT_DECAY)\n    \n    # --- Use ReduceLROnPlateau as in v1.txt ---\n    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, \n                                  patience=config.SCHEDULER_PATIENCE, verbose=True)\n    scaler = GradScaler()\n    \n    best_r2 = -float('inf')\n    best_mae = float('inf')\n    best_lll = -float('inf')\n    patience_counter = 0\n    \n    for epoch in range(config.EPOCHS):\n        print(f\"\\nEpoch {epoch+1}/{config.EPOCHS}\")\n        \n        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, scaler)\n        print(f\"Train Loss: {train_loss:.4f}\")\n        \n        val_metrics = validate_one_epoch(model, val_loader)\n        \n        print(\"--- Validation Metrics ---\")\n        print(f\"  RÂ²: {val_metrics['R2']:.4f}  |  MAE: {val_metrics['MAE']:.4f}  |  RMSE: {val_metrics['RMSE']:.4f}\")\n        print(f\"  LLL: {val_metrics['LaplaceLL']:.4f}\")\n        print(\"--------------------------\")\n        \n        scheduler.step(val_metrics['R2']) \n        \n        if val_metrics['R2'] > best_r2:\n            best_r2 = val_metrics['R2']\n            best_mae = val_metrics['MAE']\n            best_lll = val_metrics['LaplaceLL']\n            print(f\"ðŸŽ¯ New Best Model! RÂ²: {best_r2:.4f}, MAE: {best_mae:.4f}, LLL: {best_lll:.4f}. Saving...\")\n            torch.save(model.state_dict(), config.OUTPUT_DIR / 'best_v6_replication_model.pth')\n            patience_counter = 0\n        else:\n            patience_counter += 1\n        \n        if patience_counter >= config.EARLY_STOP_PATIENCE:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n            \n    print(f\"\\n{'='*25} FINAL RESULTS (v1.txt replication) {'='*25}\")\n    print(f\"ðŸ”¥ Best RÂ²: {best_r2:.4f}\")\n    print(f\"ðŸ”¥ Best MAE: {best_mae:.4f}\")\n    print(f\"ðŸ”¥ Best LLL: {best_lll:.4f}\")\n    \n    print(\"\\n--- Baseline Comparison ---\")\n    print(f\"Baseline RÂ²: {baseline_r2:.4f}\")\n    print(f\"Model RÂ²:    {best_r2:.4f}\")\n    print(f\"Improvement: {best_r2 - baseline_r2:.4f}\")\n\nif __name__ == \"__main__\":\n    run()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# 7. MAIN EXECUTION (Single Split)\n# ============================================================================\n\ndef run():\n    df, targets, baseline_df, tabular_dim = prepare_data()\n    \n    patients = baseline_df['Patient'].values\n    bins = baseline_df['decay_bins'].values\n    \n    # --- Create the v1.txt split ---\n    train_p, val_p = train_test_split(\n        patients, \n        test_size=config.TEST_SIZE, \n        random_state=config.SEED, \n        stratify=bins\n    )\n    \n    print(f\"\\n--- Single Split (v1.txt replication) ---\")\n    print(f\"Train: {len(train_p)} patients, Val: {len(val_p)} patients\")\n    \n    # --- 1. Clinical Baseline (XGBoost) ---\n    print(\"\\n--- Training Clinical Baseline (XGBoost) ---\")\n    \n    # --- *** FIX HERE *** ---\n    # `targets[p][1]` is tabular_features\n    # `targets[p][0]` is slope\n    X_train = np.array([targets[p][1] for p in train_p])\n    y_train = np.array([targets[p][0] for p in train_p])\n    X_val = np.array([targets[p][1] for p in val_p])\n    y_val = np.array([targets[p][0] for p in val_p])\n    \n    xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, \n                                 early_stopping_rounds=10, random_state=config.SEED)\n    xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n    y_pred = xgb_model.predict(X_val)\n    \n    baseline_r2 = r2_score(y_val, y_pred)\n    baseline_mae = mean_absolute_error(y_val, y_pred)\n    print(f\"Baseline XGBoost RÂ²: {baseline_r2:.4f}, MAE: {baseline_mae:.4f}\")\n    \n    # --- 2. Deep Learning Model ---\n    print(\"\\n--- Training Full Multimodal Model (v1.txt replication) ---\")\n    \n    train_ds = OptimizedOSICDataset(train_p, targets, config.TRAIN_DIR, 'train')\n    val_ds = OptimizedOSICDataset(val_p, targets, config.TRAIN_DIR, 'val')\n    \n    train_loader = DataLoader(train_ds, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=config.NUM_WORKERS, pin_memory=True, drop_last=True)\n    val_loader = DataLoader(val_ds, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=config.NUM_WORKERS)\n    \n    model = OptimizedDenseNetModel(tabular_dim=tabular_dim).to(config.DEVICE)\n    criterion = RegressionLoss()\n    \n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), \n                                  lr=config.LR, weight_decay=config.WEIGHT_DECAY)\n    \n    # --- Use ReduceLROnPlateau as in v1.txt ---\n    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, \n                                  patience=config.SCHEDULER_PATIENCE, verbose=True)\n    scaler = GradScaler()\n    \n    best_r2 = -float('inf')\n    best_mae = float('inf')\n    best_lll = -float('inf')\n    patience_counter = 0\n    \n    for epoch in range(config.EPOCHS):\n        print(f\"\\nEpoch {epoch+1}/{config.EPOCHS}\")\n        \n        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, scaler)\n        print(f\"Train Loss: {train_loss:.4f}\")\n        \n        val_metrics = validate_one_epoch(model, val_loader)\n        \n        print(\"--- Validation Metrics ---\")\n        print(f\"  RÂ²: {val_metrics['R2']:.4f}  |  MAE: {val_metrics['MAE']:.4f}  |  RMSE: {val_metrics['RMSE']:.4f}\")\n        print(f\"  LLL: {val_metrics['LaplaceLL']:.4f}\")\n        print(\"--------------------------\")\n        \n        scheduler.step(val_metrics['R2']) \n        \n        if val_metrics['R2'] > best_r2:\n            best_r2 = val_metrics['R2']\n            best_mae = val_metrics['MAE']\n            best_lll = val_metrics['LaplaceLL']\n            print(f\"ðŸŽ¯ New Best Model! RÂ²: {best_r2:.4f}, MAE: {best_mae:.4f}, LLL: {best_lll:.4f}. Saving...\")\n            torch.save(model.state_dict(), config.OUTPUT_DIR / 'best_v7_replication_model.pth')\n            patience_counter = 0\n        else:\n            patience_counter += 1\n        \n        if patience_counter >= config.EARLY_STOP_PATIENCE:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n            \n    print(f\"\\n{'='*25} FINAL RESULTS (v1.txt replication) {'='*25}\")\n    print(f\"ðŸ”¥ Best RÂ²: {best_r2:.4f}\")\n    print(f\"ðŸ”¥ Best MAE: {best_mae:.4f}\")\n    print(f\"ðŸ”¥ Best LLL: {best_lll:.4f}\")\n    \n    print(\"\\n--- Baseline Comparison ---\")\n    print(f\"Baseline RÂ²: {baseline_r2:.4f}\")\n    print(f\"Model RÂ²:    {best_r2:.4f}\")\n    print(f\"Improvement: {best_r2 - baseline_r2:.4f}\")\n\nif __name__ == \"__main__\":\n    run()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# 7. MAIN EXECUTION (Single Split)\n# ============================================================================\n\ndef run():\n    df, targets, baseline_df, tabular_dim = prepare_data()\n    \n    patients = baseline_df['Patient'].values\n    bins = baseline_df['decay_bins'].values\n    \n    # --- Create the v1.txt split ---\n    train_p, val_p = train_test_split(\n        patients, \n        test_size=config.TEST_SIZE, \n        random_state=config.SEED, \n        stratify=bins\n    )\n    \n    print(f\"\\n--- Single Split (v1.txt replication) ---\")\n    print(f\"Train: {len(train_p)} patients, Val: {len(val_p)} patients\")\n    \n    # --- 1. Clinical Baseline (XGBoost) ---\n    print(\"\\n--- Training Clinical Baseline (XGBoost) ---\")\n    \n    # --- *** FIX HERE *** ---\n    # `targets[p][1]` is tabular_features\n    # `targets[p][0]` is slope\n    X_train = np.array([targets[p][1] for p in train_p])\n    y_train = np.array([targets[p][0] for p in train_p])\n    X_val = np.array([targets[p][1] for p in val_p])\n    y_val = np.array([targets[p][0] for p in val_p])\n    \n    xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, \n                                 early_stopping_rounds=10, random_state=config.SEED)\n    xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n    y_pred = xgb_model.predict(X_val)\n    \n    baseline_r2 = r2_score(y_val, y_pred)\n    baseline_mae = mean_absolute_error(y_val, y_pred)\n    print(f\"Baseline XGBoost RÂ²: {baseline_r2:.4f}, MAE: {baseline_mae:.4f}\")\n    \n    # --- 2. Deep Learning Model ---\n    print(\"\\n--- Training Full Multimodal Model (v1.txt replication) ---\")\n    \n    train_ds = OptimizedOSICDataset(train_p, targets, config.TRAIN_DIR, 'train')\n    val_ds = OptimizedOSICDataset(val_p, targets, config.TRAIN_DIR, 'val')\n    \n    train_loader = DataLoader(train_ds, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=config.NUM_WORKERS, pin_memory=True, drop_last=True)\n    val_loader = DataLoader(val_ds, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=config.NUM_WORKERS)\n    \n    model = OptimizedDenseNetModel(tabular_dim=tabular_dim).to(config.DEVICE)\n    criterion = RegressionLoss()\n    \n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), \n                                  lr=config.LR, weight_decay=config.WEIGHT_DECAY)\n    \n    # --- Use ReduceLROnPlateau as in v1.txt ---\n    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, \n                                  patience=config.SCHEDULER_PATIENCE, verbose=True)\n    scaler = GradScaler()\n    \n    best_r2 = -float('inf')\n    best_mae = float('inf')\n    best_lll = -float('inf')\n    patience_counter = 0\n    \n    for epoch in range(config.EPOCHS):\n        print(f\"\\nEpoch {epoch+1}/{config.EPOCHS}\")\n        \n        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, scaler)\n        print(f\"Train Loss: {train_loss:.4f}\")\n        \n        val_metrics = validate_one_epoch(model, val_loader)\n        \n        print(\"--- Validation Metrics ---\")\n        print(f\"  RÂ²: {val_metrics['R2']:.4f}  |  MAE: {val_metrics['MAE']:.4f}  |  RMSE: {val_metrics['RMSE']:.4f}\")\n        print(f\"  LLL: {val_metrics['LaplaceLL']:.4f}\")\n        print(\"--------------------------\")\n        \n        scheduler.step(val_metrics['R2']) \n        \n        if val_metrics['R2'] > best_r2:\n            best_r2 = val_metrics['R2']\n            best_mae = val_metrics['MAE']\n            best_lll = val_metrics['LaplaceLL']\n            print(f\"ðŸŽ¯ New Best Model! RÂ²: {best_r2:.4f}, MAE: {best_mae:.4f}, LLL: {best_lll:.4f}. Saving...\")\n            torch.save(model.state_dict(), config.OUTPUT_DIR / 'best_v8_replication_model.pth')\n            patience_counter = 0\n        else:\n            patience_counter += 1\n        \n        if patience_counter >= config.EARLY_STOP_PATIENCE:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n            \n    print(f\"\\n{'='*25} FINAL RESULTS (v1.txt replication) {'='*25}\")\n    print(f\"ðŸ”¥ Best RÂ²: {best_r2:.4f}\")\n    print(f\"ðŸ”¥ Best MAE: {best_mae:.4f}\")\n    print(f\"ðŸ”¥ Best LLL: {best_lll:.4f}\")\n    \n    print(\"\\n--- Baseline Comparison ---\")\n    print(f\"Baseline RÂ²: {baseline_r2:.4f}\")\n    print(f\"Model RÂ²:    {best_r2:.4f}\")\n    print(f\"Improvement: {best_r2 - baseline_r2:.4f}\")\n\nif __name__ == \"__main__\":\n    run()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# 7. MAIN EXECUTION (Single Split)\n# ============================================================================\n\ndef run():\n    df, targets, baseline_df, tabular_dim = prepare_data()\n    \n    patients = baseline_df['Patient'].values\n    bins = baseline_df['decay_bins'].values\n    \n    # --- Create the v1.txt split ---\n    train_p, val_p = train_test_split(\n        patients, \n        test_size=config.TEST_SIZE, \n        random_state=config.SEED, \n        stratify=bins\n    )\n    \n    print(f\"\\n--- Single Split (v1.txt replication) ---\")\n    print(f\"Train: {len(train_p)} patients, Val: {len(val_p)} patients\")\n    \n    # --- 1. Clinical Baseline (XGBoost) ---\n    print(\"\\n--- Training Clinical Baseline (XGBoost) ---\")\n    \n    # --- *** FIX HERE *** ---\n    # `targets[p][1]` is tabular_features\n    # `targets[p][0]` is slope\n    X_train = np.array([targets[p][1] for p in train_p])\n    y_train = np.array([targets[p][0] for p in train_p])\n    X_val = np.array([targets[p][1] for p in val_p])\n    y_val = np.array([targets[p][0] for p in val_p])\n    \n    xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, \n                                 early_stopping_rounds=10, random_state=config.SEED)\n    xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n    y_pred = xgb_model.predict(X_val)\n    \n    baseline_r2 = r2_score(y_val, y_pred)\n    baseline_mae = mean_absolute_error(y_val, y_pred)\n    print(f\"Baseline XGBoost RÂ²: {baseline_r2:.4f}, MAE: {baseline_mae:.4f}\")\n    \n    # --- 2. Deep Learning Model ---\n    print(\"\\n--- Training Full Multimodal Model (v1.txt replication) ---\")\n    \n    train_ds = OptimizedOSICDataset(train_p, targets, config.TRAIN_DIR, 'train')\n    val_ds = OptimizedOSICDataset(val_p, targets, config.TRAIN_DIR, 'val')\n    \n    train_loader = DataLoader(train_ds, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=config.NUM_WORKERS, pin_memory=True, drop_last=True)\n    val_loader = DataLoader(val_ds, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=config.NUM_WORKERS)\n    \n    model = OptimizedDenseNetModel(tabular_dim=tabular_dim).to(config.DEVICE)\n    criterion = RegressionLoss()\n    \n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), \n                                  lr=config.LR, weight_decay=config.WEIGHT_DECAY)\n    \n    # --- Use ReduceLROnPlateau as in v1.txt ---\n    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, \n                                  patience=config.SCHEDULER_PATIENCE, verbose=True)\n    scaler = GradScaler()\n    \n    best_r2 = -float('inf')\n    best_mae = float('inf')\n    best_lll = -float('inf')\n    patience_counter = 0\n    \n    for epoch in range(config.EPOCHS):\n        print(f\"\\nEpoch {epoch+1}/{config.EPOCHS}\")\n        \n        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, scaler)\n        print(f\"Train Loss: {train_loss:.4f}\")\n        \n        val_metrics = validate_one_epoch(model, val_loader)\n        \n        print(\"--- Validation Metrics ---\")\n        print(f\"  RÂ²: {val_metrics['R2']:.4f}  |  MAE: {val_metrics['MAE']:.4f}  |  RMSE: {val_metrics['RMSE']:.4f}\")\n        print(f\"  LLL: {val_metrics['LaplaceLL']:.4f}\")\n        print(\"--------------------------\")\n        \n        scheduler.step(val_metrics['R2']) \n        \n        if val_metrics['R2'] > best_r2:\n            best_r2 = val_metrics['R2']\n            best_mae = val_metrics['MAE']\n            best_lll = val_metrics['LaplaceLL']\n            print(f\"ðŸŽ¯ New Best Model! RÂ²: {best_r2:.4f}, MAE: {best_mae:.4f}, LLL: {best_lll:.4f}. Saving...\")\n            torch.save(model.state_dict(), config.OUTPUT_DIR / 'best_v9_replication_model.pth')\n            patience_counter = 0\n        else:\n            patience_counter += 1\n        \n        if patience_counter >= config.EARLY_STOP_PATIENCE:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n            \n    print(f\"\\n{'='*25} FINAL RESULTS (v1.txt replication) {'='*25}\")\n    print(f\"ðŸ”¥ Best RÂ²: {best_r2:.4f}\")\n    print(f\"ðŸ”¥ Best MAE: {best_mae:.4f}\")\n    print(f\"ðŸ”¥ Best LLL: {best_lll:.4f}\")\n    \n    print(\"\\n--- Baseline Comparison ---\")\n    print(f\"Baseline RÂ²: {baseline_r2:.4f}\")\n    print(f\"Model RÂ²:    {best_r2:.4f}\")\n    print(f\"Improvement: {best_r2 - baseline_r2:.4f}\")\n\nif __name__ == \"__main__\":\n    run()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# 7. MAIN EXECUTION (Single Split)\n# ============================================================================\n\ndef run():\n    df, targets, baseline_df, tabular_dim = prepare_data()\n    \n    patients = baseline_df['Patient'].values\n    bins = baseline_df['decay_bins'].values\n    \n    # --- Create the v1.txt split ---\n    train_p, val_p = train_test_split(\n        patients, \n        test_size=config.TEST_SIZE, \n        random_state=config.SEED, \n        stratify=bins\n    )\n    \n    print(f\"\\n--- Single Split (v1.txt replication) ---\")\n    print(f\"Train: {len(train_p)} patients, Val: {len(val_p)} patients\")\n    \n    # --- 1. Clinical Baseline (XGBoost) ---\n    print(\"\\n--- Training Clinical Baseline (XGBoost) ---\")\n    \n    # --- *** FIX HERE *** ---\n    # `targets[p][1]` is tabular_features\n    # `targets[p][0]` is slope\n    X_train = np.array([targets[p][1] for p in train_p])\n    y_train = np.array([targets[p][0] for p in train_p])\n    X_val = np.array([targets[p][1] for p in val_p])\n    y_val = np.array([targets[p][0] for p in val_p])\n    \n    xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, \n                                 early_stopping_rounds=10, random_state=config.SEED)\n    xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n    y_pred = xgb_model.predict(X_val)\n    \n    baseline_r2 = r2_score(y_val, y_pred)\n    baseline_mae = mean_absolute_error(y_val, y_pred)\n    print(f\"Baseline XGBoost RÂ²: {baseline_r2:.4f}, MAE: {baseline_mae:.4f}\")\n    \n    # --- 2. Deep Learning Model ---\n    print(\"\\n--- Training Full Multimodal Model (v1.txt replication) ---\")\n    \n    train_ds = OptimizedOSICDataset(train_p, targets, config.TRAIN_DIR, 'train')\n    val_ds = OptimizedOSICDataset(val_p, targets, config.TRAIN_DIR, 'val')\n    \n    train_loader = DataLoader(train_ds, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=config.NUM_WORKERS, pin_memory=True, drop_last=True)\n    val_loader = DataLoader(val_ds, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=config.NUM_WORKERS)\n    \n    model = OptimizedDenseNetModel(tabular_dim=tabular_dim).to(config.DEVICE)\n    criterion = RegressionLoss()\n    \n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), \n                                  lr=config.LR, weight_decay=config.WEIGHT_DECAY)\n    \n    # --- Use ReduceLROnPlateau as in v1.txt ---\n    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, \n                                  patience=config.SCHEDULER_PATIENCE, verbose=True)\n    scaler = GradScaler()\n    \n    best_r2 = -float('inf')\n    best_mae = float('inf')\n    best_lll = -float('inf')\n    patience_counter = 0\n    \n    for epoch in range(config.EPOCHS):\n        print(f\"\\nEpoch {epoch+1}/{config.EPOCHS}\")\n        \n        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, scaler)\n        print(f\"Train Loss: {train_loss:.4f}\")\n        \n        val_metrics = validate_one_epoch(model, val_loader)\n        \n        print(\"--- Validation Metrics ---\")\n        print(f\"  RÂ²: {val_metrics['R2']:.4f}  |  MAE: {val_metrics['MAE']:.4f}  |  RMSE: {val_metrics['RMSE']:.4f}\")\n        print(f\"  LLL: {val_metrics['LaplaceLL']:.4f}\")\n        print(\"--------------------------\")\n        \n        scheduler.step(val_metrics['R2']) \n        \n        if val_metrics['R2'] > best_r2:\n            best_r2 = val_metrics['R2']\n            best_mae = val_metrics['MAE']\n            best_lll = val_metrics['LaplaceLL']\n            print(f\"ðŸŽ¯ New Best Model! RÂ²: {best_r2:.4f}, MAE: {best_mae:.4f}, LLL: {best_lll:.4f}. Saving...\")\n            torch.save(model.state_dict(), config.OUTPUT_DIR / 'best_v10_replication_model.pth')\n            patience_counter = 0\n        else:\n            patience_counter += 1\n        \n        if patience_counter >= config.EARLY_STOP_PATIENCE:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n            \n    print(f\"\\n{'='*25} FINAL RESULTS (v1.txt replication) {'='*25}\")\n    print(f\"ðŸ”¥ Best RÂ²: {best_r2:.4f}\")\n    print(f\"ðŸ”¥ Best MAE: {best_mae:.4f}\")\n    print(f\"ðŸ”¥ Best LLL: {best_lll:.4f}\")\n    \n    print(\"\\n--- Baseline Comparison ---\")\n    print(f\"Baseline RÂ²: {baseline_r2:.4f}\")\n    print(f\"Model RÂ²:    {best_r2:.4f}\")\n    print(f\"Improvement: {best_r2 - baseline_r2:.4f}\")\n\nif __name__ == \"__main__\":\n    run()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":20604,"databundleVersionId":1357052,"sourceType":"competition"},{"sourceId":13905913,"sourceType":"datasetVersion","datasetId":8852072},{"sourceId":662983,"sourceType":"modelInstanceVersion","modelInstanceId":501638,"modelId":516804}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# AI-Driven Early Prediction of Pulmonary Fibrosis Using Deep Learning\n","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n# ==========================================\n# 1. CONFIGURATION\n# ==========================================\nCONFIG = {\n    \"lr\": 2e-3,\n    \"weight_decay\": 1e-4,\n    \"batch_size\": 128,\n    \"epochs\": 200,\n    \"n_folds\": 5,\n    \"quantiles\": [0.2, 0.5, 0.8], \n    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n    \"data_dir\": \"../input/osic-pulmonary-fibrosis-progression\",\n    # Point to your extracted features file\n    \"biomarker_path\": \"../input/feature-extraction-u-net-segmentation/master_dataset.csv\"\n}\n\n# ==========================================\n# 2. DATA PREPROCESSING (ROBUST MERGE)\n# ==========================================\ndef preprocess_data(config):\n    global TARGET_SCALER\n    \n    # 1. Load Clinical Data (Guarantees Sex, Age, Weeks exist)\n    clinical_df = pd.read_csv(f\"{config['data_dir']}/train.csv\")\n    \n    # 2. Load Biomarkers\n    biomarkers_df = pd.read_csv(config['biomarker_path'])\n    \n    # 3. Robust Merge Logic\n    # We only want the image features from the biomarkers file to avoid duplicates\n    image_features = [\n        'lung_vol_ml', 'hu_mean', 'hu_std', 'hu_skew', 'hu_kurt',\n        'glcm_contrast', 'glcm_homogeneity', 'glcm_energy', 'glcm_correlation'\n    ]\n    \n    # Filter biomarkers_df to only keep Patient + Image Features\n    # This prevents 'Sex_x', 'Sex_y' conflicts if columns repeat\n    cols_to_keep = ['Patient'] + [c for c in image_features if c in biomarkers_df.columns]\n    \n    # Remove duplicates in biomarkers (keep 1 row per patient)\n    biomarkers_clean = biomarkers_df[cols_to_keep].drop_duplicates(subset=['Patient'])\n    \n    # MERGE: Clinical (Left) + Biomarkers (Right)\n    train = clinical_df.merge(biomarkers_clean, on='Patient', how='inner')\n    \n    # 4. Feature Engineering (Baseline & Relative Weeks)\n    train['Weeks'] = train['Weeks'].astype(int)\n    train.sort_values(['Patient', 'Weeks'], inplace=True)\n    \n    baseline = train.groupby('Patient').first().reset_index()\n    baseline = baseline[['Patient', 'FVC', 'Percent']].rename(columns={'FVC': 'Base_FVC', 'Percent': 'Base_Percent'})\n    train = train.merge(baseline, on='Patient', how='left')\n    \n    base_weeks = train.groupby('Patient')['Weeks'].min().reset_index().rename(columns={'Weeks': 'Base_Week'})\n    train = train.merge(base_weeks, on='Patient', how='left')\n    train['Relative_Weeks'] = train['Weeks'] - train['Base_Week']\n    \n    # 5. Feature Scaling\n    scaler = RobustScaler()\n    # Dynamic check for available columns\n    available_img_feats = [c for c in image_features if c in train.columns]\n    \n    num_cols = ['Age', 'Base_FVC', 'Base_Percent', 'Relative_Weeks'] + available_img_feats\n    train[num_cols] = scaler.fit_transform(train[num_cols])\n    \n    # 6. Encoding (Now guaranteed to work)\n    train['Sex'] = train['Sex'].apply(lambda x: 1 if x == 'Male' else 0)\n    train['Smk_Ex'] = train['SmokingStatus'].apply(lambda x: 1 if x == 'Ex-smoker' else 0)\n    train['Smk_Cur'] = train['SmokingStatus'].apply(lambda x: 1 if x == 'Currently smokes' else 0)\n    \n    feature_cols = num_cols + ['Sex', 'Smk_Ex', 'Smk_Cur']\n    \n    # 7. Target Scaling\n    TARGET_SCALER.fit(train[['FVC']])\n    train['FVC_scaled'] = TARGET_SCALER.transform(train[['FVC']])\n    \n    print(f\"âœ… Preprocessing Complete. Final Shape: {train.shape}\")\n    print(f\"âœ… Features Used: {len(feature_cols)}\")\n    \n    return train, feature_cols\n\n# ==========================================\n# 3. MODEL: QUANTILE MLP\n# ==========================================\nclass QuantileMLP(nn.Module):\n    def __init__(self, input_dim, quantiles):\n        super().__init__()\n        hidden = 128\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden), nn.BatchNorm1d(hidden), nn.LeakyReLU(0.1), nn.Dropout(0.3),\n            nn.Linear(hidden, hidden), nn.BatchNorm1d(hidden), nn.LeakyReLU(0.1), nn.Dropout(0.3),\n            nn.Linear(hidden, len(quantiles))\n        )\n    def forward(self, x): return self.net(x)\n\ndef quantile_loss(preds, target, quantiles):\n    \"\"\"Pinball Loss on SCALED targets.\"\"\"\n    assert not target.requires_grad\n    losses = []\n    for i, q in enumerate(quantiles):\n        errors = target - preds[:, i].unsqueeze(1)\n        loss = torch.max((q-1) * errors, q * errors)\n        losses.append(loss)\n    return torch.mean(torch.sum(torch.cat(losses, dim=1), dim=1))\n\n# ==========================================\n# 4. METRIC CALCULATION\n# ==========================================\ndef calculate_metrics(y_true, q_preds_real):\n    \"\"\"\n    Calculates metrics on UNSCALED (Real) values.\n    \"\"\"\n    q20 = q_preds_real[:, 0]\n    q50 = q_preds_real[:, 1]\n    q80 = q_preds_real[:, 2]\n    \n    # Sigma (Uncertainty)\n    sigma = q80 - q20\n    sigma_clipped = np.maximum(sigma, 70)\n    \n    # LLL\n    delta = np.minimum(np.abs(y_true - q50), 1000)\n    lll = - (np.sqrt(2) * delta / sigma_clipped) - np.log(np.sqrt(2) * sigma_clipped)\n    \n    # Regression Metrics\n    mse = mean_squared_error(y_true, q50)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(y_true, q50)\n    r2 = r2_score(y_true, q50)\n    rmae = mae / (np.mean(np.abs(y_true)) + 1e-6)\n    \n    return np.mean(lll), mse, rmse, mae, rmae, r2\n\n# ==========================================\n# 5. TRAINING LOOP\n# ==========================================\ndef train_model():\n    df, features = preprocess_data(CONFIG)\n    patients = df['Patient'].unique()\n    kf = KFold(n_splits=CONFIG['n_folds'], shuffle=True, random_state=42)\n    \n    global_trues, global_preds = [], []\n    \n    print(f\"ðŸš€ Training Direct FVC Quantile Regression on {len(df)} visits.\")\n    print(f\"Features: {features}\")\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(patients)):\n        train_p, val_p = patients[train_idx], patients[val_idx]\n        train_data = df[df['Patient'].isin(train_p)]\n        val_data = df[df['Patient'].isin(val_p)]\n        \n        # Train on SCALED FVC\n        X_train = torch.tensor(train_data[features].values, dtype=torch.float32).to(CONFIG['device'])\n        y_train_scaled = torch.tensor(train_data['FVC_scaled'].values, dtype=torch.float32).unsqueeze(1).to(CONFIG['device'])\n        \n        X_val = torch.tensor(val_data[features].values, dtype=torch.float32).to(CONFIG['device'])\n        \n        # Validation on RAW FVC (for final metrics)\n        y_val_real = val_data['FVC'].values\n        \n        model = QuantileMLP(len(features), CONFIG['quantiles']).to(CONFIG['device'])\n        optimizer = optim.AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=CONFIG['weight_decay'])\n        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['epochs'], eta_min=1e-6)\n        \n        best_lll = -float('inf')\n        best_preds_real = None\n        \n        for epoch in range(CONFIG['epochs']):\n            model.train()\n            optimizer.zero_grad()\n            preds_scaled = model(X_train)\n            \n            # Loss on SCALED targets\n            loss = quantile_loss(preds_scaled, y_train_scaled, CONFIG['quantiles'])\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            \n            # Validation\n            model.eval()\n            with torch.no_grad():\n                val_preds_scaled = model(X_val)\n                \n                # UNSCALE Predictions for metrics\n                val_preds_real = TARGET_SCALER.inverse_transform(val_preds_scaled.cpu().numpy())\n                \n                lll, _, _, _, _, _ = calculate_metrics(y_val_real, val_preds_real)\n                \n            if lll > best_lll:\n                best_lll = lll\n                best_preds_real = val_preds_real\n                \n        print(f\"Fold {fold+1} Best | LLL: {best_lll:.4f}\")\n        \n        global_trues.extend(y_val_real)\n        global_preds.extend(best_preds_real)\n\n    # --- FINAL SCORE ---\n    g_true = np.array(global_trues)\n    g_pred = np.array(global_preds) \n    \n    lll, mse, rmse, mae, rmae, r2 = calculate_metrics(g_true, g_pred)\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"ðŸ† FINAL DIRECT FVC QUANTILE RESULTS\")\n    print(\"=\"*50)\n    print(f\"RÂ² (FVC Correlation) : {r2:.4f}   (Target > 0.88)\")\n    print(f\"MSE (mLÂ²)            : {mse:.4f}\")\n    print(f\"RMSE (mL)            : {rmse:.4f}  (Target < 170)\")\n    print(f\"MAE (mL)             : {mae:.4f}\")\n    print(f\"RMAE (Relative Error): {rmae:.4f}\")\n    print(f\"LLL (OSIC Metric)    : {lll:.4f}   (Target > -6.64)\")\n    print(\"=\"*50)\n\nif __name__ == \"__main__\":\n    train_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T09:16:27.892078Z","iopub.execute_input":"2025-11-28T09:16:27.892768Z","iopub.status.idle":"2025-11-28T09:16:32.464549Z","shell.execute_reply.started":"2025-11-28T09:16:27.892743Z","shell.execute_reply":"2025-11-28T09:16:32.463767Z"}},"outputs":[{"name":"stdout","text":"âœ… Preprocessing Complete. Final Shape: (1549, 23)\nâœ… Features Used: 16\nðŸš€ Training Direct FVC Quantile Regression on 1549 visits.\nFeatures: ['Age', 'Base_FVC', 'Base_Percent', 'Relative_Weeks', 'lung_vol_ml', 'hu_mean', 'hu_std', 'hu_skew', 'hu_kurt', 'glcm_contrast', 'glcm_homogeneity', 'glcm_energy', 'glcm_correlation', 'Sex', 'Smk_Ex', 'Smk_Cur']\nFold 1 Best | LLL: -6.8408\nFold 2 Best | LLL: -6.9963\nFold 3 Best | LLL: -6.8772\nFold 4 Best | LLL: -6.9388\nFold 5 Best | LLL: -6.8931\n\n==================================================\nðŸ† FINAL DIRECT FVC QUANTILE RESULTS\n==================================================\nRÂ² (FVC Correlation) : 0.9049   (Target > 0.88)\nMSE (mLÂ²)            : 65878.0976\nRMSE (mL)            : 256.6673  (Target < 170)\nMAE (mL)             : 185.8179\nRMAE (Relative Error): 0.0691\nLLL (OSIC Metric)    : -6.9088   (Target > -6.64)\n==================================================\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## Hurray","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
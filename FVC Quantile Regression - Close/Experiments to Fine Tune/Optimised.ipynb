{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":20604,"databundleVersionId":1357052,"sourceType":"competition"},{"sourceId":13905913,"sourceType":"datasetVersion","datasetId":8852072},{"sourceId":662983,"sourceType":"modelInstanceVersion","modelInstanceId":501638,"modelId":516804}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# AI-Driven Early Prediction of Pulmonary Fibrosis Using Deep Learning\n","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n# ==========================================\n# 1. CONFIGURATION\n# ==========================================\nCONFIG = {\n    \"lr\": 2e-3,\n    \"weight_decay\": 1e-4,\n    \"batch_size\": 128,\n    \"epochs\": 200,\n    \"n_folds\": 5,\n    \"quantiles\": [0.2, 0.5, 0.8], \n    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n    \"data_dir\": \"../input/osic-pulmonary-fibrosis-progression\",\n    # Point to your extracted features file\n    \"biomarker_path\": \"../input/feature-extraction-u-net-segmentation/master_dataset.csv\"\n}\n\n# ==========================================\n# 2. DATA PREPROCESSING (ROBUST MERGE)\n# ==========================================\ndef preprocess_data(config):\n    global TARGET_SCALER\n    \n    # 1. Load Clinical Data (Guarantees Sex, Age, Weeks exist)\n    clinical_df = pd.read_csv(f\"{config['data_dir']}/train.csv\")\n    \n    # 2. Load Biomarkers\n    biomarkers_df = pd.read_csv(config['biomarker_path'])\n    \n    # 3. Robust Merge Logic\n    # We only want the image features from the biomarkers file to avoid duplicates\n    image_features = [\n        'lung_vol_ml', 'hu_mean', 'hu_std', 'hu_skew', 'hu_kurt',\n        'glcm_contrast', 'glcm_homogeneity', 'glcm_energy', 'glcm_correlation'\n    ]\n    \n    # Filter biomarkers_df to only keep Patient + Image Features\n    # This prevents 'Sex_x', 'Sex_y' conflicts if columns repeat\n    cols_to_keep = ['Patient'] + [c for c in image_features if c in biomarkers_df.columns]\n    \n    # Remove duplicates in biomarkers (keep 1 row per patient)\n    biomarkers_clean = biomarkers_df[cols_to_keep].drop_duplicates(subset=['Patient'])\n    \n    # MERGE: Clinical (Left) + Biomarkers (Right)\n    train = clinical_df.merge(biomarkers_clean, on='Patient', how='inner')\n    \n    # 4. Feature Engineering (Baseline & Relative Weeks)\n    train['Weeks'] = train['Weeks'].astype(int)\n    train.sort_values(['Patient', 'Weeks'], inplace=True)\n    \n    baseline = train.groupby('Patient').first().reset_index()\n    baseline = baseline[['Patient', 'FVC', 'Percent']].rename(columns={'FVC': 'Base_FVC', 'Percent': 'Base_Percent'})\n    train = train.merge(baseline, on='Patient', how='left')\n    \n    base_weeks = train.groupby('Patient')['Weeks'].min().reset_index().rename(columns={'Weeks': 'Base_Week'})\n    train = train.merge(base_weeks, on='Patient', how='left')\n    train['Relative_Weeks'] = train['Weeks'] - train['Base_Week']\n    \n    # 5. Feature Scaling\n    scaler = RobustScaler()\n    # Dynamic check for available columns\n    available_img_feats = [c for c in image_features if c in train.columns]\n    \n    num_cols = ['Age', 'Base_FVC', 'Base_Percent', 'Relative_Weeks'] + available_img_feats\n    train[num_cols] = scaler.fit_transform(train[num_cols])\n    \n    # 6. Encoding (Now guaranteed to work)\n    train['Sex'] = train['Sex'].apply(lambda x: 1 if x == 'Male' else 0)\n    train['Smk_Ex'] = train['SmokingStatus'].apply(lambda x: 1 if x == 'Ex-smoker' else 0)\n    train['Smk_Cur'] = train['SmokingStatus'].apply(lambda x: 1 if x == 'Currently smokes' else 0)\n    \n    feature_cols = num_cols + ['Sex', 'Smk_Ex', 'Smk_Cur']\n    \n    # 7. Target Scaling\n    TARGET_SCALER.fit(train[['FVC']])\n    train['FVC_scaled'] = TARGET_SCALER.transform(train[['FVC']])\n    \n    print(f\"‚úÖ Preprocessing Complete. Final Shape: {train.shape}\")\n    print(f\"‚úÖ Features Used: {len(feature_cols)}\")\n    \n    return train, feature_cols\n\n# ==========================================\n# 3. MODEL: QUANTILE MLP\n# ==========================================\nclass QuantileMLP(nn.Module):\n    def __init__(self, input_dim, quantiles):\n        super().__init__()\n        hidden = 128\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden), nn.BatchNorm1d(hidden), nn.LeakyReLU(0.1), nn.Dropout(0.3),\n            nn.Linear(hidden, hidden), nn.BatchNorm1d(hidden), nn.LeakyReLU(0.1), nn.Dropout(0.3),\n            nn.Linear(hidden, len(quantiles))\n        )\n    def forward(self, x): return self.net(x)\n\ndef quantile_loss(preds, target, quantiles):\n    \"\"\"Pinball Loss on SCALED targets.\"\"\"\n    assert not target.requires_grad\n    losses = []\n    for i, q in enumerate(quantiles):\n        errors = target - preds[:, i].unsqueeze(1)\n        loss = torch.max((q-1) * errors, q * errors)\n        losses.append(loss)\n    return torch.mean(torch.sum(torch.cat(losses, dim=1), dim=1))\n\n# ==========================================\n# 4. METRIC CALCULATION\n# ==========================================\ndef calculate_metrics(y_true, q_preds_real):\n    \"\"\"\n    Calculates metrics on UNSCALED (Real) values.\n    \"\"\"\n    q20 = q_preds_real[:, 0]\n    q50 = q_preds_real[:, 1]\n    q80 = q_preds_real[:, 2]\n    \n    # Sigma (Uncertainty)\n    sigma = q80 - q20\n    sigma_clipped = np.maximum(sigma, 70)\n    \n    # LLL\n    delta = np.minimum(np.abs(y_true - q50), 1000)\n    lll = - (np.sqrt(2) * delta / sigma_clipped) - np.log(np.sqrt(2) * sigma_clipped)\n    \n    # Regression Metrics\n    mse = mean_squared_error(y_true, q50)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(y_true, q50)\n    r2 = r2_score(y_true, q50)\n    rmae = mae / (np.mean(np.abs(y_true)) + 1e-6)\n    \n    return np.mean(lll), mse, rmse, mae, rmae, r2\n\n# ==========================================\n# 5. TRAINING LOOP\n# ==========================================\ndef train_model():\n    df, features = preprocess_data(CONFIG)\n    patients = df['Patient'].unique()\n    kf = KFold(n_splits=CONFIG['n_folds'], shuffle=True, random_state=42)\n    \n    global_trues, global_preds = [], []\n    \n    print(f\"üöÄ Training Direct FVC Quantile Regression on {len(df)} visits.\")\n    print(f\"Features: {features}\")\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(patients)):\n        train_p, val_p = patients[train_idx], patients[val_idx]\n        train_data = df[df['Patient'].isin(train_p)]\n        val_data = df[df['Patient'].isin(val_p)]\n        \n        # Train on SCALED FVC\n        X_train = torch.tensor(train_data[features].values, dtype=torch.float32).to(CONFIG['device'])\n        y_train_scaled = torch.tensor(train_data['FVC_scaled'].values, dtype=torch.float32).unsqueeze(1).to(CONFIG['device'])\n        \n        X_val = torch.tensor(val_data[features].values, dtype=torch.float32).to(CONFIG['device'])\n        \n        # Validation on RAW FVC (for final metrics)\n        y_val_real = val_data['FVC'].values\n        \n        model = QuantileMLP(len(features), CONFIG['quantiles']).to(CONFIG['device'])\n        optimizer = optim.AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=CONFIG['weight_decay'])\n        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['epochs'], eta_min=1e-6)\n        \n        best_lll = -float('inf')\n        best_preds_real = None\n        \n        for epoch in range(CONFIG['epochs']):\n            model.train()\n            optimizer.zero_grad()\n            preds_scaled = model(X_train)\n            \n            # Loss on SCALED targets\n            loss = quantile_loss(preds_scaled, y_train_scaled, CONFIG['quantiles'])\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            \n            # Validation\n            model.eval()\n            with torch.no_grad():\n                val_preds_scaled = model(X_val)\n                \n                # UNSCALE Predictions for metrics\n                val_preds_real = TARGET_SCALER.inverse_transform(val_preds_scaled.cpu().numpy())\n                \n                lll, _, _, _, _, _ = calculate_metrics(y_val_real, val_preds_real)\n                \n            if lll > best_lll:\n                best_lll = lll\n                best_preds_real = val_preds_real\n                \n        print(f\"Fold {fold+1} Best | LLL: {best_lll:.4f}\")\n        \n        global_trues.extend(y_val_real)\n        global_preds.extend(best_preds_real)\n\n    # --- FINAL SCORE ---\n    g_true = np.array(global_trues)\n    g_pred = np.array(global_preds) \n    \n    lll, mse, rmse, mae, rmae, r2 = calculate_metrics(g_true, g_pred)\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"üèÜ FINAL DIRECT FVC QUANTILE RESULTS\")\n    print(\"=\"*50)\n    print(f\"R¬≤ (FVC Correlation) : {r2:.4f}   (Target > 0.88)\")\n    print(f\"MSE (mL¬≤)            : {mse:.4f}\")\n    print(f\"RMSE (mL)            : {rmse:.4f}  (Target < 170)\")\n    print(f\"MAE (mL)             : {mae:.4f}\")\n    print(f\"RMAE (Relative Error): {rmae:.4f}\")\n    print(f\"LLL (OSIC Metric)    : {lll:.4f}   (Target > -6.64)\")\n    print(\"=\"*50)\n\nif __name__ == \"__main__\":\n    train_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T11:03:49.355005Z","iopub.execute_input":"2025-11-28T11:03:49.355445Z","iopub.status.idle":"2025-11-28T11:03:53.547772Z","shell.execute_reply.started":"2025-11-28T11:03:49.355419Z","shell.execute_reply":"2025-11-28T11:03:53.547103Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Preprocessing Complete. Final Shape: (1549, 23)\n‚úÖ Features Used: 16\nüöÄ Training Direct FVC Quantile Regression on 1549 visits.\nFeatures: ['Age', 'Base_FVC', 'Base_Percent', 'Relative_Weeks', 'lung_vol_ml', 'hu_mean', 'hu_std', 'hu_skew', 'hu_kurt', 'glcm_contrast', 'glcm_homogeneity', 'glcm_energy', 'glcm_correlation', 'Sex', 'Smk_Ex', 'Smk_Cur']\nFold 1 Best | LLL: -6.8668\nFold 2 Best | LLL: -7.0244\nFold 3 Best | LLL: -6.9294\nFold 4 Best | LLL: -6.8974\nFold 5 Best | LLL: -6.8904\n\n==================================================\nüèÜ FINAL DIRECT FVC QUANTILE RESULTS\n==================================================\nR¬≤ (FVC Correlation) : 0.9014   (Target > 0.88)\nMSE (mL¬≤)            : 68311.0493\nRMSE (mL)            : 261.3638  (Target < 170)\nMAE (mL)             : 184.9357\nRMAE (Relative Error): 0.0687\nLLL (OSIC Metric)    : -6.9214   (Target > -6.64)\n==================================================\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Hurray","metadata":{}},{"cell_type":"code","source":"‚úÖ Preprocessing Complete. Final Shape: (1549, 23)\n‚úÖ Features Used: 16\nüöÄ Training Direct FVC Quantile Regression on 1549 visits.\nFeatures: ['Age', 'Base_FVC', 'Base_Percent', 'Relative_Weeks', 'lung_vol_ml', 'hu_mean', 'hu_std', 'hu_skew', 'hu_kurt', 'glcm_contrast', 'glcm_homogeneity', 'glcm_energy', 'glcm_correlation', 'Sex', 'Smk_Ex', 'Smk_Cur']\nFold 1 Best | LLL: -6.8408\nFold 2 Best | LLL: -6.9963\nFold 3 Best | LLL: -6.8772\nFold 4 Best | LLL: -6.9388\nFold 5 Best | LLL: -6.8931\n\n==================================================\nüèÜ FINAL DIRECT FVC QUANTILE RESULTS\n==================================================\nR¬≤ (FVC Correlation) : 0.9049   (Target > 0.88)\nMSE (mL¬≤)            : 65878.0976\nRMSE (mL)            : 256.6673  (Target < 170)\nMAE (mL)             : 185.8179\nRMAE (Relative Error): 0.0691\nLLL (OSIC Metric)    : -6.9088   (Target > -6.64)\n==================================================","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Phase 2 - Fine Tunining","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n# ==========================================\n# 1. ENHANCED CONFIGURATION\n# ==========================================\nCONFIG = {\n    \"lr\": 1.5e-3,  # Slightly reduced for stability\n    \"weight_decay\": 5e-5,  # Reduced regularization\n    \"batch_size\": 64,  # Smaller batches for better generalization\n    \"epochs\": 300,  # More epochs with patience\n    \"n_folds\": 5,\n    \"quantiles\": [0.2, 0.5, 0.8], \n    \"patience\": 50,  # Early stopping\n    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n    \"data_dir\": \"../input/osic-pulmonary-fibrosis-progression\",\n    \"biomarker_path\": \"../input/feature-extraction-u-net-segmentation/master_dataset.csv\",\n    \"seed\": 42\n}\n\n# Set seeds for reproducibility\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(CONFIG['seed'])\n\n# ==========================================\n# 2. ENHANCED DATA PREPROCESSING\n# ==========================================\ndef preprocess_data(config):\n    # 1. Load Clinical Data\n    clinical_df = pd.read_csv(f\"{config['data_dir']}/train.csv\")\n    \n    # 2. Load Biomarkers\n    biomarkers_df = pd.read_csv(config['biomarker_path'])\n    \n    # 3. Robust Merge Logic\n    image_features = [\n        'lung_vol_ml', 'hu_mean', 'hu_std', 'hu_skew', 'hu_kurt',\n        'glcm_contrast', 'glcm_homogeneity', 'glcm_energy', 'glcm_correlation'\n    ]\n    \n    cols_to_keep = ['Patient'] + [c for c in image_features if c in biomarkers_df.columns]\n    biomarkers_clean = biomarkers_df[cols_to_keep].drop_duplicates(subset=['Patient'])\n    \n    train = clinical_df.merge(biomarkers_clean, on='Patient', how='inner')\n    \n    # 4. Enhanced Feature Engineering\n    train['Weeks'] = train['Weeks'].astype(int)\n    train.sort_values(['Patient', 'Weeks'], inplace=True)\n    \n    # Baseline features\n    baseline = train.groupby('Patient').first().reset_index()\n    baseline = baseline[['Patient', 'FVC', 'Percent']].rename(\n        columns={'FVC': 'Base_FVC', 'Percent': 'Base_Percent'}\n    )\n    train = train.merge(baseline, on='Patient', how='left')\n    \n    base_weeks = train.groupby('Patient')['Weeks'].min().reset_index().rename(\n        columns={'Weeks': 'Base_Week'}\n    )\n    train = train.merge(base_weeks, on='Patient', how='left')\n    train['Relative_Weeks'] = train['Weeks'] - train['Base_Week']\n    \n    # NEW: Add interaction features (critical for reducing RMSE)\n    train['FVC_Week_Interaction'] = train['Base_FVC'] * train['Relative_Weeks']\n    train['Age_Week_Interaction'] = train['Age'] * train['Relative_Weeks']\n    \n    available_img_feats = [c for c in image_features if c in train.columns]\n    \n    # NEW: Add lung volume decline rate proxy\n    if 'lung_vol_ml' in train.columns:\n        train['LungVol_FVC_Ratio'] = train['lung_vol_ml'] / (train['Base_FVC'] + 1e-6)\n    \n    # 5. Feature Scaling (Using StandardScaler for better tail behavior)\n    scaler = StandardScaler()\n    \n    num_cols = ['Age', 'Base_FVC', 'Base_Percent', 'Relative_Weeks'] + available_img_feats\n    interaction_cols = ['FVC_Week_Interaction', 'Age_Week_Interaction']\n    \n    if 'LungVol_FVC_Ratio' in train.columns:\n        interaction_cols.append('LungVol_FVC_Ratio')\n    \n    all_num_cols = num_cols + interaction_cols\n    train[all_num_cols] = scaler.fit_transform(train[all_num_cols])\n    \n    # 6. Encoding\n    train['Sex'] = train['Sex'].apply(lambda x: 1 if x == 'Male' else 0)\n    train['Smk_Ex'] = train['SmokingStatus'].apply(lambda x: 1 if x == 'Ex-smoker' else 0)\n    train['Smk_Cur'] = train['SmokingStatus'].apply(lambda x: 1 if x == 'Currently smokes' else 0)\n    \n    feature_cols = all_num_cols + ['Sex', 'Smk_Ex', 'Smk_Cur']\n    \n    # 7. Target Scaling (Use StandardScaler for FVC)\n    target_scaler = StandardScaler()\n    target_scaler.fit(train[['FVC']])\n    train['FVC_scaled'] = target_scaler.transform(train[['FVC']])\n    \n    print(f\"‚úÖ Preprocessing Complete. Final Shape: {train.shape}\")\n    print(f\"‚úÖ Features Used: {len(feature_cols)}\")\n    \n    return train, feature_cols, target_scaler\n\n# ==========================================\n# 3. IMPROVED MODEL: DEEPER QUANTILE MLP\n# ==========================================\nclass EnhancedQuantileMLP(nn.Module):\n    def __init__(self, input_dim, quantiles, dropout=0.25):\n        super().__init__()\n        # Wider and deeper architecture\n        h1, h2, h3 = 256, 128, 64\n        \n        self.net = nn.Sequential(\n            nn.Linear(input_dim, h1),\n            nn.BatchNorm1d(h1),\n            nn.LeakyReLU(0.1),\n            nn.Dropout(dropout),\n            \n            nn.Linear(h1, h2),\n            nn.BatchNorm1d(h2),\n            nn.LeakyReLU(0.1),\n            nn.Dropout(dropout),\n            \n            nn.Linear(h2, h3),\n            nn.BatchNorm1d(h3),\n            nn.LeakyReLU(0.1),\n            nn.Dropout(dropout),\n            \n            nn.Linear(h3, len(quantiles))\n        )\n    \n    def forward(self, x):\n        return self.net(x)\n\ndef quantile_loss(preds, target, quantiles):\n    \"\"\"Pinball Loss on SCALED targets.\"\"\"\n    assert not target.requires_grad\n    losses = []\n    for i, q in enumerate(quantiles):\n        errors = target - preds[:, i].unsqueeze(1)\n        loss = torch.max((q-1) * errors, q * errors)\n        losses.append(loss)\n    return torch.mean(torch.sum(torch.cat(losses, dim=1), dim=1))\n\n# ==========================================\n# 4. METRIC CALCULATION (UNCHANGED)\n# ==========================================\ndef calculate_metrics(y_true, q_preds_real):\n    q20 = q_preds_real[:, 0]\n    q50 = q_preds_real[:, 1]\n    q80 = q_preds_real[:, 2]\n    \n    sigma = q80 - q20\n    sigma_clipped = np.maximum(sigma, 70)\n    \n    delta = np.minimum(np.abs(y_true - q50), 1000)\n    lll = - (np.sqrt(2) * delta / sigma_clipped) - np.log(np.sqrt(2) * sigma_clipped)\n    \n    mse = mean_squared_error(y_true, q50)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(y_true, q50)\n    r2 = r2_score(y_true, q50)\n    rmae = mae / (np.mean(np.abs(y_true)) + 1e-6)\n    \n    return np.mean(lll), mse, rmse, mae, rmae, r2\n\n# ==========================================\n# 5. ENHANCED TRAINING LOOP\n# ==========================================\ndef train_model():\n    df, features, target_scaler = preprocess_data(CONFIG)\n    patients = df['Patient'].unique()\n    kf = KFold(n_splits=CONFIG['n_folds'], shuffle=True, random_state=CONFIG['seed'])\n    \n    global_trues, global_preds = [], []\n    fold_models = []\n    \n    print(f\"üöÄ Training Enhanced Quantile Regression on {len(df)} visits.\")\n    print(f\"Features ({len(features)}): {features[:5]}... (showing first 5)\")\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(patients)):\n        train_p, val_p = patients[train_idx], patients[val_idx]\n        train_data = df[df['Patient'].isin(train_p)]\n        val_data = df[df['Patient'].isin(val_p)]\n        \n        X_train = torch.tensor(train_data[features].values, dtype=torch.float32).to(CONFIG['device'])\n        y_train_scaled = torch.tensor(train_data['FVC_scaled'].values, dtype=torch.float32).unsqueeze(1).to(CONFIG['device'])\n        \n        X_val = torch.tensor(val_data[features].values, dtype=torch.float32).to(CONFIG['device'])\n        y_val_real = val_data['FVC'].values\n        \n        model = EnhancedQuantileMLP(len(features), CONFIG['quantiles']).to(CONFIG['device'])\n        optimizer = optim.AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=CONFIG['weight_decay'])\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode='max', factor=0.5, patience=20, verbose=False\n        )\n        \n        best_lll = -float('inf')\n        best_preds_real = None\n        patience_counter = 0\n        \n        for epoch in range(CONFIG['epochs']):\n            model.train()\n            optimizer.zero_grad()\n            preds_scaled = model(X_train)\n            \n            loss = quantile_loss(preds_scaled, y_train_scaled, CONFIG['quantiles'])\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n            optimizer.step()\n            \n            # Validation\n            model.eval()\n            with torch.no_grad():\n                val_preds_scaled = model(X_val)\n                val_preds_real = target_scaler.inverse_transform(val_preds_scaled.cpu().numpy())\n                \n                lll, _, rmse, _, _, _ = calculate_metrics(y_val_real, val_preds_real)\n                \n            scheduler.step(lll)\n            \n            if lll > best_lll:\n                best_lll = lll\n                best_preds_real = val_preds_real\n                patience_counter = 0\n            else:\n                patience_counter += 1\n                \n            if patience_counter >= CONFIG['patience']:\n                print(f\"  Early stopping at epoch {epoch+1}\")\n                break\n                \n        print(f\"Fold {fold+1} Best | LLL: {best_lll:.4f}\")\n        \n        global_trues.extend(y_val_real)\n        global_preds.extend(best_preds_real)\n        fold_models.append(model)\n\n    # --- FINAL SCORE ---\n    g_true = np.array(global_trues)\n    g_pred = np.array(global_preds) \n    \n    lll, mse, rmse, mae, rmae, r2 = calculate_metrics(g_true, g_pred)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"üèÜ FINAL ENHANCED QUANTILE RESULTS\")\n    print(\"=\"*60)\n    print(f\"R¬≤ (FVC Correlation) : {r2:.4f}   (Target > 0.88) {'‚úÖ' if r2 > 0.88 else '‚ùå'}\")\n    print(f\"MSE (mL¬≤)            : {mse:.4f}\")\n    print(f\"RMSE (mL)            : {rmse:.4f}  (Target < 170) {'‚úÖ' if rmse < 170 else '‚ùå'}\")\n    print(f\"MAE (mL)             : {mae:.4f}\")\n    print(f\"RMAE (Relative Error): {rmae:.4f}\")\n    print(f\"LLL (OSIC Metric)    : {lll:.4f}   (Target > -6.64) {'‚úÖ' if lll > -6.64 else '‚ùå'}\")\n    print(\"=\"*60)\n    \n    return fold_models, target_scaler\n\nif __name__ == \"__main__\":\n    models, scaler = train_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T11:09:25.677948Z","iopub.execute_input":"2025-11-28T11:09:25.678639Z","iopub.status.idle":"2025-11-28T11:09:29.075012Z","shell.execute_reply.started":"2025-11-28T11:09:25.678612Z","shell.execute_reply":"2025-11-28T11:09:29.074202Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Preprocessing Complete. Final Shape: (1549, 26)\n‚úÖ Features Used: 19\nüöÄ Training Enhanced Quantile Regression on 1549 visits.\nFeatures (19): ['Age', 'Base_FVC', 'Base_Percent', 'Relative_Weeks', 'lung_vol_ml']... (showing first 5)\n  Early stopping at epoch 165\nFold 1 Best | LLL: -6.8198\n  Early stopping at epoch 103\nFold 2 Best | LLL: -6.9550\n  Early stopping at epoch 164\nFold 3 Best | LLL: -6.8752\n  Early stopping at epoch 101\nFold 4 Best | LLL: -6.9504\n  Early stopping at epoch 96\nFold 5 Best | LLL: -6.8886\n\n============================================================\nüèÜ FINAL ENHANCED QUANTILE RESULTS\n============================================================\nR¬≤ (FVC Correlation) : 0.9132   (Target > 0.88) ‚úÖ\nMSE (mL¬≤)            : 60135.3067\nRMSE (mL)            : 245.2250  (Target < 170) ‚ùå\nMAE (mL)             : 176.6318\nRMAE (Relative Error): 0.0657\nLLL (OSIC Metric)    : -6.8973   (Target > -6.64) ‚ùå\n============================================================\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"### 1. The Diagnosis: The \"Floating Curve\" Syndrome\nYour model has learned the disease mechanism perfectly, but it has failed to learn the calibration.High $R^2$ (0.91): Your model knows exactly who is sick and who is getting worse. If Patient A drops 50mL and Patient B drops 200mL, your model correctly ranks them. The U-Net features are working.High RMSE (245 vs 170): The model is \"floating.\" It predicts the correct shape of the decline, but the whole curve is shifted up or down by a constant amount (bias). It knows the slope, but it's missing the intercept.The Cause: You are feeding Baseline FVC as just another feature (like Age or Sex) into a neural network. Neural networks introduce noise. The model sees \"Baseline = 2500\" but outputs \"Prediction = 2650\" even for Week 0, because it's approximating, not calculating.The Fix: You need to force the model to respect the Anchor\n\n###  Point.2. Strategy A: Post-Processing \"Anchoring\" (The Quickest Fix)\nThis requires zero retraining. It is a mathematical adjustment applied to your final predictions.The Logic:We know for a fact that at Week 0 (Baseline), the error must be zero. Currently, your model likely has an error at Week 0.If your model predicts 2400 at Week 0, but the real value is 2500, your model has a -100 bias for that patient. It is highly probable that this -100 bias persists for Week 10, Week 50, etc.The Solution:Calculate the error your model makes at the Baseline week for each patient. Add that error back to every future prediction for that same patient. This \"shifts\" the floating curve to snap onto the known starting point. In similar competitions, this single step often drops RMSE by 30-50\n\n### points.3. Strategy B: Uncertainty \"Clipping\" (Fixing LLL)Your LLL is -6.90 (Target -6.64). \nLLL is damaged by two things: being wrong (RMSE) and being \"overconfidently wrong.\"The Logic:Your model is outputting a confidence interval ($\\sigma$). Currently, it is likely predicting very large sigmas to protect itself from the high RMSE.However, once you apply the \"Anchoring\" fix (Strategy A), your RMSE will drop. Your model doesn't know this yet, so it will still output huge, pessimistic uncertainty values.The Solution:After improving RMSE via anchoring, you must artificially tighten the confidence intervals. You can multiply your predicted $\\sigma$ (uncertainty) by a factor (e.g., $0.8$ or $0.9$). Since your predictions are now more accurate, you should claim more confidence. This will boost the LLL score directly.\n### 4. Strategy C: The \"Decay Factor\" Approach (Architectural Shift)\nIf the above post-processing doesn't fully close the gap, the issue is how the problem is framed for the Neural Network.The Logic:Currently, you ask the model: \"Here is a lung and an age. Guess the FVC in mL.\" This is hard because FVC ranges from 1,000 to 6,000.It is much easier to ask: \"Here is a lung and an age. Predict the percentage of the Baseline FVC remaining.\"The Solution:Change the target. Instead of predicting raw FVC, train the model to predict Ratio = FVC_current / FVC_baseline.This normalizes every patient to start at 1.0.The model only has to learn the decay rate (e.g., 0.98, 0.95).At inference time, you simply multiply Predicted_Ratio * Real_Baseline_FVC.This forces the model to heavily rely on the known baseline, mechanically reducing variance.\n\n### Summary of the Plan : You do not need more complex code. \nYou need Calibration.Do not retrain yet. Take your existing OOF predictions from the last run.Apply Anchoring: For every patient, calculate Shift = Real_Baseline - Predicted_Baseline. Add this Shift to all predictions. Check the new RMSE.Calibrate Sigma: If RMSE improves, multiply your uncertainty column by a scalar (try 0.9, 0.8) to maximize LLL.If this brings RMSE close to 200 but not 170, then we switch to Strategy C (Ratio Prediction) for the final model.","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n# ==========================================\n# 1. CONFIGURATION\n# ==========================================\nCONFIG = {\n    \"lr\": 1.5e-3,\n    \"weight_decay\": 5e-5,\n    \"batch_size\": 64,\n    \"epochs\": 300,\n    \"n_folds\": 5,\n    \"quantiles\": [0.2, 0.5, 0.8], \n    \"patience\": 50,\n    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n    \"data_dir\": \"../input/osic-pulmonary-fibrosis-progression\",\n    \"biomarker_path\": \"../input/feature-extraction-u-net-segmentation/master_dataset.csv\",\n    \"seed\": 42\n}\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(CONFIG['seed'])\n\n# ==========================================\n# 2. DATA PREPROCESSING\n# ==========================================\ndef preprocess_data(config):\n    clinical_df = pd.read_csv(f\"{config['data_dir']}/train.csv\")\n    biomarkers_df = pd.read_csv(config['biomarker_path'])\n    \n    image_features = [\n        'lung_vol_ml', 'hu_mean', 'hu_std', 'hu_skew', 'hu_kurt',\n        'glcm_contrast', 'glcm_homogeneity', 'glcm_energy', 'glcm_correlation'\n    ]\n    \n    cols_to_keep = ['Patient'] + [c for c in image_features if c in biomarkers_df.columns]\n    biomarkers_clean = biomarkers_df[cols_to_keep].drop_duplicates(subset=['Patient'])\n    \n    train = clinical_df.merge(biomarkers_clean, on='Patient', how='inner')\n    \n    train['Weeks'] = train['Weeks'].astype(int)\n    train.sort_values(['Patient', 'Weeks'], inplace=True)\n    \n    baseline = train.groupby('Patient').first().reset_index()\n    baseline = baseline[['Patient', 'FVC', 'Percent']].rename(\n        columns={'FVC': 'Base_FVC', 'Percent': 'Base_Percent'}\n    )\n    train = train.merge(baseline, on='Patient', how='left')\n    \n    base_weeks = train.groupby('Patient')['Weeks'].min().reset_index().rename(\n        columns={'Weeks': 'Base_Week'}\n    )\n    train = train.merge(base_weeks, on='Patient', how='left')\n    train['Relative_Weeks'] = train['Weeks'] - train['Base_Week']\n    \n    # Interaction features\n    train['FVC_Week_Interaction'] = train['Base_FVC'] * train['Relative_Weeks']\n    train['Age_Week_Interaction'] = train['Age'] * train['Relative_Weeks']\n    \n    available_img_feats = [c for c in image_features if c in train.columns]\n    \n    if 'lung_vol_ml' in train.columns:\n        train['LungVol_FVC_Ratio'] = train['lung_vol_ml'] / (train['Base_FVC'] + 1e-6)\n    \n    scaler = StandardScaler()\n    \n    num_cols = ['Age', 'Base_FVC', 'Base_Percent', 'Relative_Weeks'] + available_img_feats\n    interaction_cols = ['FVC_Week_Interaction', 'Age_Week_Interaction']\n    \n    if 'LungVol_FVC_Ratio' in train.columns:\n        interaction_cols.append('LungVol_FVC_Ratio')\n    \n    all_num_cols = num_cols + interaction_cols\n    train[all_num_cols] = scaler.fit_transform(train[all_num_cols])\n    \n    train['Sex'] = train['Sex'].apply(lambda x: 1 if x == 'Male' else 0)\n    train['Smk_Ex'] = train['SmokingStatus'].apply(lambda x: 1 if x == 'Ex-smoker' else 0)\n    train['Smk_Cur'] = train['SmokingStatus'].apply(lambda x: 1 if x == 'Currently smokes' else 0)\n    \n    feature_cols = all_num_cols + ['Sex', 'Smk_Ex', 'Smk_Cur']\n    \n    target_scaler = StandardScaler()\n    target_scaler.fit(train[['FVC']])\n    train['FVC_scaled'] = target_scaler.transform(train[['FVC']])\n    \n    print(f\"‚úÖ Preprocessing Complete. Final Shape: {train.shape}\")\n    print(f\"‚úÖ Features Used: {len(feature_cols)}\")\n    \n    return train, feature_cols, target_scaler\n\n# ==========================================\n# 3. MODEL\n# ==========================================\nclass EnhancedQuantileMLP(nn.Module):\n    def __init__(self, input_dim, quantiles, dropout=0.25):\n        super().__init__()\n        h1, h2, h3 = 256, 128, 64\n        \n        self.net = nn.Sequential(\n            nn.Linear(input_dim, h1), nn.BatchNorm1d(h1), nn.LeakyReLU(0.1), nn.Dropout(dropout),\n            nn.Linear(h1, h2), nn.BatchNorm1d(h2), nn.LeakyReLU(0.1), nn.Dropout(dropout),\n            nn.Linear(h2, h3), nn.BatchNorm1d(h3), nn.LeakyReLU(0.1), nn.Dropout(dropout),\n            nn.Linear(h3, len(quantiles))\n        )\n    \n    def forward(self, x):\n        return self.net(x)\n\ndef quantile_loss(preds, target, quantiles):\n    assert not target.requires_grad\n    losses = []\n    for i, q in enumerate(quantiles):\n        errors = target - preds[:, i].unsqueeze(1)\n        loss = torch.max((q-1) * errors, q * errors)\n        losses.append(loss)\n    return torch.mean(torch.sum(torch.cat(losses, dim=1), dim=1))\n\n# ==========================================\n# 4. METRICS\n# ==========================================\ndef calculate_metrics(y_true, q_preds_real):\n    q20 = q_preds_real[:, 0]\n    q50 = q_preds_real[:, 1]\n    q80 = q_preds_real[:, 2]\n    \n    sigma = q80 - q20\n    sigma_clipped = np.maximum(sigma, 70)\n    \n    delta = np.minimum(np.abs(y_true - q50), 1000)\n    lll = - (np.sqrt(2) * delta / sigma_clipped) - np.log(np.sqrt(2) * sigma_clipped)\n    \n    mse = mean_squared_error(y_true, q50)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(y_true, q50)\n    r2 = r2_score(y_true, q50)\n    rmae = mae / (np.mean(np.abs(y_true)) + 1e-6)\n    \n    return {\n        'lll': np.mean(lll),\n        'mse': mse,\n        'rmse': rmse,\n        'mae': mae,\n        'rmae': rmae,\n        'r2': r2\n    }\n\n# ==========================================\n# 5. üß™ EXPERIMENT 1: BASELINE ANCHORING (FIXED)\n# ==========================================\ndef apply_baseline_anchoring(df, predictions):\n    \"\"\"\n    Fixed to avoid column name collisions (KeyError: Base_Week).\n    \"\"\"\n    df_copy = df.copy()\n    preds_copy = predictions.copy()\n    \n    # Calculate fresh baseline info with UNIQUE column names\n    baseline_info = df_copy.groupby('Patient').agg({\n        'Weeks': 'min',\n        'FVC': 'first'\n    }).reset_index()\n    baseline_info.columns = ['Patient', 'Anchoring_Week', 'Anchoring_FVC']\n    \n    # Merge\n    df_copy = df_copy.merge(baseline_info, on='Patient', how='left')\n    \n    patient_shifts = {}\n    \n    for patient in df_copy['Patient'].unique():\n        patient_mask = df_copy['Patient'] == patient\n        patient_data = df_copy[patient_mask]\n        patient_preds = preds_copy[patient_mask]\n        \n        # Use the specific anchoring week column\n        baseline_mask = patient_data['Weeks'] == patient_data['Anchoring_Week'].iloc[0]\n        \n        if baseline_mask.sum() > 0:\n            baseline_pred = patient_preds[baseline_mask.values][0, 1]  # q50\n            baseline_true = patient_data[baseline_mask]['FVC'].iloc[0] # Use original FVC\n            shift = baseline_true - baseline_pred\n        else:\n            shift = 0\n        \n        patient_shifts[patient] = shift\n    \n    # Apply shifts\n    for i in range(len(df_copy)):\n        patient = df_copy.iloc[i]['Patient']\n        shift = patient_shifts.get(patient, 0)\n        preds_copy[i] += shift\n    \n    return preds_copy, patient_shifts\n\n# ==========================================\n# 6. üß™ EXPERIMENT 2: SIGMA CALIBRATION\n# ==========================================\ndef calibrate_sigma(predictions, scale_factor=0.85):\n    preds_copy = predictions.copy()\n    q20, q50, q80 = preds_copy[:, 0], preds_copy[:, 1], preds_copy[:, 2]\n    \n    new_q20 = q50 - (q50 - q20) * scale_factor\n    new_q80 = q50 + (q80 - q50) * scale_factor\n    \n    return np.column_stack([new_q20, q50, new_q80])\n\n# ==========================================\n# 7. MAIN TRAINING LOOP\n# ==========================================\ndef train_with_experiments():\n    df, features, target_scaler = preprocess_data(CONFIG)\n    patients = df['Patient'].unique()\n    kf = KFold(n_splits=CONFIG['n_folds'], shuffle=True, random_state=CONFIG['seed'])\n    \n    oof_indices = []\n    oof_raw_preds = []\n    oof_trues = []\n    \n    print(f\"\\nüöÄ Training with {len(df)} visits across {len(patients)} patients\")\n    print(f\"üìä Features: {len(features)}\")\n    print(\"=\"*80)\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(patients)):\n        print(f\"\\n{'='*80}\")\n        print(f\"FOLD {fold+1}/{CONFIG['n_folds']}\")\n        print(f\"{'='*80}\")\n        \n        train_p, val_p = patients[train_idx], patients[val_idx]\n        train_data = df[df['Patient'].isin(train_p)]\n        val_data = df[df['Patient'].isin(val_p)]\n        \n        X_train = torch.tensor(train_data[features].values, dtype=torch.float32).to(CONFIG['device'])\n        y_train_scaled = torch.tensor(train_data['FVC_scaled'].values, dtype=torch.float32).unsqueeze(1).to(CONFIG['device'])\n        \n        X_val = torch.tensor(val_data[features].values, dtype=torch.float32).to(CONFIG['device'])\n        y_val_real = val_data['FVC'].values\n        \n        model = EnhancedQuantileMLP(len(features), CONFIG['quantiles']).to(CONFIG['device'])\n        optimizer = optim.AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=CONFIG['weight_decay'])\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode='max', factor=0.5, patience=20, verbose=False\n        )\n        \n        best_lll = -float('inf')\n        best_preds_real = None\n        patience_counter = 0\n        \n        for epoch in range(CONFIG['epochs']):\n            model.train()\n            optimizer.zero_grad()\n            preds_scaled = model(X_train)\n            \n            loss = quantile_loss(preds_scaled, y_train_scaled, CONFIG['quantiles'])\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            \n            model.eval()\n            with torch.no_grad():\n                val_preds_scaled = model(X_val)\n                val_preds_real = target_scaler.inverse_transform(val_preds_scaled.cpu().numpy())\n                \n                metrics = calculate_metrics(y_val_real, val_preds_real)\n                lll = metrics['lll']\n                \n            scheduler.step(lll)\n            \n            if lll > best_lll:\n                best_lll = lll\n                best_preds_real = val_preds_real.copy()\n                patience_counter = 0\n            else:\n                patience_counter += 1\n                \n            if patience_counter >= CONFIG['patience']:\n                break\n        \n        oof_indices.extend(val_data.index.tolist())\n        oof_raw_preds.append(best_preds_real)\n        oof_trues.extend(y_val_real)\n        \n        fold_metrics = calculate_metrics(y_val_real, best_preds_real)\n        print(f\"\\nüìà FOLD {fold+1} BASELINE RESULTS:\")\n        print(f\"  R¬≤:   {fold_metrics['r2']:.4f}\")\n        print(f\"  RMSE: {fold_metrics['rmse']:.2f} mL\")\n        print(f\"  LLL:  {fold_metrics['lll']:.4f}\")\n    \n    # Combine OOF\n    oof_raw_preds = np.vstack(oof_raw_preds)\n    oof_trues = np.array(oof_trues)\n    oof_df = df.loc[oof_indices].copy()\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"üèÅ BASELINE (NO POST-PROCESSING)\")\n    print(\"=\"*80)\n    baseline_metrics = calculate_metrics(oof_trues, oof_raw_preds)\n    print(f\"R¬≤:   {baseline_metrics['r2']:.4f}   {'‚úÖ' if baseline_metrics['r2'] > 0.88 else '‚ùå'} (Target > 0.88)\")\n    print(f\"RMSE: {baseline_metrics['rmse']:.2f} mL {'‚úÖ' if baseline_metrics['rmse'] < 170 else '‚ùå'} (Target < 170)\")\n    print(f\"LLL:  {baseline_metrics['lll']:.4f}   {'‚úÖ' if baseline_metrics['lll'] > -6.64 else '‚ùå'} (Target > -6.64)\")\n    \n    # ==========================================\n    # üß™ EXPERIMENT 1: BASELINE ANCHORING\n    # ==========================================\n    print(\"\\n\" + \"=\"*80)\n    print(\"üß™ EXPERIMENT 1: BASELINE ANCHORING\")\n    print(\"=\"*80)\n    \n    # This function is now fixed!\n    anchored_preds, shifts = apply_baseline_anchoring(oof_df, oof_raw_preds)\n    anchored_metrics = calculate_metrics(oof_trues, anchored_preds)\n    \n    print(f\"Average patient shift: {np.mean(list(shifts.values())):.2f} mL\")\n    print(f\"\\nR¬≤:   {anchored_metrics['r2']:.4f}   {'‚úÖ' if anchored_metrics['r2'] > 0.88 else '‚ùå'}\")\n    print(f\"RMSE: {anchored_metrics['rmse']:.2f} mL {'‚úÖ' if anchored_metrics['rmse'] < 170 else '‚ùå'} (Œî: {anchored_metrics['rmse'] - baseline_metrics['rmse']:+.2f})\")\n    print(f\"LLL:  {anchored_metrics['lll']:.4f}   {'‚úÖ' if anchored_metrics['lll'] > -6.64 else '‚ùå'} (Œî: {anchored_metrics['lll'] - baseline_metrics['lll']:+.4f})\")\n    \n    # ==========================================\n    # üß™ EXPERIMENT 2: SIGMA CALIBRATION\n    # ==========================================\n    print(\"\\n\" + \"=\"*80)\n    print(\"üß™ EXPERIMENT 2: SIGMA CALIBRATION (After Anchoring)\")\n    print(\"=\"*80)\n    \n    best_lll = anchored_metrics['lll']\n    best_factor = 1.0\n    \n    for scale in [0.95, 0.90, 0.85, 0.80, 0.75, 0.70]:\n        calibrated = calibrate_sigma(anchored_preds, scale)\n        metrics = calculate_metrics(oof_trues, calibrated)\n        \n        improvement = \"‚úÖ\" if metrics['lll'] > best_lll else \"\"\n        print(f\"  œÉ√ó{scale:.2f} ‚Üí LLL: {metrics['lll']:.4f} (RMSE: {metrics['rmse']:.2f}) {improvement}\")\n        \n        if metrics['lll'] > best_lll:\n            best_lll = metrics['lll']\n            best_factor = scale\n    \n    # Apply best sigma calibration\n    final_preds = calibrate_sigma(anchored_preds, best_factor)\n    final_metrics = calculate_metrics(oof_trues, final_preds)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"üèÜ FINAL RESULTS (Anchoring + Best Sigma)\")\n    print(\"=\"*80)\n    print(f\"Best œÉ scale factor: {best_factor:.2f}\")\n    print(f\"\\nR¬≤:   {final_metrics['r2']:.4f}   {'‚úÖ' if final_metrics['r2'] > 0.88 else '‚ùå'} (Target > 0.88)\")\n    print(f\"RMSE: {final_metrics['rmse']:.2f} mL {'‚úÖ' if final_metrics['rmse'] < 170 else '‚ùå'} (Target < 170)\")\n    print(f\"LLL:  {final_metrics['lll']:.4f}   {'‚úÖ' if final_metrics['lll'] > -6.64 else '‚ùå'} (Target > -6.64)\")\n\nif __name__ == \"__main__\":\n    train_with_experiments()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T11:33:00.525364Z","iopub.execute_input":"2025-11-28T11:33:00.526104Z","iopub.status.idle":"2025-11-28T11:33:04.012568Z","shell.execute_reply.started":"2025-11-28T11:33:00.526081Z","shell.execute_reply":"2025-11-28T11:33:04.011696Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Preprocessing Complete. Final Shape: (1549, 26)\n‚úÖ Features Used: 19\n\nüöÄ Training with 1549 visits across 176 patients\nüìä Features: 19\n================================================================================\n\n================================================================================\nFOLD 1/5\n================================================================================\n\nüìà FOLD 1 BASELINE RESULTS:\n  R¬≤:   0.9070\n  RMSE: 239.89 mL\n  LLL:  -6.8198\n\n================================================================================\nFOLD 2/5\n================================================================================\n\nüìà FOLD 2 BASELINE RESULTS:\n  R¬≤:   0.9180\n  RMSE: 255.34 mL\n  LLL:  -6.9550\n\n================================================================================\nFOLD 3/5\n================================================================================\n\nüìà FOLD 3 BASELINE RESULTS:\n  R¬≤:   0.8814\n  RMSE: 259.07 mL\n  LLL:  -6.8752\n\n================================================================================\nFOLD 4/5\n================================================================================\n\nüìà FOLD 4 BASELINE RESULTS:\n  R¬≤:   0.8966\n  RMSE: 246.11 mL\n  LLL:  -6.9504\n\n================================================================================\nFOLD 5/5\n================================================================================\n\nüìà FOLD 5 BASELINE RESULTS:\n  R¬≤:   0.9387\n  RMSE: 224.47 mL\n  LLL:  -6.8886\n\n================================================================================\nüèÅ BASELINE (NO POST-PROCESSING)\n================================================================================\nR¬≤:   0.9132   ‚úÖ (Target > 0.88)\nRMSE: 245.23 mL ‚ùå (Target < 170)\nLLL:  -6.8973   ‚ùå (Target > -6.64)\n\n================================================================================\nüß™ EXPERIMENT 1: BASELINE ANCHORING\n================================================================================\nAverage patient shift: 14.84 mL\n\nR¬≤:   0.9292   ‚úÖ\nRMSE: 221.49 mL ‚ùå (Œî: -23.73)\nLLL:  -6.7673   ‚ùå (Œî: +0.1300)\n\n================================================================================\nüß™ EXPERIMENT 2: SIGMA CALIBRATION (After Anchoring)\n================================================================================\n  œÉ√ó0.95 ‚Üí LLL: -6.7501 (RMSE: 221.49) ‚úÖ\n  œÉ√ó0.90 ‚Üí LLL: -6.7338 (RMSE: 221.49) ‚úÖ\n  œÉ√ó0.85 ‚Üí LLL: -6.7190 (RMSE: 221.49) ‚úÖ\n  œÉ√ó0.80 ‚Üí LLL: -6.7059 (RMSE: 221.49) ‚úÖ\n  œÉ√ó0.75 ‚Üí LLL: -6.6953 (RMSE: 221.49) ‚úÖ\n  œÉ√ó0.70 ‚Üí LLL: -6.6880 (RMSE: 221.49) ‚úÖ\n\n================================================================================\nüèÜ FINAL RESULTS (Anchoring + Best Sigma)\n================================================================================\nBest œÉ scale factor: 0.70\n\nR¬≤:   0.9292   ‚úÖ (Target > 0.88)\nRMSE: 221.49 mL ‚ùå (Target < 170)\nLLL:  -6.6880   ‚ùå (Target > -6.64)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## The Diagnosis: You have solved the Disease, but not the Noise.\n## Strategy C\n### Deploy Strategy C: Ratio Prediction Architecture\nWhy this will work:\n\n#### Mathematical Constraint: \nBy predicting FVC_ratio = FVC_current / FVC_baseline, you force the model to output values near 1.0 (range: 0.7-1.1 instead of 1000-6000)\n#### Automatic Anchoring: \nAt inference, Predicted_FVC = Predicted_Ratio √ó True_Baseline_FVC - this mechanically eliminates baseline error\n#### Smaller Loss Surface: \nPredicting a ratio (normalized target) is a simpler optimization problem","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n# ==========================================\n# CONFIGURATION\n# ==========================================\nCONFIG = {\n    \"lr\": 2e-3,\n    \"weight_decay\": 3e-5,\n    \"batch_size\": 64,\n    \"epochs\": 300,\n    \"n_folds\": 5,\n    \"quantiles\": [0.2, 0.5, 0.8], \n    \"patience\": 50,\n    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n    \"data_dir\": \"../input/osic-pulmonary-fibrosis-progression\",\n    \"biomarker_path\": \"../input/feature-extraction-u-net-segmentation/master_dataset.csv\",\n    \"seed\": 42\n}\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(CONFIG['seed'])\n\n# ==========================================\n# STRATEGY C: RATIO-BASED DATA PREPROCESSING\n# ==========================================\ndef preprocess_data_ratio(config):\n    \"\"\"\n    KEY CHANGE: Target is now FVC_RATIO = Current_FVC / Baseline_FVC\n    This normalizes all patients to start at 1.0 and learn decay rates.\n    \"\"\"\n    clinical_df = pd.read_csv(f\"{config['data_dir']}/train.csv\")\n    biomarkers_df = pd.read_csv(config['biomarker_path'])\n    \n    image_features = [\n        'lung_vol_ml', 'hu_mean', 'hu_std', 'hu_skew', 'hu_kurt',\n        'glcm_contrast', 'glcm_homogeneity', 'glcm_energy', 'glcm_correlation'\n    ]\n    \n    cols_to_keep = ['Patient'] + [c for c in image_features if c in biomarkers_df.columns]\n    biomarkers_clean = biomarkers_df[cols_to_keep].drop_duplicates(subset=['Patient'])\n    \n    train = clinical_df.merge(biomarkers_clean, on='Patient', how='inner')\n    \n    train['Weeks'] = train['Weeks'].astype(int)\n    train.sort_values(['Patient', 'Weeks'], inplace=True)\n    \n    # Get baseline FVC for each patient\n    baseline = train.groupby('Patient').first().reset_index()\n    baseline = baseline[['Patient', 'FVC', 'Percent']].rename(\n        columns={'FVC': 'Base_FVC', 'Percent': 'Base_Percent'}\n    )\n    train = train.merge(baseline, on='Patient', how='left')\n    \n    base_weeks = train.groupby('Patient')['Weeks'].min().reset_index().rename(\n        columns={'Weeks': 'Base_Week'}\n    )\n    train = train.merge(base_weeks, on='Patient', how='left')\n    train['Relative_Weeks'] = train['Weeks'] - train['Base_Week']\n    \n    # ============================================\n    # üî• CRITICAL CHANGE: CREATE RATIO TARGET\n    # ============================================\n    train['FVC_Ratio'] = train['FVC'] / train['Base_FVC']\n    \n    # Sanity check: Most ratios should be 0.7-1.1 (fibrosis causes decline)\n    print(f\"üìä FVC_Ratio Distribution:\")\n    print(f\"   Mean: {train['FVC_Ratio'].mean():.3f}\")\n    print(f\"   Std:  {train['FVC_Ratio'].std():.3f}\")\n    print(f\"   Min:  {train['FVC_Ratio'].min():.3f}\")\n    print(f\"   Max:  {train['FVC_Ratio'].max():.3f}\")\n    \n    # Interaction features (same as before)\n    train['FVC_Week_Interaction'] = train['Base_FVC'] * train['Relative_Weeks']\n    train['Age_Week_Interaction'] = train['Age'] * train['Relative_Weeks']\n    \n    available_img_feats = [c for c in image_features if c in train.columns]\n    \n    if 'lung_vol_ml' in train.columns:\n        train['LungVol_FVC_Ratio'] = train['lung_vol_ml'] / (train['Base_FVC'] + 1e-6)\n    \n    # Feature scaling (EXCLUDE Base_FVC from scaling - we need its real value at inference)\n    scaler = StandardScaler()\n    \n    num_cols = ['Age', 'Base_Percent', 'Relative_Weeks'] + available_img_feats\n    interaction_cols = ['FVC_Week_Interaction', 'Age_Week_Interaction']\n    \n    if 'LungVol_FVC_Ratio' in train.columns:\n        interaction_cols.append('LungVol_FVC_Ratio')\n    \n    all_num_cols = num_cols + interaction_cols\n    train[all_num_cols] = scaler.fit_transform(train[all_num_cols])\n    \n    # Keep Base_FVC UNSCALED for reconstruction\n    train['Base_FVC_Raw'] = train['Base_FVC']\n    \n    train['Sex'] = train['Sex'].apply(lambda x: 1 if x == 'Male' else 0)\n    train['Smk_Ex'] = train['SmokingStatus'].apply(lambda x: 1 if x == 'Ex-smoker' else 0)\n    train['Smk_Cur'] = train['SmokingStatus'].apply(lambda x: 1 if x == 'Currently smokes' else 0)\n    \n    # Feature list (Base_FVC is now RAW, not scaled)\n    feature_cols = all_num_cols + ['Base_FVC_Raw', 'Sex', 'Smk_Ex', 'Smk_Cur']\n    \n    # Target scaling (RATIO instead of raw FVC)\n    ratio_scaler = StandardScaler()\n    ratio_scaler.fit(train[['FVC_Ratio']])\n    train['FVC_Ratio_Scaled'] = ratio_scaler.transform(train[['FVC_Ratio']])\n    \n    print(f\"‚úÖ Preprocessing Complete. Final Shape: {train.shape}\")\n    print(f\"‚úÖ Features Used: {len(feature_cols)}\")\n    \n    return train, feature_cols, ratio_scaler\n\n# ==========================================\n# MODEL (Same architecture)\n# ==========================================\nclass EnhancedQuantileMLP(nn.Module):\n    def __init__(self, input_dim, quantiles, dropout=0.25):\n        super().__init__()\n        h1, h2, h3 = 256, 128, 64\n        \n        self.net = nn.Sequential(\n            nn.Linear(input_dim, h1),\n            nn.BatchNorm1d(h1),\n            nn.LeakyReLU(0.1),\n            nn.Dropout(dropout),\n            \n            nn.Linear(h1, h2),\n            nn.BatchNorm1d(h2),\n            nn.LeakyReLU(0.1),\n            nn.Dropout(dropout),\n            \n            nn.Linear(h2, h3),\n            nn.BatchNorm1d(h3),\n            nn.LeakyReLU(0.1),\n            nn.Dropout(dropout),\n            \n            nn.Linear(h3, len(quantiles))\n        )\n    \n    def forward(self, x):\n        return self.net(x)\n\ndef quantile_loss(preds, target, quantiles):\n    assert not target.requires_grad\n    losses = []\n    for i, q in enumerate(quantiles):\n        errors = target - preds[:, i].unsqueeze(1)\n        loss = torch.max((q-1) * errors, q * errors)\n        losses.append(loss)\n    return torch.mean(torch.sum(torch.cat(losses, dim=1), dim=1))\n\n# ==========================================\n# METRICS (Now convert ratios back to FVC)\n# ==========================================\ndef calculate_metrics(y_true_fvc, q_preds_ratio, baseline_fvc):\n    \"\"\"\n    KEY CHANGE: Predictions are ratios, must multiply by baseline to get FVC.\n    \n    Args:\n        y_true_fvc: True FVC values (mL)\n        q_preds_ratio: Predicted quantiles as RATIOS\n        baseline_fvc: Baseline FVC for each sample (mL)\n    \"\"\"\n    # Reconstruct FVC predictions from ratios\n    q20 = q_preds_ratio[:, 0] * baseline_fvc\n    q50 = q_preds_ratio[:, 1] * baseline_fvc\n    q80 = q_preds_ratio[:, 2] * baseline_fvc\n    \n    sigma = q80 - q20\n    sigma_clipped = np.maximum(sigma, 70)\n    \n    delta = np.minimum(np.abs(y_true_fvc - q50), 1000)\n    lll = - (np.sqrt(2) * delta / sigma_clipped) - np.log(np.sqrt(2) * sigma_clipped)\n    \n    mse = mean_squared_error(y_true_fvc, q50)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(y_true_fvc, q50)\n    r2 = r2_score(y_true_fvc, q50)\n    rmae = mae / (np.mean(np.abs(y_true_fvc)) + 1e-6)\n    \n    return {\n        'lll': np.mean(lll),\n        'mse': mse,\n        'rmse': rmse,\n        'mae': mae,\n        'rmae': rmae,\n        'r2': r2\n    }\n\n# ==========================================\n# SIGMA CALIBRATION (Optional post-processing)\n# ==========================================\ndef calibrate_sigma_ratio(ratio_preds, scale_factor=0.85):\n    \"\"\"Compress uncertainty on ratio predictions.\"\"\"\n    preds_copy = ratio_preds.copy()\n    q20, q50, q80 = preds_copy[:, 0], preds_copy[:, 1], preds_copy[:, 2]\n    \n    new_q20 = q50 - (q50 - q20) * scale_factor\n    new_q80 = q50 + (q80 - q50) * scale_factor\n    \n    return np.column_stack([new_q20, q50, new_q80])\n\n# ==========================================\n# TRAINING WITH RATIO PREDICTION\n# ==========================================\ndef train_ratio_model():\n    df, features, ratio_scaler = preprocess_data_ratio(CONFIG)\n    patients = df['Patient'].unique()\n    kf = KFold(n_splits=CONFIG['n_folds'], shuffle=True, random_state=CONFIG['seed'])\n    \n    oof_indices = []\n    oof_ratio_preds = []\n    oof_trues_fvc = []\n    oof_baselines = []\n    \n    print(f\"\\nüöÄ Training RATIO PREDICTION Model\")\n    print(f\"üìä Training on {len(df)} visits across {len(patients)} patients\")\n    print(f\"üéØ Target: FVC_Ratio = Current_FVC / Baseline_FVC\")\n    print(\"=\"*80)\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(patients)):\n        print(f\"\\n{'='*80}\")\n        print(f\"FOLD {fold+1}/{CONFIG['n_folds']}\")\n        print(f\"{'='*80}\")\n        \n        train_p, val_p = patients[train_idx], patients[val_idx]\n        train_data = df[df['Patient'].isin(train_p)]\n        val_data = df[df['Patient'].isin(val_p)]\n        \n        X_train = torch.tensor(train_data[features].values, dtype=torch.float32).to(CONFIG['device'])\n        y_train_ratio_scaled = torch.tensor(train_data['FVC_Ratio_Scaled'].values, dtype=torch.float32).unsqueeze(1).to(CONFIG['device'])\n        \n        X_val = torch.tensor(val_data[features].values, dtype=torch.float32).to(CONFIG['device'])\n        y_val_fvc = val_data['FVC'].values\n        val_baselines = val_data['Base_FVC_Raw'].values\n        \n        model = EnhancedQuantileMLP(len(features), CONFIG['quantiles']).to(CONFIG['device'])\n        optimizer = optim.AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=CONFIG['weight_decay'])\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode='max', factor=0.5, patience=20, verbose=False\n        )\n        \n        best_lll = -float('inf')\n        best_ratio_preds = None\n        patience_counter = 0\n        \n        for epoch in range(CONFIG['epochs']):\n            model.train()\n            optimizer.zero_grad()\n            preds_ratio_scaled = model(X_train)\n            \n            loss = quantile_loss(preds_ratio_scaled, y_train_ratio_scaled, CONFIG['quantiles'])\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            \n            model.eval()\n            with torch.no_grad():\n                val_preds_ratio_scaled = model(X_val)\n                val_preds_ratio = ratio_scaler.inverse_transform(val_preds_ratio_scaled.cpu().numpy())\n                \n                metrics = calculate_metrics(y_val_fvc, val_preds_ratio, val_baselines)\n                lll = metrics['lll']\n                \n            scheduler.step(lll)\n            \n            if lll > best_lll:\n                best_lll = lll\n                best_ratio_preds = val_preds_ratio.copy()\n                patience_counter = 0\n            else:\n                patience_counter += 1\n                \n            if patience_counter >= CONFIG['patience']:\n                break\n        \n        oof_indices.extend(val_data.index.tolist())\n        oof_ratio_preds.append(best_ratio_preds)\n        oof_trues_fvc.extend(y_val_fvc)\n        oof_baselines.extend(val_baselines)\n        \n        fold_metrics = calculate_metrics(y_val_fvc, best_ratio_preds, val_baselines)\n        \n        print(f\"\\nüìà FOLD {fold+1} RESULTS:\")\n        print(f\"  R¬≤:   {fold_metrics['r2']:.4f}\")\n        print(f\"  RMSE: {fold_metrics['rmse']:.2f} mL\")\n        print(f\"  MAE:  {fold_metrics['mae']:.2f} mL\")\n        print(f\"  LLL:  {fold_metrics['lll']:.4f}\")\n    \n    oof_ratio_preds = np.vstack(oof_ratio_preds)\n    oof_trues_fvc = np.array(oof_trues_fvc)\n    oof_baselines = np.array(oof_baselines)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"üèÅ BASELINE RATIO PREDICTION RESULTS\")\n    print(\"=\"*80)\n    baseline_metrics = calculate_metrics(oof_trues_fvc, oof_ratio_preds, oof_baselines)\n    print(f\"R¬≤:   {baseline_metrics['r2']:.4f}   {'‚úÖ' if baseline_metrics['r2'] > 0.88 else '‚ùå'} (Target > 0.88)\")\n    print(f\"RMSE: {baseline_metrics['rmse']:.2f} mL {'‚úÖ' if baseline_metrics['rmse'] < 170 else '‚ùå'} (Target < 170)\")\n    print(f\"MAE:  {baseline_metrics['mae']:.2f} mL\")\n    print(f\"LLL:  {baseline_metrics['lll']:.4f}   {'‚úÖ' if baseline_metrics['lll'] > -6.64 else '‚ùå'} (Target > -6.64)\")\n    \n    # Optional: Test sigma calibration\n    print(\"\\n\" + \"=\"*80)\n    print(\"üß™ OPTIONAL: SIGMA CALIBRATION\")\n    print(\"=\"*80)\n    \n    best_lll = baseline_metrics['lll']\n    best_factor = 1.0\n    \n    for scale in [0.95, 0.90, 0.85, 0.80, 0.75]:\n        calibrated = calibrate_sigma_ratio(oof_ratio_preds, scale)\n        metrics = calculate_metrics(oof_trues_fvc, calibrated, oof_baselines)\n        \n        improvement = \"‚úÖ\" if metrics['lll'] > best_lll else \"\"\n        print(f\"  œÉ√ó{scale:.2f} ‚Üí LLL: {metrics['lll']:.4f} (RMSE: {metrics['rmse']:.2f}) {improvement}\")\n        \n        if metrics['lll'] > best_lll:\n            best_lll = metrics['lll']\n            best_factor = scale\n    \n    if best_factor < 1.0:\n        final_preds = calibrate_sigma_ratio(oof_ratio_preds, best_factor)\n        final_metrics = calculate_metrics(oof_trues_fvc, final_preds, oof_baselines)\n        \n        print(\"\\n\" + \"=\"*80)\n        print(\"üèÜ FINAL RESULTS (With Optimal Sigma)\")\n        print(\"=\"*80)\n        print(f\"Best œÉ scale factor: {best_factor:.2f}\")\n        print(f\"\\nR¬≤:   {final_metrics['r2']:.4f}   {'‚úÖ' if final_metrics['r2'] > 0.88 else '‚ùå'} (Target > 0.88)\")\n        print(f\"RMSE: {final_metrics['rmse']:.2f} mL {'‚úÖ' if final_metrics['rmse'] < 170 else '‚ùå'} (Target < 170)\")\n        print(f\"MAE:  {final_metrics['mae']:.2f} mL\")\n        print(f\"LLL:  {final_metrics['lll']:.4f}   {'‚úÖ' if final_metrics['lll'] > -6.64 else '‚ùå'} (Target > -6.64)\")\n        \n        print(\"\\n\" + \"=\"*80)\n        print(\"üìä IMPROVEMENT OVER BASELINE\")\n        print(\"=\"*80)\n        print(f\"RMSE: {baseline_metrics['rmse']:.2f} ‚Üí {final_metrics['rmse']:.2f} ({final_metrics['rmse'] - baseline_metrics['rmse']:+.2f} mL)\")\n        print(f\"LLL:  {baseline_metrics['lll']:.4f} ‚Üí {final_metrics['lll']:.4f} ({final_metrics['lll'] - baseline_metrics['lll']:+.4f})\")\n    else:\n        print(\"\\n‚úÖ No sigma calibration needed - baseline predictions are optimal!\")\n    \n    print(\"=\"*80)\n\nif __name__ == \"__main__\":\n    train_ratio_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T11:41:07.940666Z","iopub.execute_input":"2025-11-28T11:41:07.941452Z","iopub.status.idle":"2025-11-28T11:41:11.537048Z","shell.execute_reply.started":"2025-11-28T11:41:07.941425Z","shell.execute_reply":"2025-11-28T11:41:11.536447Z"}},"outputs":[{"name":"stdout","text":"üìä FVC_Ratio Distribution:\n   Mean: 0.967\n   Std:  0.086\n   Min:  0.461\n   Max:  1.288\n‚úÖ Preprocessing Complete. Final Shape: (1549, 28)\n‚úÖ Features Used: 19\n\nüöÄ Training RATIO PREDICTION Model\nüìä Training on 1549 visits across 176 patients\nüéØ Target: FVC_Ratio = Current_FVC / Baseline_FVC\n================================================================================\n\n================================================================================\nFOLD 1/5\n================================================================================\n\nüìà FOLD 1 RESULTS:\n  R¬≤:   0.9355\n  RMSE: 199.69 mL\n  MAE:  130.93 mL\n  LLL:  -6.5190\n\n================================================================================\nFOLD 2/5\n================================================================================\n\nüìà FOLD 2 RESULTS:\n  R¬≤:   0.9414\n  RMSE: 215.84 mL\n  MAE:  151.09 mL\n  LLL:  -6.6674\n\n================================================================================\nFOLD 3/5\n================================================================================\n\nüìà FOLD 3 RESULTS:\n  R¬≤:   0.9052\n  RMSE: 231.61 mL\n  MAE:  159.39 mL\n  LLL:  -6.7766\n\n================================================================================\nFOLD 4/5\n================================================================================\n\nüìà FOLD 4 RESULTS:\n  R¬≤:   0.9227\n  RMSE: 212.80 mL\n  MAE:  147.48 mL\n  LLL:  -6.6250\n\n================================================================================\nFOLD 5/5\n================================================================================\n\nüìà FOLD 5 RESULTS:\n  R¬≤:   0.9511\n  RMSE: 200.52 mL\n  MAE:  132.51 mL\n  LLL:  -6.5309\n\n================================================================================\nüèÅ BASELINE RATIO PREDICTION RESULTS\n================================================================================\nR¬≤:   0.9349   ‚úÖ (Target > 0.88)\nRMSE: 212.34 mL ‚ùå (Target < 170)\nMAE:  144.19 mL\nLLL:  -6.6231   ‚úÖ (Target > -6.64)\n\n================================================================================\nüß™ OPTIONAL: SIGMA CALIBRATION\n================================================================================\n  œÉ√ó0.95 ‚Üí LLL: -6.6207 (RMSE: 212.34) ‚úÖ\n  œÉ√ó0.90 ‚Üí LLL: -6.6209 (RMSE: 212.34) \n  œÉ√ó0.85 ‚Üí LLL: -6.6244 (RMSE: 212.34) \n  œÉ√ó0.80 ‚Üí LLL: -6.6318 (RMSE: 212.34) \n  œÉ√ó0.75 ‚Üí LLL: -6.6443 (RMSE: 212.34) \n\n================================================================================\nüèÜ FINAL RESULTS (With Optimal Sigma)\n================================================================================\nBest œÉ scale factor: 0.95\n\nR¬≤:   0.9349   ‚úÖ (Target > 0.88)\nRMSE: 212.34 mL ‚ùå (Target < 170)\nMAE:  144.19 mL\nLLL:  -6.6207   ‚úÖ (Target > -6.64)\n\n================================================================================\nüìä IMPROVEMENT OVER BASELINE\n================================================================================\nRMSE: 212.34 ‚Üí 212.34 (+0.00 mL)\nLLL:  -6.6231 ‚Üí -6.6207 (+0.0024)\n================================================================================\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## Action Plan:\n\n* Save your OOF predictions from this run (oof_ratio_preds and oof_trues_fvc).\n\n* Train a Gradient Boosting Model (LightGBM/XGBoost) on the exact same dataset (master_dataset.csv).\n\n* Blend: Average the predictions: Final = 0.6 * NeuralNet + 0.4 * LightGBM.\n\n* GBMs are excellent at finding \"edge cases\" (like Fold 3) that Neural Nets smooth over.\n\nThis typically drops RMSE by another 5-10 points.","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n# ==========================================\n# CONFIGURATION\n# ==========================================\nCONFIG = {\n    \"lr\": 2e-3,\n    \"weight_decay\": 3e-5,\n    \"batch_size\": 64,\n    \"epochs\": 300,\n    \"n_folds\": 5,\n    \"quantiles\": [0.2, 0.5, 0.8], \n    \"patience\": 50,\n    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n    \"data_dir\": \"../input/osic-pulmonary-fibrosis-progression\",\n    \"biomarker_path\": \"../input/feature-extraction-u-net-segmentation/master_dataset.csv\",\n    \"seed\": 42\n}\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(CONFIG['seed'])\n\n# ==========================================\n# STRATEGY C: RATIO-BASED DATA PREPROCESSING\n# ==========================================\ndef preprocess_data_ratio(config):\n    \"\"\"\n    KEY CHANGE: Target is now FVC_RATIO = Current_FVC / Baseline_FVC\n    This normalizes all patients to start at 1.0 and learn decay rates.\n    \"\"\"\n    clinical_df = pd.read_csv(f\"{config['data_dir']}/train.csv\")\n    biomarkers_df = pd.read_csv(config['biomarker_path'])\n    \n    image_features = [\n        'lung_vol_ml', 'hu_mean', 'hu_std', 'hu_skew', 'hu_kurt',\n        'glcm_contrast', 'glcm_homogeneity', 'glcm_energy', 'glcm_correlation'\n    ]\n    \n    cols_to_keep = ['Patient'] + [c for c in image_features if c in biomarkers_df.columns]\n    biomarkers_clean = biomarkers_df[cols_to_keep].drop_duplicates(subset=['Patient'])\n    \n    train = clinical_df.merge(biomarkers_clean, on='Patient', how='inner')\n    \n    train['Weeks'] = train['Weeks'].astype(int)\n    train.sort_values(['Patient', 'Weeks'], inplace=True)\n    \n    # Get baseline FVC for each patient\n    baseline = train.groupby('Patient').first().reset_index()\n    baseline = baseline[['Patient', 'FVC', 'Percent']].rename(\n        columns={'FVC': 'Base_FVC', 'Percent': 'Base_Percent'}\n    )\n    train = train.merge(baseline, on='Patient', how='left')\n    \n    base_weeks = train.groupby('Patient')['Weeks'].min().reset_index().rename(\n        columns={'Weeks': 'Base_Week'}\n    )\n    train = train.merge(base_weeks, on='Patient', how='left')\n    train['Relative_Weeks'] = train['Weeks'] - train['Base_Week']\n    \n    # ============================================\n    # üî• CRITICAL CHANGE: CREATE RATIO TARGET\n    # ============================================\n    train['FVC_Ratio'] = train['FVC'] / train['Base_FVC']\n    \n    # Sanity check: Most ratios should be 0.7-1.1 (fibrosis causes decline)\n    print(f\"üìä FVC_Ratio Distribution:\")\n    print(f\"   Mean: {train['FVC_Ratio'].mean():.3f}\")\n    print(f\"   Std:  {train['FVC_Ratio'].std():.3f}\")\n    print(f\"   Min:  {train['FVC_Ratio'].min():.3f}\")\n    print(f\"   Max:  {train['FVC_Ratio'].max():.3f}\")\n    \n    # Interaction features (same as before)\n    train['FVC_Week_Interaction'] = train['Base_FVC'] * train['Relative_Weeks']\n    train['Age_Week_Interaction'] = train['Age'] * train['Relative_Weeks']\n    \n    available_img_feats = [c for c in image_features if c in train.columns]\n    \n    if 'lung_vol_ml' in train.columns:\n        train['LungVol_FVC_Ratio'] = train['lung_vol_ml'] / (train['Base_FVC'] + 1e-6)\n    \n    # Feature scaling (EXCLUDE Base_FVC from scaling - we need its real value at inference)\n    scaler = StandardScaler()\n    \n    num_cols = ['Age', 'Base_Percent', 'Relative_Weeks'] + available_img_feats\n    interaction_cols = ['FVC_Week_Interaction', 'Age_Week_Interaction']\n    \n    if 'LungVol_FVC_Ratio' in train.columns:\n        interaction_cols.append('LungVol_FVC_Ratio')\n    \n    all_num_cols = num_cols + interaction_cols\n    train[all_num_cols] = scaler.fit_transform(train[all_num_cols])\n    \n    # Keep Base_FVC UNSCALED for reconstruction\n    train['Base_FVC_Raw'] = train['Base_FVC']\n    \n    train['Sex'] = train['Sex'].apply(lambda x: 1 if x == 'Male' else 0)\n    train['Smk_Ex'] = train['SmokingStatus'].apply(lambda x: 1 if x == 'Ex-smoker' else 0)\n    train['Smk_Cur'] = train['SmokingStatus'].apply(lambda x: 1 if x == 'Currently smokes' else 0)\n    \n    # Feature list (Base_FVC is now RAW, not scaled)\n    feature_cols = all_num_cols + ['Base_FVC_Raw', 'Sex', 'Smk_Ex', 'Smk_Cur']\n    \n    # Target scaling (RATIO instead of raw FVC)\n    ratio_scaler = StandardScaler()\n    ratio_scaler.fit(train[['FVC_Ratio']])\n    train['FVC_Ratio_Scaled'] = ratio_scaler.transform(train[['FVC_Ratio']])\n    \n    print(f\"‚úÖ Preprocessing Complete. Final Shape: {train.shape}\")\n    print(f\"‚úÖ Features Used: {len(feature_cols)}\")\n    \n    return train, feature_cols, ratio_scaler\n\n# ==========================================\n# MODEL (Same architecture)\n# ==========================================\nclass EnhancedQuantileMLP(nn.Module):\n    def __init__(self, input_dim, quantiles, dropout=0.25):\n        super().__init__()\n        h1, h2, h3 = 256, 128, 64\n        \n        self.net = nn.Sequential(\n            nn.Linear(input_dim, h1),\n            nn.BatchNorm1d(h1),\n            nn.LeakyReLU(0.1),\n            nn.Dropout(dropout),\n            \n            nn.Linear(h1, h2),\n            nn.BatchNorm1d(h2),\n            nn.LeakyReLU(0.1),\n            nn.Dropout(dropout),\n            \n            nn.Linear(h2, h3),\n            nn.BatchNorm1d(h3),\n            nn.LeakyReLU(0.1),\n            nn.Dropout(dropout),\n            \n            nn.Linear(h3, len(quantiles))\n        )\n    \n    def forward(self, x):\n        return self.net(x)\n\ndef quantile_loss(preds, target, quantiles):\n    assert not target.requires_grad\n    losses = []\n    for i, q in enumerate(quantiles):\n        errors = target - preds[:, i].unsqueeze(1)\n        loss = torch.max((q-1) * errors, q * errors)\n        losses.append(loss)\n    return torch.mean(torch.sum(torch.cat(losses, dim=1), dim=1))\n\n# ==========================================\n# METRICS (Now convert ratios back to FVC)\n# ==========================================\ndef calculate_metrics(y_true_fvc, q_preds_ratio, baseline_fvc):\n    \"\"\"\n    KEY CHANGE: Predictions are ratios, must multiply by baseline to get FVC.\n    \n    Args:\n        y_true_fvc: True FVC values (mL)\n        q_preds_ratio: Predicted quantiles as RATIOS\n        baseline_fvc: Baseline FVC for each sample (mL)\n    \"\"\"\n    # Reconstruct FVC predictions from ratios\n    q20 = q_preds_ratio[:, 0] * baseline_fvc\n    q50 = q_preds_ratio[:, 1] * baseline_fvc\n    q80 = q_preds_ratio[:, 2] * baseline_fvc\n    \n    sigma = q80 - q20\n    sigma_clipped = np.maximum(sigma, 70)\n    \n    delta = np.minimum(np.abs(y_true_fvc - q50), 1000)\n    lll = - (np.sqrt(2) * delta / sigma_clipped) - np.log(np.sqrt(2) * sigma_clipped)\n    \n    mse = mean_squared_error(y_true_fvc, q50)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(y_true_fvc, q50)\n    r2 = r2_score(y_true_fvc, q50)\n    rmae = mae / (np.mean(np.abs(y_true_fvc)) + 1e-6)\n    \n    return {\n        'lll': np.mean(lll),\n        'mse': mse,\n        'rmse': rmse,\n        'mae': mae,\n        'rmae': rmae,\n        'r2': r2\n    }\n\n# ==========================================\n# SIGMA CALIBRATION (Optional post-processing)\n# ==========================================\ndef calibrate_sigma_ratio(ratio_preds, scale_factor=0.85):\n    \"\"\"Compress uncertainty on ratio predictions.\"\"\"\n    preds_copy = ratio_preds.copy()\n    q20, q50, q80 = preds_copy[:, 0], preds_copy[:, 1], preds_copy[:, 2]\n    \n    new_q20 = q50 - (q50 - q20) * scale_factor\n    new_q80 = q50 + (q80 - q50) * scale_factor\n    \n    return np.column_stack([new_q20, q50, new_q80])\n\n# ==========================================\n# TRAINING WITH RATIO PREDICTION\n# ==========================================\ndef train_ratio_model():\n    df, features, ratio_scaler = preprocess_data_ratio(CONFIG)\n    patients = df['Patient'].unique()\n    kf = KFold(n_splits=CONFIG['n_folds'], shuffle=True, random_state=CONFIG['seed'])\n    \n    oof_indices = []\n    oof_ratio_preds = []\n    oof_trues_fvc = []\n    oof_baselines = []\n    \n    print(f\"\\nüöÄ Training RATIO PREDICTION Model\")\n    print(f\"üìä Training on {len(df)} visits across {len(patients)} patients\")\n    print(f\"üéØ Target: FVC_Ratio = Current_FVC / Baseline_FVC\")\n    print(\"=\"*80)\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(patients)):\n        print(f\"\\n{'='*80}\")\n        print(f\"FOLD {fold+1}/{CONFIG['n_folds']}\")\n        print(f\"{'='*80}\")\n        \n        train_p, val_p = patients[train_idx], patients[val_idx]\n        train_data = df[df['Patient'].isin(train_p)]\n        val_data = df[df['Patient'].isin(val_p)]\n        \n        X_train = torch.tensor(train_data[features].values, dtype=torch.float32).to(CONFIG['device'])\n        y_train_ratio_scaled = torch.tensor(train_data['FVC_Ratio_Scaled'].values, dtype=torch.float32).unsqueeze(1).to(CONFIG['device'])\n        \n        X_val = torch.tensor(val_data[features].values, dtype=torch.float32).to(CONFIG['device'])\n        y_val_fvc = val_data['FVC'].values\n        val_baselines = val_data['Base_FVC_Raw'].values\n        \n        model = EnhancedQuantileMLP(len(features), CONFIG['quantiles']).to(CONFIG['device'])\n        optimizer = optim.AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=CONFIG['weight_decay'])\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode='max', factor=0.5, patience=20, verbose=False\n        )\n        \n        best_lll = -float('inf')\n        best_ratio_preds = None\n        patience_counter = 0\n        \n        for epoch in range(CONFIG['epochs']):\n            model.train()\n            optimizer.zero_grad()\n            preds_ratio_scaled = model(X_train)\n            \n            loss = quantile_loss(preds_ratio_scaled, y_train_ratio_scaled, CONFIG['quantiles'])\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            \n            model.eval()\n            with torch.no_grad():\n                val_preds_ratio_scaled = model(X_val)\n                val_preds_ratio = ratio_scaler.inverse_transform(val_preds_ratio_scaled.cpu().numpy())\n                \n                metrics = calculate_metrics(y_val_fvc, val_preds_ratio, val_baselines)\n                lll = metrics['lll']\n                \n            scheduler.step(lll)\n            \n            if lll > best_lll:\n                best_lll = lll\n                best_ratio_preds = val_preds_ratio.copy()\n                patience_counter = 0\n            else:\n                patience_counter += 1\n                \n            if patience_counter >= CONFIG['patience']:\n                break\n        \n        oof_indices.extend(val_data.index.tolist())\n        oof_ratio_preds.append(best_ratio_preds)\n        oof_trues_fvc.extend(y_val_fvc)\n        oof_baselines.extend(val_baselines)\n        \n        fold_metrics = calculate_metrics(y_val_fvc, best_ratio_preds, val_baselines)\n        \n        print(f\"\\nüìà FOLD {fold+1} RESULTS:\")\n        print(f\"  R¬≤:   {fold_metrics['r2']:.4f}\")\n        print(f\"  RMSE: {fold_metrics['rmse']:.2f} mL\")\n        print(f\"  MAE:  {fold_metrics['mae']:.2f} mL\")\n        print(f\"  LLL:  {fold_metrics['lll']:.4f}\")\n    \n    oof_ratio_preds = np.vstack(oof_ratio_preds)\n    oof_trues_fvc = np.array(oof_trues_fvc)\n    oof_baselines = np.array(oof_baselines)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"üèÅ BASELINE RATIO PREDICTION RESULTS\")\n    print(\"=\"*80)\n    baseline_metrics = calculate_metrics(oof_trues_fvc, oof_ratio_preds, oof_baselines)\n    print(f\"R¬≤:   {baseline_metrics['r2']:.4f}   {'‚úÖ' if baseline_metrics['r2'] > 0.88 else '‚ùå'} (Target > 0.88)\")\n    print(f\"RMSE: {baseline_metrics['rmse']:.2f} mL {'‚úÖ' if baseline_metrics['rmse'] < 170 else '‚ùå'} (Target < 170)\")\n    print(f\"MAE:  {baseline_metrics['mae']:.2f} mL\")\n    print(f\"LLL:  {baseline_metrics['lll']:.4f}   {'‚úÖ' if baseline_metrics['lll'] > -6.64 else '‚ùå'} (Target > -6.64)\")\n    \n    # Optional: Test sigma calibration\n    print(\"\\n\" + \"=\"*80)\n    print(\"üß™ OPTIONAL: SIGMA CALIBRATION\")\n    print(\"=\"*80)\n    \n    best_lll = baseline_metrics['lll']\n    best_factor = 1.0\n    \n    for scale in [0.95, 0.90, 0.85, 0.80, 0.75]:\n        calibrated = calibrate_sigma_ratio(oof_ratio_preds, scale)\n        metrics = calculate_metrics(oof_trues_fvc, calibrated, oof_baselines)\n        \n        improvement = \"‚úÖ\" if metrics['lll'] > best_lll else \"\"\n        print(f\"  œÉ√ó{scale:.2f} ‚Üí LLL: {metrics['lll']:.4f} (RMSE: {metrics['rmse']:.2f}) {improvement}\")\n        \n        if metrics['lll'] > best_lll:\n            best_lll = metrics['lll']\n            best_factor = scale\n            torch.save(model.state_dict(), f\"optimised_model_fold{fold+1}.pth\")\n    \n    if best_factor < 1.0:\n        final_preds = calibrate_sigma_ratio(oof_ratio_preds, best_factor)\n        final_metrics = calculate_metrics(oof_trues_fvc, final_preds, oof_baselines)\n        \n        print(\"\\n\" + \"=\"*80)\n        print(\"üèÜ FINAL RESULTS (With Optimal Sigma)\")\n        print(\"=\"*80)\n        print(f\"Best œÉ scale factor: {best_factor:.2f}\")\n        print(f\"\\nR¬≤:   {final_metrics['r2']:.4f}   {'‚úÖ' if final_metrics['r2'] > 0.88 else '‚ùå'} (Target > 0.88)\")\n        print(f\"RMSE: {final_metrics['rmse']:.2f} mL {'‚úÖ' if final_metrics['rmse'] < 170 else '‚ùå'} (Target < 170)\")\n        print(f\"MAE:  {final_metrics['mae']:.2f} mL\")\n        print(f\"LLL:  {final_metrics['lll']:.4f}   {'‚úÖ' if final_metrics['lll'] > -6.64 else '‚ùå'} (Target > -6.64)\")\n        \n        print(\"\\n\" + \"=\"*80)\n        print(\"üìä IMPROVEMENT OVER BASELINE\")\n        print(\"=\"*80)\n        print(f\"RMSE: {baseline_metrics['rmse']:.2f} ‚Üí {final_metrics['rmse']:.2f} ({final_metrics['rmse'] - baseline_metrics['rmse']:+.2f} mL)\")\n        print(f\"LLL:  {baseline_metrics['lll']:.4f} ‚Üí {final_metrics['lll']:.4f} ({final_metrics['lll'] - baseline_metrics['lll']:+.4f})\")\n    else:\n        print(\"\\n‚úÖ No sigma calibration needed - baseline predictions are optimal!\")\n    \n    print(\"=\"*80)\n\nif __name__ == \"__main__\":\n    train_ratio_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T12:00:26.414842Z","iopub.execute_input":"2025-11-28T12:00:26.415459Z","iopub.status.idle":"2025-11-28T12:00:30.224841Z","shell.execute_reply.started":"2025-11-28T12:00:26.415434Z","shell.execute_reply":"2025-11-28T12:00:30.224265Z"}},"outputs":[{"name":"stdout","text":"üìä FVC_Ratio Distribution:\n   Mean: 0.967\n   Std:  0.086\n   Min:  0.461\n   Max:  1.288\n‚úÖ Preprocessing Complete. Final Shape: (1549, 28)\n‚úÖ Features Used: 19\n\nüöÄ Training RATIO PREDICTION Model\nüìä Training on 1549 visits across 176 patients\nüéØ Target: FVC_Ratio = Current_FVC / Baseline_FVC\n================================================================================\n\n================================================================================\nFOLD 1/5\n================================================================================\n\nüìà FOLD 1 RESULTS:\n  R¬≤:   0.9355\n  RMSE: 199.69 mL\n  MAE:  130.93 mL\n  LLL:  -6.5190\n\n================================================================================\nFOLD 2/5\n================================================================================\n\nüìà FOLD 2 RESULTS:\n  R¬≤:   0.9414\n  RMSE: 215.84 mL\n  MAE:  151.09 mL\n  LLL:  -6.6674\n\n================================================================================\nFOLD 3/5\n================================================================================\n\nüìà FOLD 3 RESULTS:\n  R¬≤:   0.9052\n  RMSE: 231.61 mL\n  MAE:  159.39 mL\n  LLL:  -6.7766\n\n================================================================================\nFOLD 4/5\n================================================================================\n\nüìà FOLD 4 RESULTS:\n  R¬≤:   0.9227\n  RMSE: 212.80 mL\n  MAE:  147.48 mL\n  LLL:  -6.6250\n\n================================================================================\nFOLD 5/5\n================================================================================\n\nüìà FOLD 5 RESULTS:\n  R¬≤:   0.9511\n  RMSE: 200.52 mL\n  MAE:  132.51 mL\n  LLL:  -6.5309\n\n================================================================================\nüèÅ BASELINE RATIO PREDICTION RESULTS\n================================================================================\nR¬≤:   0.9349   ‚úÖ (Target > 0.88)\nRMSE: 212.34 mL ‚ùå (Target < 170)\nMAE:  144.19 mL\nLLL:  -6.6231   ‚úÖ (Target > -6.64)\n\n================================================================================\nüß™ OPTIONAL: SIGMA CALIBRATION\n================================================================================\n  œÉ√ó0.95 ‚Üí LLL: -6.6207 (RMSE: 212.34) ‚úÖ\n  œÉ√ó0.90 ‚Üí LLL: -6.6209 (RMSE: 212.34) \n  œÉ√ó0.85 ‚Üí LLL: -6.6244 (RMSE: 212.34) \n  œÉ√ó0.80 ‚Üí LLL: -6.6318 (RMSE: 212.34) \n  œÉ√ó0.75 ‚Üí LLL: -6.6443 (RMSE: 212.34) \n\n================================================================================\nüèÜ FINAL RESULTS (With Optimal Sigma)\n================================================================================\nBest œÉ scale factor: 0.95\n\nR¬≤:   0.9349   ‚úÖ (Target > 0.88)\nRMSE: 212.34 mL ‚ùå (Target < 170)\nMAE:  144.19 mL\nLLL:  -6.6207   ‚úÖ (Target > -6.64)\n\n================================================================================\nüìä IMPROVEMENT OVER BASELINE\n================================================================================\nRMSE: 212.34 ‚Üí 212.34 (+0.00 mL)\nLLL:  -6.6231 ‚Üí -6.6207 (+0.0024)\n================================================================================\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
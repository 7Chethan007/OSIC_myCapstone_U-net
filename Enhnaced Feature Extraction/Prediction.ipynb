{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":20604,"databundleVersionId":1357052,"sourceType":"competition"},{"sourceId":13905913,"sourceType":"datasetVersion","datasetId":8852072},{"sourceId":662983,"sourceType":"modelInstanceVersion","modelInstanceId":501638,"modelId":516804}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# AI-Driven Early Prediction of Pulmonary Fibrosis Using Deep Learning\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Retrain Quantile MLP: \nUse the new, richer Master CSV (total $\\approx 15 \\text{ features}$) as input to the existing Quantile MLP structure. The increased dimensionality will allow the network to find correlations missed by the current shallow feature set.\n## Ensemble Inference: \nRun the final prediction by averaging the 5 fold models (Out-Of-Fold) to generate the final, smoothest LLL/RMSE score.","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n# ==========================================\n# 1. CONFIGURATION\n# ==========================================\nCONFIG = {\n    \"data_path\": \"/kaggle/input/feature-extraction-u-net-segmentation/master_dataset.csv\",\n    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n    \"n_folds\": 5,\n    \"epochs\": 100,\n    \"batch_size\": 64,\n    \"learning_rate\": 0.001,\n    \"weight_decay\": 1e-4,\n    \"patience\": 15,  # Early stopping\n    \"quantiles\": [0.1, 0.5, 0.9],  # For LLL: q10, median, q90\n    \"seed\": 42\n}\n\n# Set random seeds\ntorch.manual_seed(CONFIG['seed'])\nnp.random.seed(CONFIG['seed'])\n\n# ==========================================\n# 2. QUANTILE REGRESSION MLP\n# ==========================================\nclass QuantileMLP(nn.Module):\n    \"\"\"\n    Deep MLP for Quantile Regression\n    Outputs 3 quantiles: [q10, q50 (median), q90]\n    \"\"\"\n    def __init__(self, input_dim, hidden_dims=[256, 128, 64], n_quantiles=3, dropout=0.3):\n        super().__init__()\n        \n        layers = []\n        prev_dim = input_dim\n        \n        for hidden_dim in hidden_dims:\n            layers.append(nn.Linear(prev_dim, hidden_dim))\n            layers.append(nn.BatchNorm1d(hidden_dim))\n            layers.append(nn.ReLU())\n            layers.append(nn.Dropout(dropout))\n            prev_dim = hidden_dim\n        \n        # Output layer: 3 quantile predictions\n        layers.append(nn.Linear(prev_dim, n_quantiles))\n        \n        self.network = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        return self.network(x)\n\n# ==========================================\n# 3. PINBALL LOSS (Quantile Loss)\n# ==========================================\ndef quantile_loss(preds, target, quantiles):\n    \"\"\"\n    Pinball Loss for Quantile Regression\n    preds: (batch, 3) - predictions for [q10, q50, q90]\n    target: (batch,) - true FVC values\n    quantiles: [0.1, 0.5, 0.9]\n    \"\"\"\n    target = target.unsqueeze(1)  # (batch, 1)\n    errors = target - preds  # (batch, 3)\n    \n    quantiles_tensor = torch.tensor(quantiles, device=preds.device).unsqueeze(0)\n    \n    loss = torch.max(quantiles_tensor * errors, (quantiles_tensor - 1) * errors)\n    return loss.mean()\n\n# ==========================================\n# 4. METRIC CALCULATIONS\n# ==========================================\ndef calculate_metrics(y_true, y_pred_median, y_pred_q10, y_pred_q90):\n    \"\"\"\n    Calculate comprehensive metrics:\n    - R¬≤: Coefficient of determination\n    - MSE: Mean Squared Error\n    - RMSE: Root Mean Squared Error\n    - MAE: Mean Absolute Error\n    - RMAE: Relative Mean Absolute Error\n    - LLL: Laplace Log Likelihood (Competition Metric)\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred_median = np.array(y_pred_median)\n    y_pred_q10 = np.array(y_pred_q10)\n    y_pred_q90 = np.array(y_pred_q90)\n    \n    # R¬≤\n    ss_res = np.sum((y_true - y_pred_median) ** 2)\n    ss_tot = np.sum((y_true - y_true.mean()) ** 2)\n    r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n    \n    # MSE, RMSE, MAE\n    mse = np.mean((y_true - y_pred_median) ** 2)\n    rmse = np.sqrt(mse)\n    mae = np.mean(np.abs(y_true - y_pred_median))\n    \n    # RMAE (Relative MAE)\n    mean_true = np.mean(np.abs(y_true))\n    rmae = mae / mean_true if mean_true > 0 else 0\n    \n    # LLL (Laplace Log Likelihood)\n    # sigma = (q90 - q10) / 2.56  # Approximation for Laplace scale\n    # For competition: sigma_clipped = max(70, sigma)\n    sigma = np.maximum(70, (y_pred_q90 - y_pred_q10) / 2.56)\n    delta = np.abs(y_true - y_pred_median)\n    lll = -np.sqrt(2) * delta / sigma - np.log(np.sqrt(2) * sigma)\n    lll_mean = np.mean(lll)\n    \n    return {\n        'R2': r2,\n        'MSE': mse,\n        'RMSE': rmse,\n        'MAE': mae,\n        'RMAE': rmae,\n        'LLL': lll_mean\n    }\n\n# ==========================================\n# 5. DATASET CLASS\n# ==========================================\nclass FibrosisFVCDataset(Dataset):\n    def __init__(self, features, targets, weeks):\n        self.features = torch.FloatTensor(features)\n        self.targets = torch.FloatTensor(targets)\n        self.weeks = torch.FloatTensor(weeks)\n    \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, idx):\n        return self.features[idx], self.targets[idx], self.weeks[idx]\n\n# ==========================================\n# 6. DATA PREPARATION\n# ==========================================\ndef prepare_data(df):\n    \"\"\"\n    Prepare features and targets from master dataset\n    \"\"\"\n    print(\"üìä Preparing Data...\")\n    \n    # Clinical/Demographic features\n    clinical_features = ['Age', 'Sex', 'SmokingStatus', 'Weeks', 'Percent']\n    \n    # Image-derived biomarkers (from U-Net extraction)\n    image_features = [\n        'lung_vol_ml', 'hu_mean', 'hu_std', 'hu_skew', 'hu_kurt',\n        'glcm_contrast', 'glcm_homogeneity', 'glcm_energy', 'glcm_correlation'\n    ]\n    \n    # Encode categorical\n    df['Sex'] = df['Sex'].map({'Male': 1, 'Female': 0})\n    df['SmokingStatus'] = df['SmokingStatus'].map({\n        'Never smoked': 0, \n        'Ex-smoker': 1, \n        'Currently smokes': 2\n    })\n    \n    # Target: FVC value prediction (not slope!)\n    all_features = clinical_features + image_features\n    \n    # Remove rows with missing features\n    df_clean = df[all_features + ['FVC']].dropna()\n    \n    X = df_clean[all_features].values\n    y = df_clean['FVC'].values\n    weeks = df_clean['Weeks'].values\n    \n    print(f\"‚úÖ Dataset Shape: {X.shape}\")\n    print(f\"‚úÖ Features ({len(all_features)}): {all_features}\")\n    print(f\"‚úÖ Target Range: FVC [{y.min():.0f}, {y.max():.0f}]\")\n    print(f\"‚úÖ Samples: {len(X)}\\n\")\n    \n    return X, y, weeks, all_features\n\n# ==========================================\n# 7. TRAINING FUNCTION\n# ==========================================\ndef train_epoch(model, loader, optimizer, quantiles, device):\n    model.train()\n    total_loss = 0\n    \n    for features, targets, _ in loader:\n        features, targets = features.to(device), targets.to(device)\n        \n        optimizer.zero_grad()\n        preds = model(features)\n        loss = quantile_loss(preds, targets, quantiles)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    return total_loss / len(loader)\n\ndef evaluate_epoch(model, loader, quantiles, device):\n    model.eval()\n    all_preds = []\n    all_targets = []\n    \n    with torch.no_grad():\n        for features, targets, _ in loader:\n            features, targets = features.to(device), targets.to(device)\n            preds = model(features)\n            \n            all_preds.append(preds.cpu().numpy())\n            all_targets.append(targets.cpu().numpy())\n    \n    all_preds = np.vstack(all_preds)  # (N, 3) -> [q10, q50, q90]\n    all_targets = np.concatenate(all_targets)\n    \n    metrics = calculate_metrics(\n        all_targets,\n        all_preds[:, 1],  # median (q50)\n        all_preds[:, 0],  # q10\n        all_preds[:, 2]   # q90\n    )\n    \n    return metrics, all_preds\n\n# ==========================================\n# 8. MAIN TRAINING LOOP (5-FOLD CV)\n# ==========================================\ndef train_quantile_mlp():\n    print(\"=\"*70)\n    print(\"üöÄ PHASE 2: QUANTILE MLP TRAINING (5-FOLD ENSEMBLE)\")\n    print(\"=\"*70)\n    print(f\"Device: {CONFIG['device']}\\n\")\n    \n    # Load data\n    df = pd.read_csv(CONFIG['data_path'])\n    X, y, weeks, feature_names = prepare_data(df)\n    \n    # Initialize KFold\n    kfold = KFold(n_splits=CONFIG['n_folds'], shuffle=True, random_state=CONFIG['seed'])\n    \n    # Storage for fold results\n    fold_results = []\n    fold_models = []\n    oof_predictions = np.zeros((len(X), 3))  # Out-of-fold predictions\n    \n    # Training loop\n    for fold, (train_idx, val_idx) in enumerate(kfold.split(X), 1):\n        print(f\"\\n{'='*70}\")\n        print(f\"üìÇ FOLD {fold}/{CONFIG['n_folds']}\")\n        print(f\"{'='*70}\")\n        \n        # Split data\n        X_train, X_val = X[train_idx], X[val_idx]\n        y_train, y_val = y[train_idx], y[val_idx]\n        weeks_train, weeks_val = weeks[train_idx], weeks[val_idx]\n        \n        # Scale features\n        scaler = StandardScaler()\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_val_scaled = scaler.transform(X_val)\n        \n        # Create datasets\n        train_dataset = FibrosisFVCDataset(X_train_scaled, y_train, weeks_train)\n        val_dataset = FibrosisFVCDataset(X_val_scaled, y_val, weeks_val)\n        \n        train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n        val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n        \n        # Initialize model\n        model = QuantileMLP(\n            input_dim=X.shape[1],\n            hidden_dims=[256, 128, 64],\n            n_quantiles=3,\n            dropout=0.3\n        ).to(CONFIG['device'])\n        \n        optimizer = optim.AdamW(\n            model.parameters(),\n            lr=CONFIG['learning_rate'],\n            weight_decay=CONFIG['weight_decay']\n        )\n        \n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode='max', factor=0.5, patience=5, verbose=False\n        )\n        \n        # Training\n        best_lll = -np.inf\n        patience_counter = 0\n        \n        print(f\"\\n{'Epoch':<6} {'Train Loss':<12} {'R¬≤':<8} {'MSE':<10} {'RMSE':<8} {'MAE':<8} {'RMAE':<8} {'LLL':<8}\")\n        print(\"-\" * 70)\n        \n        for epoch in range(1, CONFIG['epochs'] + 1):\n            train_loss = train_epoch(model, train_loader, optimizer, CONFIG['quantiles'], CONFIG['device'])\n            val_metrics, _ = evaluate_epoch(model, val_loader, CONFIG['quantiles'], CONFIG['device'])\n            \n            # Print metrics\n            print(f\"{epoch:<6} {train_loss:<12.4f} {val_metrics['R2']:<8.4f} \"\n                  f\"{val_metrics['MSE']:<10.2f} {val_metrics['RMSE']:<8.2f} \"\n                  f\"{val_metrics['MAE']:<8.2f} {val_metrics['RMAE']:<8.4f} \"\n                  f\"{val_metrics['LLL']:<8.4f}\")\n            \n            # Scheduler step\n            scheduler.step(val_metrics['LLL'])\n            \n            # Early stopping based on LLL\n            if val_metrics['LLL'] > best_lll:\n                best_lll = val_metrics['LLL']\n                patience_counter = 0\n                # Save best model for this fold\n                torch.save(model.state_dict(), f'best_model_fold{fold}.pth')\n            else:\n                patience_counter += 1\n            \n            if patience_counter >= CONFIG['patience']:\n                print(f\"‚ö†Ô∏è  Early stopping at epoch {epoch}\")\n                break\n        \n        # Load best model and evaluate\n        model.load_state_dict(torch.load(f'best_model_fold{fold}.pth'))\n        final_metrics, val_preds = evaluate_epoch(model, val_loader, CONFIG['quantiles'], CONFIG['device'])\n        \n        # Store OOF predictions\n        oof_predictions[val_idx] = val_preds\n        \n        # Store results\n        fold_results.append(final_metrics)\n        fold_models.append((model, scaler))\n        \n        print(f\"\\n‚úÖ FOLD {fold} FINAL METRICS:\")\n        print(f\"   R¬≤ = {final_metrics['R2']:.4f}\")\n        print(f\"   RMSE = {final_metrics['RMSE']:.2f} mL\")\n        print(f\"   LLL = {final_metrics['LLL']:.4f}\")\n    \n    # ==========================================\n    # 9. ENSEMBLE RESULTS\n    # ==========================================\n    print(f\"\\n{'='*70}\")\n    print(\"üèÜ FINAL ENSEMBLE RESULTS (5-FOLD AVERAGE)\")\n    print(f\"{'='*70}\")\n    \n    # Calculate OOF ensemble metrics\n    oof_metrics = calculate_metrics(\n        y,\n        oof_predictions[:, 1],  # median\n        oof_predictions[:, 0],  # q10\n        oof_predictions[:, 2]   # q90\n    )\n    \n    print(f\"\\nüìä Out-of-Fold (OOF) Ensemble Performance:\")\n    print(f\"   R¬≤    = {oof_metrics['R2']:.4f}\")\n    print(f\"   MSE   = {oof_metrics['MSE']:.2f}\")\n    print(f\"   RMSE  = {oof_metrics['RMSE']:.2f} mL\")\n    print(f\"   MAE   = {oof_metrics['MAE']:.2f} mL\")\n    print(f\"   RMAE  = {oof_metrics['RMAE']:.4f}\")\n    print(f\"   LLL   = {oof_metrics['LLL']:.4f}\")\n    \n    # Benchmark comparison\n    print(f\"\\nüéØ Benchmark Comparison:\")\n    print(f\"   Target RMSE: < 170 mL  ‚Üí  {'‚úÖ PASSED' if oof_metrics['RMSE'] < 170 else '‚ùå NEEDS IMPROVEMENT'}\")\n    print(f\"   Target LLL:  > -6.64   ‚Üí  {'‚úÖ PASSED' if oof_metrics['LLL'] > -6.64 else '‚ùå NEEDS IMPROVEMENT'}\")\n    \n    # Average fold metrics\n    avg_metrics = {k: np.mean([f[k] for f in fold_results]) for k in fold_results[0].keys()}\n    print(f\"\\nüìà Average Across Folds:\")\n    for metric, value in avg_metrics.items():\n        print(f\"   {metric:<6} = {value:.4f}\")\n    \n    return fold_models, oof_predictions, oof_metrics\n\n# ==========================================\n# 10. RUN TRAINING\n# ==========================================\nif __name__ == \"__main__\":\n    models, oof_preds, final_metrics = train_quantile_mlp()\n    print(\"\\n‚úÖ Training Complete! Models saved as 'best_model_fold{1-5}.pth'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T08:56:18.299958Z","iopub.execute_input":"2025-11-28T08:56:18.300706Z","iopub.status.idle":"2025-11-28T08:56:56.294664Z","shell.execute_reply.started":"2025-11-28T08:56:18.300680Z","shell.execute_reply":"2025-11-28T08:56:56.294060Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüöÄ PHASE 2: QUANTILE MLP TRAINING (5-FOLD ENSEMBLE)\n======================================================================\nDevice: cuda\n\nüìä Preparing Data...\n‚úÖ Dataset Shape: (1549, 14)\n‚úÖ Features (14): ['Age', 'Sex', 'SmokingStatus', 'Weeks', 'Percent', 'lung_vol_ml', 'hu_mean', 'hu_std', 'hu_skew', 'hu_kurt', 'glcm_contrast', 'glcm_homogeneity', 'glcm_energy', 'glcm_correlation']\n‚úÖ Target Range: FVC [827, 6399]\n‚úÖ Samples: 1549\n\n\n======================================================================\nüìÇ FOLD 1/5\n======================================================================\n\nEpoch  Train Loss   R¬≤       MSE        RMSE     MAE      RMAE     LLL     \n----------------------------------------------------------------------\n1      1349.4300    -11.9587 7793084.50 2791.61  2681.74  0.9998   -58.7744\n2      1343.6302    -11.9528 7789507.00 2790.97  2681.07  0.9996   -58.7609\n3      1343.5650    -11.9473 7786223.50 2790.38  2680.46  0.9993   -58.7485\n4      1347.6377    -11.9404 7782072.00 2789.64  2679.68  0.9990   -58.7328\n5      1346.6469    -11.9324 7777280.00 2788.78  2678.79  0.9987   -58.7148\n6      1343.3291    -11.9222 7771157.50 2787.68  2677.62  0.9983   -58.6912\n7      1343.5076    -11.9111 7764438.50 2786.47  2676.37  0.9978   -58.6660\n8      1345.7901    -11.8969 7755924.50 2784.95  2674.78  0.9972   -58.6337\n9      1342.5872    -11.8813 7746508.00 2783.25  2673.02  0.9966   -58.5983\n10     1339.4125    -11.8674 7738178.00 2781.76  2671.45  0.9960   -58.5664\n11     1340.7091    -11.8505 7727996.00 2779.93  2669.57  0.9953   -58.5285\n12     1333.8823    -11.8334 7717731.00 2778.08  2667.69  0.9946   -58.4905\n13     1339.7782    -11.8129 7705380.50 2775.86  2665.41  0.9937   -58.4446\n14     1334.8120    -11.7919 7692745.50 2773.58  2663.11  0.9929   -58.3981\n15     1340.7567    -11.7685 7678716.50 2771.05  2660.61  0.9919   -58.3476\n16     1336.4616    -11.7468 7665656.50 2768.69  2658.24  0.9910   -58.2995\n17     1335.2153    -11.7197 7649376.00 2765.75  2655.25  0.9899   -58.2393\n18     1331.7097    -11.6963 7635302.50 2763.21  2652.71  0.9890   -58.1879\n19     1330.5573    -11.6698 7619356.00 2760.32  2649.71  0.9879   -58.1274\n20     1328.9437    -11.6402 7601557.50 2757.09  2646.47  0.9867   -58.0618\n21     1333.2592    -11.6050 7580385.50 2753.25  2642.66  0.9852   -57.9848\n22     1325.0638    -11.5811 7566030.50 2750.64  2640.03  0.9843   -57.9317\n23     1320.6008    -11.5446 7544047.50 2746.64  2635.94  0.9827   -57.8492\n24     1320.0951    -11.5070 7521425.50 2742.52  2631.87  0.9812   -57.7668\n25     1324.6268    -11.4745 7501882.00 2738.96  2628.23  0.9799   -57.6933\n26     1316.4562    -11.4388 7480412.50 2735.03  2624.33  0.9784   -57.6145\n27     1314.6862    -11.4029 7458842.50 2731.09  2620.31  0.9769   -57.5333\n28     1313.4123    -11.3609 7433577.00 2726.46  2615.60  0.9751   -57.4382\n29     1313.1121    -11.3266 7412980.50 2722.68  2612.02  0.9738   -57.3659\n30     1305.6969    -11.2848 7387820.00 2718.05  2607.17  0.9720   -57.2679\n31     1307.7397    -11.2321 7356151.50 2712.22  2601.37  0.9698   -57.1507\n32     1300.6845    -11.1767 7322777.00 2706.06  2595.41  0.9676   -57.0302\n33     1300.5885    -11.1375 7299219.00 2701.71  2590.88  0.9659   -56.9387\n34     1295.8161    -11.0855 7267957.50 2695.92  2585.00  0.9637   -56.8199\n35     1300.6161    -11.0510 7247212.00 2692.06  2581.20  0.9623   -56.7432\n36     1296.1688    -11.0069 7220675.50 2687.13  2576.10  0.9604   -56.6402\n37     1294.0820    -10.9472 7184807.00 2680.45  2569.33  0.9579   -56.5034\n38     1290.0231    -10.8995 7156084.50 2675.09  2564.14  0.9560   -56.3985\n39     1286.6554    -10.8563 7130099.00 2670.22  2558.82  0.9540   -56.2910\n40     1279.0763    -10.7870 7088471.50 2662.42  2551.39  0.9512   -56.1409\n41     1280.6435    -10.7430 7061998.00 2657.44  2546.08  0.9492   -56.0337\n42     1274.0911    -10.6875 7028617.00 2651.15  2539.71  0.9469   -55.9050\n43     1270.4948    -10.6425 7001570.50 2646.05  2534.58  0.9449   -55.8012\n44     1272.5004    -10.5735 6960042.00 2638.19  2526.90  0.9421   -55.6461\n45     1269.7099    -10.5109 6922425.00 2631.05  2519.17  0.9392   -55.4900\n46     1268.6009    -10.4772 6902167.00 2627.20  2515.47  0.9378   -55.4152\n47     1256.9245    -10.3923 6851072.00 2617.46  2505.46  0.9341   -55.2129\n48     1258.3929    -10.3322 6814947.50 2610.55  2498.61  0.9315   -55.0747\n49     1255.0608    -10.2643 6774109.00 2602.71  2490.78  0.9286   -54.9165\n50     1246.2221    -10.2087 6740644.50 2596.28  2483.85  0.9260   -54.7764\n51     1250.3215    -10.1482 6704302.50 2589.27  2477.03  0.9235   -54.6386\n52     1245.8773    -10.0866 6667255.50 2582.10  2469.84  0.9208   -54.4934\n53     1242.5210    -10.0142 6623675.50 2573.65  2461.20  0.9176   -54.3187\n54     1237.1181    -9.9671  6595393.00 2568.15  2455.49  0.9155   -54.2034\n55     1237.0065    -9.8929  6550756.00 2559.44  2446.79  0.9122   -54.0277\n56     1226.2457    -9.8294  6512546.00 2551.97  2439.03  0.9093   -53.8709\n57     1223.9540    -9.7776  6481405.50 2545.86  2432.36  0.9068   -53.7361\n58     1216.1524    -9.6713  6417500.00 2533.28  2420.06  0.9022   -53.4877\n59     1212.9629    -9.6237  6388891.50 2527.63  2414.49  0.9002   -53.3751\n60     1213.3938    -9.5379  6337281.50 2517.40  2403.84  0.8962   -53.1600\n61     1207.8362    -9.4699  6296399.50 2509.26  2395.82  0.8932   -52.9979\n62     1206.7308    -9.4026  6255878.00 2501.18  2387.68  0.8902   -52.8335\n63     1198.8601    -9.3522  6225593.00 2495.11  2381.28  0.8878   -52.7042\n64     1197.1708    -9.2521  6165386.50 2483.02  2369.45  0.8834   -52.4653\n65     1190.9001    -9.1534  6106029.50 2471.04  2356.35  0.8785   -52.2004\n66     1180.3804    -9.0874  6066333.50 2462.99  2347.94  0.8754   -52.0306\n67     1180.7828    -9.0378  6036488.00 2456.93  2342.12  0.8732   -51.9130\n68     1171.1400    -8.9075  5958186.00 2440.94  2325.70  0.8671   -51.5813\n69     1173.2201    -8.8373  5915961.50 2432.27  2317.29  0.8639   -51.4113\n70     1169.5045    -8.7947  5890300.00 2426.99  2311.43  0.8617   -51.2930\n71     1164.3572    -8.7115  5840269.50 2416.67  2300.56  0.8577   -51.0734\n72     1154.6277    -8.6215  5786161.50 2405.44  2289.44  0.8535   -50.8488\n73     1147.3599    -8.5338  5733415.00 2394.46  2278.29  0.8494   -50.6235\n74     1146.4314    -8.4604  5689257.50 2385.22  2268.56  0.8458   -50.4268\n75     1142.2530    -8.3723  5636279.50 2374.08  2257.54  0.8417   -50.2042\n76     1134.3335    -8.3183  5603799.00 2367.23  2250.01  0.8388   -50.0521\n77     1130.9582    -8.2630  5570582.50 2360.21  2242.58  0.8361   -49.9021\n78     1122.7091    -8.1329  5492312.50 2343.57  2225.86  0.8298   -49.5643\n79     1119.3576    -8.1073  5476955.50 2340.29  2221.44  0.8282   -49.4749\n80     1116.8042    -7.9662  5392100.50 2322.09  2203.12  0.8214   -49.1047\n81     1108.7817    -7.9204  5364545.00 2316.15  2197.02  0.8191   -48.9817\n82     1100.4046    -7.7986  5291264.00 2300.27  2181.05  0.8131   -48.6588\n83     1097.9551    -7.7339  5252374.50 2291.81  2171.94  0.8097   -48.4749\n84     1088.8469    -7.6469  5200035.00 2280.36  2160.12  0.8053   -48.2361\n85     1087.0035    -7.5415  5136691.50 2266.43  2145.67  0.7999   -47.9441\n86     1081.1918    -7.4755  5096966.00 2257.65  2135.79  0.7963   -47.7446\n87     1078.5296    -7.4407  5076075.00 2253.01  2131.51  0.7947   -47.6580\n88     1067.8691    -7.3344  5012136.50 2238.78  2116.91  0.7892   -47.3632\n89     1066.3392    -7.2460  4958951.50 2226.87  2104.36  0.7845   -47.1096\n90     1062.5781    -7.1862  4922978.50 2218.78  2095.39  0.7812   -46.9284\n91     1048.1818    -7.0679  4851848.50 2202.69  2077.87  0.7747   -46.5745\n92     1047.8337    -6.9898  4804901.00 2192.01  2067.43  0.7708   -46.3634\n93     1043.3737    -6.9142  4759401.50 2181.61  2057.22  0.7670   -46.1572\n94     1031.2188    -6.7747  4675536.00 2162.30  2037.34  0.7596   -45.7555\n95     1028.1173    -6.7253  4645838.50 2155.42  2029.82  0.7568   -45.6035\n96     1024.0543    -6.6332  4590470.00 2142.54  2017.22  0.7521   -45.3491\n97     1013.2116    -6.5433  4536406.50 2129.88  2003.59  0.7470   -45.0736\n98     1008.5496    -6.4818  4499411.50 2121.18  1993.52  0.7432   -44.8703\n99     1006.7310    -6.4004  4450470.50 2109.61  1981.54  0.7388   -44.6282\n100    995.7884     -6.3260  4405723.00 2098.98  1970.31  0.7346   -44.4014\n\n‚úÖ FOLD 1 FINAL METRICS:\n   R¬≤ = -6.3260\n   RMSE = 2098.98 mL\n   LLL = -44.4014\n\n======================================================================\nüìÇ FOLD 2/5\n======================================================================\n\nEpoch  Train Loss   R¬≤       MSE        RMSE     MAE      RMAE     LLL     \n----------------------------------------------------------------------\n1      1344.0440    -10.3928 7981856.00 2825.22  2698.39  0.9998   -59.1108\n2      1345.2293    -10.3887 7979005.50 2824.71  2697.86  0.9996   -59.1001\n3      1339.4470    -10.3840 7975672.50 2824.12  2697.26  0.9994   -59.0879\n4      1338.5514    -10.3781 7971584.00 2823.40  2696.53  0.9991   -59.0732\n5      1345.9514    -10.3712 7966729.00 2822.54  2695.62  0.9988   -59.0549\n6      1345.0294    -10.3635 7961312.00 2821.58  2694.62  0.9984   -59.0346\n7      1339.8059    -10.3540 7954649.00 2820.40  2693.38  0.9980   -59.0096\n8      1338.5970    -10.3419 7946198.50 2818.90  2691.91  0.9974   -58.9798\n9      1338.6342    -10.3292 7937339.50 2817.33  2690.31  0.9968   -58.9476\n10     1336.5618    -10.3146 7927103.50 2815.51  2688.50  0.9962   -58.9109\n11     1340.6815    -10.3027 7918709.50 2814.02  2686.96  0.9956   -58.8799\n12     1341.0860    -10.2846 7906046.50 2811.77  2684.79  0.9948   -58.8360\n13     1337.1820    -10.2652 7892461.00 2809.35  2682.41  0.9939   -58.7879\n14     1338.2972    -10.2478 7880311.00 2807.19  2680.22  0.9931   -58.7437\n15     1329.6638    -10.2305 7868125.50 2805.02  2678.02  0.9923   -58.6992\n16     1333.7406    -10.2124 7855452.50 2802.76  2675.64  0.9914   -58.6512\n17     1333.7204    -10.1878 7838259.00 2799.69  2672.62  0.9903   -58.5901\n18     1327.1987    -10.1637 7821357.50 2796.67  2669.57  0.9891   -58.5285\n19     1328.1774    -10.1402 7804878.50 2793.72  2666.48  0.9880   -58.4661\n20     1330.1468    -10.1194 7790294.50 2791.11  2663.94  0.9871   -58.4149\n21     1327.4140    -10.0866 7767334.00 2786.99  2659.87  0.9855   -58.3326\n22     1326.5995    -10.0623 7750286.00 2783.93  2656.74  0.9844   -58.2693\n23     1323.8424    -10.0353 7731418.00 2780.54  2653.14  0.9831   -58.1966\n24     1322.2663    -10.0052 7710313.50 2776.75  2649.31  0.9816   -58.1193\n25     1318.6509    -9.9766  7690286.00 2773.14  2645.56  0.9802   -58.0435\n26     1319.0394    -9.9439  7667337.00 2769.00  2641.17  0.9786   -57.9547\n27     1312.8003    -9.9078  7642058.50 2764.43  2636.65  0.9769   -57.8635\n28     1315.2984    -9.8750  7619102.50 2760.27  2632.40  0.9754   -57.7775\n29     1311.5785    -9.8467  7599254.50 2756.67  2628.65  0.9740   -57.7019\n30     1309.5034    -9.8077  7571926.50 2751.71  2623.35  0.9720   -57.5948\n31     1310.3303    -9.7700  7545501.00 2746.91  2618.36  0.9702   -57.4940\n32     1301.5519    -9.7147  7506764.00 2739.85  2611.33  0.9676   -57.3519\n33     1299.3450    -9.6795  7482110.50 2735.34  2606.57  0.9658   -57.2558\n34     1296.3355    -9.6386  7453476.50 2730.11  2600.45  0.9635   -57.1320\n35     1296.4349    -9.5981  7425082.00 2724.90  2595.23  0.9616   -57.0266\n36     1291.9599    -9.5729  7407444.00 2721.66  2591.91  0.9604   -56.9596\n37     1290.8914    -9.5213  7371298.50 2715.01  2585.17  0.9579   -56.8233\n38     1284.3326    -9.4894  7348958.00 2710.90  2580.69  0.9562   -56.7329\n39     1285.2312    -9.4315  7308395.50 2703.40  2573.60  0.9536   -56.5897\n40     1281.9713    -9.4052  7289937.00 2699.99  2569.78  0.9522   -56.5125\n41     1279.7138    -9.3548  7254649.50 2693.45  2562.68  0.9495   -56.3690\n42     1272.5253    -9.2936  7211781.00 2685.48  2555.02  0.9467   -56.2143\n43     1274.6022    -9.2475  7179431.50 2679.45  2548.87  0.9444   -56.0899\n44     1269.3942    -9.1978  7144656.00 2672.95  2541.81  0.9418   -55.9473\n45     1267.4144    -9.1727  7127038.50 2669.65  2538.19  0.9405   -55.8743\n46     1264.7018    -9.1022  7077666.50 2660.39  2528.93  0.9370   -55.6872\n47     1261.7429    -9.0529  7043115.00 2653.89  2522.43  0.9346   -55.5558\n48     1257.1269    -9.0017  7007278.00 2647.13  2515.43  0.9320   -55.4145\n49     1251.8784    -8.9515  6972104.00 2640.47  2508.38  0.9294   -55.2721\n50     1245.4367    -8.9015  6937061.50 2633.83  2501.81  0.9270   -55.1393\n51     1245.2622    -8.8538  6903652.00 2627.48  2494.84  0.9244   -54.9985\n52     1234.2555    -8.7681  6843613.50 2616.03  2483.66  0.9203   -54.7726\n53     1234.4164    -8.7318  6818128.00 2611.15  2478.59  0.9184   -54.6701\n54     1232.4060    -8.6822  6783381.50 2604.49  2471.02  0.9156   -54.5173\n55     1228.6670    -8.6081  6731497.00 2594.51  2460.46  0.9117   -54.3038\n56     1224.1327    -8.5609  6698439.00 2588.13  2454.65  0.9095   -54.1866\n57     1222.6457    -8.4810  6642468.00 2577.30  2443.89  0.9055   -53.9690\n58     1213.3232    -8.4292  6606125.50 2570.24  2437.04  0.9030   -53.8307\n59     1211.1386    -8.3741  6567556.00 2562.72  2428.58  0.8998   -53.6598\n60     1206.9835    -8.2981  6514307.50 2552.31  2418.25  0.8960   -53.4511\n61     1205.9648    -8.2187  6458711.50 2541.40  2407.27  0.8920   -53.2293\n62     1199.1635    -8.1522  6412075.00 2532.21  2397.39  0.8883   -53.0296\n63     1194.4502    -8.0878  6366980.00 2523.29  2389.03  0.8852   -52.8607\n64     1190.0054    -8.0560  6344693.50 2518.87  2382.91  0.8829   -52.7371\n65     1189.8040    -8.0004  6305704.50 2511.12  2375.23  0.8801   -52.5819\n66     1179.0938    -7.9173  6247548.50 2499.51  2363.46  0.8757   -52.3441\n67     1178.3802    -7.8537  6202930.50 2490.57  2354.25  0.8723   -52.1580\n68     1168.5110    -7.7987  6164459.50 2482.83  2345.94  0.8692   -51.9903\n69     1166.6161    -7.7395  6122965.50 2474.46  2336.99  0.8659   -51.8094\n70     1162.0167    -7.6618  6068485.00 2463.43  2325.71  0.8617   -51.5814\n71     1163.9001    -7.5939  6020943.50 2453.76  2316.03  0.8581   -51.3860\n72     1152.2036    -7.5013  5956040.50 2440.50  2302.11  0.8530   -51.1047\n73     1144.5208    -7.4558  5924213.50 2433.97  2295.26  0.8504   -50.9663\n74     1144.7064    -7.3970  5883026.00 2425.50  2286.65  0.8473   -50.7924\n75     1136.4118    -7.3277  5834406.00 2415.45  2275.80  0.8432   -50.5732\n76     1130.0917    -7.3015  5816072.50 2411.65  2271.51  0.8416   -50.4865\n77     1122.8164    -7.1748  5727295.50 2393.18  2253.96  0.8351   -50.1320\n78     1116.0008    -7.0923  5669546.00 2381.08  2240.81  0.8303   -49.8663\n79     1119.2737    -7.0370  5630796.50 2372.93  2231.76  0.8269   -49.6835\n80     1112.9570    -6.9487  5568944.00 2359.86  2218.59  0.8220   -49.4174\n81     1106.7633    -6.8932  5530022.00 2351.60  2211.15  0.8193   -49.2671\n82     1100.1429    -6.8474  5497952.00 2344.77  2203.32  0.8164   -49.1088\n83     1095.8179    -6.7700  5443715.50 2333.18  2190.74  0.8117   -48.8546\n84     1090.2159    -6.6715  5374717.50 2318.34  2175.98  0.8063   -48.5565\n85     1086.1426    -6.6300  5345607.00 2312.06  2168.79  0.8036   -48.4112\n86     1083.9446    -6.5351  5279172.50 2297.65  2153.48  0.7979   -48.1020\n87     1077.9967    -6.4589  5225738.50 2285.99  2141.01  0.7933   -47.8500\n88     1069.1637    -6.4162  5195855.50 2279.44  2134.14  0.7908   -47.7112\n89     1062.9920    -6.3237  5131053.50 2265.18  2119.87  0.7855   -47.4229\n90     1052.4325    -6.2713  5094287.00 2257.05  2111.06  0.7822   -47.2448\n91     1052.3886    -6.1841  5033218.00 2243.48  2096.96  0.7770   -46.9600\n92     1041.7580    -6.1329  4997365.00 2235.48  2089.04  0.7740   -46.8001\n93     1032.7151    -5.9951  4900817.00 2213.78  2065.44  0.7653   -46.3233\n94     1032.8754    -5.9819  4891584.50 2211.69  2063.58  0.7646   -46.2857\n95     1022.3320    -5.8819  4821491.50 2195.79  2048.62  0.7591   -45.9834\n96     1020.2068    -5.8065  4768667.50 2183.73  2033.93  0.7536   -45.6866\n97     1010.4129    -5.7206  4708494.00 2169.91  2019.05  0.7481   -45.3861\n98     1009.4605    -5.6614  4667009.00 2160.33  2008.73  0.7443   -45.1776\n99     997.5249     -5.5773  4608098.50 2146.65  1994.19  0.7389   -44.8838\n100    993.3870     -5.5300  4574953.50 2138.91  1984.97  0.7355   -44.6976\n\n‚úÖ FOLD 2 FINAL METRICS:\n   R¬≤ = -5.5300\n   RMSE = 2138.91 mL\n   LLL = -44.6976\n\n======================================================================\nüìÇ FOLD 3/5\n======================================================================\n\nEpoch  Train Loss   R¬≤       MSE        RMSE     MAE      RMAE     LLL     \n----------------------------------------------------------------------\n1      1354.7001    -10.5550 7592147.50 2755.39  2633.46  0.9998   -57.7989\n2      1350.2326    -10.5500 7588883.00 2754.79  2632.83  0.9996   -57.7862\n3      1351.3169    -10.5452 7585715.00 2754.22  2632.22  0.9994   -57.7740\n4      1350.7880    -10.5388 7581480.50 2753.45  2631.42  0.9991   -57.7578\n5      1351.2413    -10.5323 7577208.50 2752.67  2630.60  0.9988   -57.7412\n6      1344.9901    -10.5242 7571945.00 2751.72  2629.62  0.9984   -57.7214\n7      1347.7686    -10.5142 7565360.00 2750.52  2628.37  0.9979   -57.6963\n8      1351.0959    -10.5037 7558452.00 2749.26  2627.06  0.9974   -57.6697\n9      1351.0061    -10.4910 7550133.50 2747.75  2625.51  0.9968   -57.6383\n10     1344.6401    -10.4766 7540643.50 2746.02  2623.76  0.9962   -57.6029\n11     1345.4476    -10.4624 7531313.50 2744.32  2621.98  0.9955   -57.5670\n12     1348.7013    -10.4471 7521255.00 2742.49  2620.06  0.9948   -57.5284\n13     1347.8212    -10.4295 7509661.00 2740.38  2617.94  0.9940   -57.4854\n14     1340.7497    -10.4120 7498184.00 2738.28  2615.78  0.9931   -57.4419\n15     1338.5925    -10.3937 7486176.50 2736.09  2613.58  0.9923   -57.3974\n16     1340.4817    -10.3700 7470567.00 2733.23  2610.71  0.9912   -57.3394\n17     1345.6379    -10.3471 7455551.50 2730.49  2607.83  0.9901   -57.2812\n18     1339.5069    -10.3240 7440398.00 2727.71  2605.07  0.9891   -57.2255\n19     1343.3536    -10.3023 7426114.00 2725.09  2602.36  0.9880   -57.1707\n20     1341.3983    -10.2766 7409220.50 2721.99  2599.03  0.9868   -57.1034\n21     1336.1577    -10.2480 7390454.00 2718.54  2595.47  0.9854   -57.0315\n22     1337.3797    -10.2185 7371057.50 2714.97  2591.81  0.9840   -56.9576\n23     1334.1669    -10.1903 7352544.50 2711.56  2588.47  0.9828   -56.8901\n24     1332.7527    -10.1577 7331135.00 2707.61  2584.34  0.9812   -56.8067\n25     1328.5234    -10.1273 7311101.50 2703.90  2580.54  0.9798   -56.7298\n26     1323.3861    -10.0918 7287785.00 2699.59  2576.08  0.9781   -56.6397\n27     1324.9849    -10.0585 7265962.00 2695.54  2572.01  0.9765   -56.5576\n28     1321.2380    -10.0257 7244374.00 2691.54  2567.98  0.9750   -56.4762\n29     1322.1866    -9.9918  7222093.50 2687.40  2563.67  0.9733   -56.3890\n30     1316.2715    -9.9554  7198214.50 2682.95  2559.09  0.9716   -56.2965\n31     1310.4838    -9.9111  7169114.00 2677.52  2553.78  0.9696   -56.1892\n32     1316.3599    -9.8763  7146187.00 2673.24  2549.26  0.9679   -56.0980\n33     1317.5819    -9.8294  7115382.00 2667.47  2543.45  0.9657   -55.9805\n34     1308.2344    -9.7873  7087740.00 2662.28  2538.36  0.9637   -55.8777\n35     1305.0980    -9.7471  7061311.00 2657.31  2533.32  0.9618   -55.7759\n36     1303.5199    -9.7018  7031534.50 2651.70  2528.01  0.9598   -55.6685\n37     1295.0078    -9.6551  7000904.50 2645.92  2522.45  0.9577   -55.5562\n38     1290.8982    -9.6034  6966911.00 2639.49  2516.13  0.9553   -55.4285\n39     1293.9034    -9.5707  6945451.00 2635.42  2511.81  0.9537   -55.3414\n40     1285.8152    -9.5150  6908846.00 2628.47  2504.74  0.9510   -55.1986\n41     1288.0084    -9.4825  6887479.50 2624.40  2500.67  0.9494   -55.1161\n42     1284.6134    -9.4321  6854374.50 2618.09  2494.12  0.9469   -54.9839\n43     1286.9521    -9.3725  6815196.00 2610.59  2486.83  0.9442   -54.8367\n44     1278.6381    -9.3376  6792234.50 2606.19  2481.97  0.9423   -54.7384\n45     1271.6880    -9.2668  6745760.50 2597.26  2473.01  0.9389   -54.5575\n46     1269.2011    -9.2102  6708569.00 2590.09  2465.79  0.9362   -54.4115\n47     1267.5364    -9.1590  6674948.50 2583.59  2459.23  0.9337   -54.2791\n48     1265.1117    -9.1216  6650359.00 2578.83  2453.98  0.9317   -54.1729\n49     1257.7122    -9.0556  6606973.50 2570.40  2445.85  0.9286   -54.0086\n50     1256.1774    -8.9995  6570107.00 2563.22  2438.21  0.9257   -53.8543\n51     1254.2590    -8.9427  6532807.00 2555.94  2430.41  0.9228   -53.6968\n52     1249.7434    -8.8726  6486732.00 2546.91  2421.35  0.9193   -53.5137\n53     1244.4166    -8.8235  6454473.50 2540.57  2414.92  0.9169   -53.3839\n54     1240.4145    -8.7503  6406387.00 2531.08  2405.36  0.9132   -53.1907\n55     1235.9751    -8.7037  6375795.00 2525.03  2398.77  0.9107   -53.0575\n56     1233.2505    -8.6344  6330227.00 2515.99  2389.51  0.9072   -52.8706\n57     1226.5270    -8.5570  6279392.00 2505.87  2379.44  0.9034   -52.6670\n58     1227.7768    -8.5261  6259044.50 2501.81  2374.59  0.9016   -52.5691\n59     1221.9415    -8.4642  6218401.00 2493.67  2366.43  0.8985   -52.4041\n60     1215.8571    -8.3899  6169601.50 2483.87  2356.71  0.8948   -52.2078\n61     1214.3215    -8.3456  6140470.50 2478.00  2349.93  0.8922   -52.0708\n62     1203.1330    -8.2414  6071982.00 2464.14  2336.20  0.8870   -51.7935\n63     1202.5790    -8.1695  6024771.50 2454.54  2326.73  0.8834   -51.6021\n64     1196.4724    -8.1269  5996751.50 2448.83  2320.16  0.8809   -51.4694\n65     1196.9693    -8.0452  5943122.50 2437.85  2308.83  0.8766   -51.2404\n66     1188.2116    -8.0127  5921725.50 2433.46  2304.52  0.8750   -51.1533\n67     1185.0593    -7.9275  5865741.00 2421.93  2291.94  0.8702   -50.8992\n68     1177.0451    -7.8608  5821933.00 2412.87  2282.48  0.8666   -50.7081\n69     1177.2128    -7.7780  5767511.50 2401.56  2271.41  0.8624   -50.4845\n70     1168.0546    -7.6877  5708193.00 2389.18  2259.10  0.8577   -50.2357\n71     1161.6497    -7.6251  5667069.50 2380.56  2250.80  0.8546   -50.0680\n72     1160.9285    -7.5596  5624062.00 2371.51  2241.52  0.8510   -49.8806\n73     1154.3374    -7.4911  5579051.00 2362.00  2230.98  0.8470   -49.6676\n74     1149.2538    -7.4295  5538542.00 2353.41  2221.70  0.8435   -49.4802\n75     1142.7337    -7.3813  5506870.00 2346.67  2215.25  0.8411   -49.3499\n76     1141.1097    -7.3024  5455037.50 2335.60  2203.63  0.8367   -49.1152\n77     1134.2473    -7.2370  5412054.00 2326.38  2194.26  0.8331   -48.9257\n78     1131.3113    -7.1897  5380993.00 2319.70  2186.92  0.8303   -48.7775\n79     1121.0709    -7.0703  5302524.00 2302.72  2170.32  0.8240   -48.4422\n80     1118.2727    -7.0097  5262761.00 2294.07  2160.93  0.8204   -48.2524\n81     1113.1201    -6.9527  5225262.50 2285.88  2152.28  0.8172   -48.0777\n82     1108.1068    -6.8039  5127539.00 2264.41  2130.74  0.8090   -47.6425\n83     1104.9116    -6.7754  5108783.00 2260.26  2125.37  0.8069   -47.5341\n84     1096.9453    -6.7293  5078515.50 2253.56  2118.45  0.8043   -47.3943\n85     1088.8240    -6.6221  5008058.00 2237.87  2102.55  0.7983   -47.0730\n86     1084.7889    -6.5245  4943908.50 2223.49  2086.90  0.7923   -46.7569\n87     1079.7059    -6.4521  4896339.50 2212.77  2075.39  0.7880   -46.5243\n88     1074.4345    -6.3881  4854337.50 2203.26  2066.44  0.7846   -46.3434\n89     1070.1559    -6.3400  4822695.00 2196.06  2058.28  0.7815   -46.1786\n90     1061.1257    -6.2771  4781396.00 2186.64  2049.33  0.7781   -45.9978\n91     1055.8920    -6.2142  4740079.50 2177.17  2039.60  0.7744   -45.8013\n92     1055.4967    -6.1229  4680091.50 2163.35  2025.36  0.7690   -45.5135\n93     1042.6702    -6.0482  4630994.00 2151.97  2012.18  0.7640   -45.2472\n94     1036.6649    -5.9686  4578702.00 2139.79  2000.08  0.7594   -45.0027\n95     1037.5832    -5.8552  4504179.00 2122.31  1981.94  0.7525   -44.6364\n96     1031.6035    -5.8018  4469086.50 2114.02  1972.23  0.7488   -44.4401\n97     1015.8786    -5.7212  4416134.00 2101.46  1960.27  0.7443   -44.1985\n98     1010.5713    -5.6549  4372580.50 2091.07  1948.02  0.7396   -43.9511\n99     1006.2229    -5.6302  4356370.50 2087.19  1944.37  0.7382   -43.8772\n100    996.3005     -5.5240  4286590.00 2070.41  1927.72  0.7319   -43.5410\n\n‚úÖ FOLD 3 FINAL METRICS:\n   R¬≤ = -5.5240\n   RMSE = 2070.41 mL\n   LLL = -43.5410\n\n======================================================================\nüìÇ FOLD 4/5\n======================================================================\n\nEpoch  Train Loss   R¬≤       MSE        RMSE     MAE      RMAE     LLL     \n----------------------------------------------------------------------\n1      1341.3828    -8.7901  8247151.50 2871.79  2721.14  0.9998   -59.5704\n2      1340.9553    -8.7862  8243869.50 2871.21  2720.50  0.9996   -59.5574\n3      1343.5606    -8.7822  8240537.00 2870.63  2719.83  0.9994   -59.5439\n4      1341.0836    -8.7780  8236954.50 2870.01  2719.17  0.9991   -59.5305\n5      1338.0574    -8.7724  8232291.00 2869.20  2718.32  0.9988   -59.5134\n6      1341.4578    -8.7661  8226984.50 2868.27  2717.27  0.9984   -59.4922\n7      1337.5787    -8.7592  8221187.50 2867.26  2716.19  0.9980   -59.4704\n8      1339.1286    -8.7503  8213619.50 2865.94  2714.77  0.9975   -59.4417\n9      1340.9860    -8.7403  8205229.50 2864.48  2713.14  0.9969   -59.4088\n10     1340.5094    -8.7305  8196979.00 2863.04  2711.58  0.9963   -59.3773\n11     1339.6448    -8.7195  8187701.00 2861.42  2709.89  0.9957   -59.3431\n12     1330.7006    -8.7074  8177494.50 2859.63  2707.90  0.9950   -59.3030\n13     1337.3334    -8.6927  8165140.00 2857.47  2705.58  0.9941   -59.2560\n14     1330.6776    -8.6777  8152486.00 2855.26  2703.33  0.9933   -59.2106\n15     1335.1965    -8.6632  8140291.50 2853.12  2700.93  0.9924   -59.1622\n16     1330.1494    -8.6466  8126303.00 2850.67  2698.23  0.9914   -59.1076\n17     1329.3123    -8.6298  8112148.00 2848.18  2695.79  0.9905   -59.0583\n18     1326.6787    -8.6121  8097237.00 2845.56  2692.97  0.9895   -59.0013\n19     1331.1580    -8.5903  8078894.00 2842.34  2689.51  0.9882   -58.9314\n20     1323.3894    -8.5694  8061273.00 2839.24  2686.32  0.9871   -58.8670\n21     1327.8747    -8.5496  8044569.00 2836.29  2683.24  0.9859   -58.8047\n22     1322.0885    -8.5284  8026741.50 2833.15  2679.78  0.9847   -58.7348\n23     1319.0748    -8.5049  8006958.00 2829.66  2676.17  0.9833   -58.6619\n24     1316.0574    -8.4843  7989565.00 2826.58  2672.89  0.9821   -58.5955\n25     1318.1442    -8.4546  7964564.00 2822.16  2668.20  0.9804   -58.5008\n26     1315.7763    -8.4316  7945146.50 2818.71  2664.42  0.9790   -58.4245\n27     1313.0013    -8.4116  7928311.50 2815.73  2661.35  0.9779   -58.3624\n28     1310.5371    -8.3743  7896926.00 2810.15  2655.68  0.9758   -58.2479\n29     1307.4179    -8.3495  7876019.00 2806.42  2651.62  0.9743   -58.1659\n30     1302.8035    -8.3177  7849233.50 2801.65  2646.62  0.9725   -58.0649\n31     1304.6736    -8.2891  7825147.00 2797.35  2642.01  0.9708   -57.9716\n32     1301.0935    -8.2580  7798944.00 2792.66  2636.98  0.9689   -57.8701\n33     1295.3662    -8.2278  7773498.00 2788.10  2632.06  0.9671   -57.7707\n34     1294.4760    -8.1931  7744299.00 2782.86  2626.64  0.9651   -57.6611\n35     1294.5751    -8.1593  7715808.50 2777.73  2621.15  0.9631   -57.5503\n36     1288.1811    -8.1301  7691158.50 2773.29  2616.89  0.9615   -57.4643\n37     1290.5715    -8.0909  7658208.50 2767.35  2610.48  0.9592   -57.3348\n38     1284.9257    -8.0589  7631242.50 2762.47  2605.05  0.9572   -57.2249\n39     1279.0977    -8.0243  7602108.50 2757.19  2599.34  0.9551   -57.1096\n40     1275.7247    -7.9904  7573517.00 2752.00  2594.10  0.9532   -57.0037\n41     1275.6500    -7.9605  7548346.50 2747.43  2588.91  0.9513   -56.8989\n42     1276.2342    -7.9097  7505513.50 2739.62  2580.76  0.9483   -56.7343\n43     1271.8488    -7.8765  7477533.50 2734.51  2575.25  0.9462   -56.6229\n44     1267.1319    -7.8302  7438598.50 2727.38  2567.80  0.9435   -56.4725\n45     1268.5129    -7.7912  7405740.00 2721.35  2561.43  0.9412   -56.3437\n46     1258.4730    -7.7588  7378445.00 2716.33  2556.01  0.9392   -56.2342\n47     1258.4702    -7.7250  7349929.50 2711.08  2550.49  0.9371   -56.1228\n48     1255.2150    -7.6773  7309768.50 2703.66  2542.46  0.9342   -55.9605\n49     1253.7910    -7.6314  7271121.00 2696.50  2535.21  0.9315   -55.8141\n50     1248.5030    -7.5866  7233366.00 2689.49  2527.28  0.9286   -55.6539\n51     1240.1221    -7.5399  7194040.00 2682.17  2519.76  0.9259   -55.5019\n52     1236.6193    -7.4960  7157015.00 2675.26  2512.37  0.9231   -55.3526\n53     1239.8046    -7.4593  7126080.50 2669.47  2505.75  0.9207   -55.2189\n54     1232.7414    -7.4096  7084289.50 2661.63  2497.16  0.9175   -55.0452\n55     1225.5456    -7.3716  7052268.00 2655.61  2491.09  0.9153   -54.9227\n56     1220.1022    -7.3107  7000903.00 2645.92  2480.81  0.9115   -54.7150\n57     1217.9618    -7.2844  6978738.50 2641.73  2475.48  0.9096   -54.6073\n58     1221.6582    -7.2203  6924797.50 2631.50  2465.17  0.9058   -54.3990\n59     1213.1437    -7.1632  6876692.00 2622.34  2455.86  0.9024   -54.2110\n60     1209.1334    -7.1304  6849074.00 2617.07  2449.54  0.9001   -54.0832\n61     1205.3331    -7.0882  6813538.00 2610.28  2442.82  0.8976   -53.9475\n62     1198.9373    -7.0227  6758363.50 2599.69  2431.33  0.8934   -53.7153\n63     1196.5572    -6.9756  6718687.50 2592.04  2422.24  0.8900   -53.5318\n64     1194.2841    -6.9002  6655145.50 2579.76  2409.31  0.8853   -53.2705\n65     1183.4570    -6.8621  6623036.50 2573.53  2403.46  0.8831   -53.1523\n66     1179.7282    -6.8222  6589408.00 2566.98  2395.86  0.8803   -52.9988\n67     1175.4796    -6.7544  6532322.50 2555.84  2383.22  0.8757   -52.7434\n68     1169.5638    -6.6970  6483949.50 2546.36  2373.17  0.8720   -52.5404\n69     1162.2597    -6.6555  6448982.00 2539.48  2365.96  0.8693   -52.3946\n70     1163.4098    -6.5898  6393655.50 2528.57  2354.79  0.8652   -52.1690\n71     1157.3484    -6.5263  6340132.00 2517.96  2342.79  0.8608   -51.9266\n72     1147.5945    -6.4827  6303446.50 2510.67  2334.61  0.8578   -51.7614\n73     1144.9758    -6.4206  6251127.50 2500.23  2324.06  0.8539   -51.5482\n74     1142.5674    -6.3626  6202222.00 2490.43  2313.03  0.8499   -51.3254\n75     1137.0146    -6.2860  6137703.00 2477.44  2299.32  0.8449   -51.0483\n76     1133.5545    -6.2438  6102142.50 2470.25  2290.59  0.8416   -50.8721\n77     1122.5461    -6.1828  6050788.00 2459.83  2279.39  0.8375   -50.6458\n78     1117.4702    -6.1432  6017456.00 2453.05  2271.18  0.8345   -50.4798\n79     1112.4896    -6.0638  5950566.00 2439.38  2257.08  0.8293   -50.1950\n80     1109.8564    -5.9923  5890279.50 2426.99  2242.64  0.8240   -49.9032\n81     1099.3469    -5.9419  5847849.50 2418.23  2234.23  0.8209   -49.7334\n82     1095.3929    -5.8358  5758474.00 2399.68  2215.60  0.8141   -49.3569\n83     1091.3478    -5.8287  5752505.00 2398.44  2214.22  0.8136   -49.3291\n84     1087.4095    -5.7718  5704526.50 2388.42  2202.96  0.8094   -49.1016\n85     1080.1750    -5.7272  5666998.00 2380.55  2194.27  0.8063   -48.9260\n86     1074.9984    -5.6594  5609877.00 2368.52  2180.26  0.8011   -48.6429\n87     1065.3106    -5.6045  5563640.00 2358.74  2168.94  0.7969   -48.4142\n88     1062.6325    -5.5407  5509869.00 2347.31  2156.55  0.7924   -48.1639\n89     1060.1897    -5.5034  5478475.00 2340.61  2148.74  0.7895   -48.0062\n90     1051.3930    -5.4502  5433653.50 2331.02  2138.28  0.7857   -47.7948\n91     1042.8056    -5.3905  5383392.00 2320.21  2125.75  0.7811   -47.5417\n92     1043.8798    -5.3165  5321020.00 2306.73  2110.73  0.7756   -47.2383\n93     1034.6670    -5.2778  5288392.50 2299.65  2103.15  0.7728   -47.0852\n94     1028.5320    -5.1921  5216229.00 2283.91  2085.49  0.7663   -46.7283\n95     1023.1616    -5.1181  5153928.00 2270.23  2069.56  0.7604   -46.4064\n96     1018.9389    -5.1032  5141325.50 2267.45  2065.94  0.7591   -46.3334\n97     1008.2911    -5.0298  5079544.00 2253.78  2051.07  0.7536   -46.0330\n98     997.6163     -4.9499  5012190.00 2238.79  2036.21  0.7482   -45.7328\n99     995.3756     -4.9265  4992511.00 2234.39  2030.26  0.7460   -45.6124\n100    994.7224     -4.8487  4926908.50 2219.66  2012.83  0.7396   -45.2603\n\n‚úÖ FOLD 4 FINAL METRICS:\n   R¬≤ = -4.8487\n   RMSE = 2219.66 mL\n   LLL = -45.2603\n\n======================================================================\nüìÇ FOLD 5/5\n======================================================================\n\nEpoch  Train Loss   R¬≤       MSE        RMSE     MAE      RMAE     LLL     \n----------------------------------------------------------------------\n1      1339.4473    -11.1955 8034456.50 2834.51  2715.81  1.0000   -59.4627\n2      1340.4247    -11.1920 8032140.00 2834.10  2715.38  0.9998   -59.4541\n3      1340.0317    -11.1873 8029075.50 2833.56  2714.81  0.9996   -59.4425\n4      1339.6397    -11.1812 8025031.50 2832.85  2714.07  0.9993   -59.4276\n5      1341.8707    -11.1746 8020704.50 2832.08  2713.26  0.9990   -59.4112\n6      1342.7839    -11.1661 8015059.50 2831.09  2712.20  0.9986   -59.3898\n7      1341.6946    -11.1562 8008575.50 2829.94  2711.01  0.9982   -59.3657\n8      1342.0637    -11.1450 8001165.50 2828.63  2709.63  0.9977   -59.3379\n9      1338.0419    -11.1313 7992144.50 2827.04  2707.99  0.9971   -59.3047\n10     1335.5241    -11.1187 7983860.50 2825.57  2706.47  0.9965   -59.2741\n11     1336.0181    -11.1032 7973667.50 2823.77  2704.58  0.9958   -59.2359\n12     1339.1836    -11.0886 7964013.50 2822.06  2702.77  0.9952   -59.1993\n13     1340.3281    -11.0710 7952403.00 2820.00  2700.64  0.9944   -59.1562\n14     1329.6042    -11.0518 7939772.00 2817.76  2698.31  0.9935   -59.1091\n15     1330.1387    -11.0331 7927432.50 2815.57  2696.04  0.9927   -59.0633\n16     1333.8981    -11.0151 7915585.50 2813.47  2693.79  0.9919   -59.0178\n17     1331.8761    -10.9972 7903793.50 2811.37  2691.54  0.9910   -58.9725\n18     1329.8158    -10.9716 7886949.00 2808.37  2688.47  0.9899   -58.9104\n19     1324.9099    -10.9523 7874257.50 2806.11  2686.09  0.9890   -58.8622\n20     1323.8871    -10.9313 7860409.00 2803.64  2683.46  0.9881   -58.8092\n21     1334.5827    -10.9044 7842702.50 2800.48  2680.14  0.9868   -58.7422\n22     1320.3413    -10.8840 7829245.50 2798.08  2677.44  0.9858   -58.6875\n23     1321.9281    -10.8588 7812650.50 2795.11  2674.14  0.9846   -58.6208\n24     1315.8475    -10.8271 7791770.00 2791.37  2670.04  0.9831   -58.5380\n25     1316.5033    -10.8052 7777303.00 2788.78  2667.13  0.9820   -58.4793\n26     1320.1924    -10.7751 7757462.00 2785.22  2663.37  0.9807   -58.4032\n27     1312.8185    -10.7348 7730955.00 2780.46  2658.31  0.9788   -58.3011\n28     1311.7338    -10.7036 7710370.50 2776.76  2654.25  0.9773   -58.2190\n29     1306.4138    -10.6656 7685320.50 2772.24  2649.76  0.9756   -58.1283\n30     1306.8773    -10.6197 7655124.00 2766.79  2644.15  0.9736   -58.0149\n31     1303.2078    -10.5758 7626163.00 2761.55  2638.62  0.9715   -57.9033\n32     1298.9253    -10.5435 7604910.00 2757.70  2634.69  0.9701   -57.8239\n33     1294.4836    -10.4983 7575159.50 2752.30  2628.85  0.9679   -57.7059\n34     1296.5539    -10.4605 7550238.50 2747.77  2623.90  0.9661   -57.6059\n35     1292.5398    -10.4117 7518062.00 2741.91  2617.68  0.9638   -57.4802\n36     1293.6380    -10.3600 7483995.00 2735.69  2610.77  0.9613   -57.3406\n37     1289.9003    -10.3179 7456274.00 2730.62  2605.85  0.9595   -57.2411\n38     1290.6738    -10.2754 7428318.00 2725.49  2600.37  0.9575   -57.1306\n39     1282.1685    -10.2250 7395101.00 2719.39  2593.92  0.9551   -57.0001\n40     1281.4793    -10.1778 7364013.00 2713.67  2587.73  0.9528   -56.8751\n41     1272.9916    -10.1292 7331992.50 2707.77  2581.71  0.9506   -56.7534\n42     1269.5754    -10.0755 7296581.50 2701.22  2575.00  0.9481   -56.6180\n43     1276.6452    -10.0365 7270877.00 2696.46  2569.08  0.9459   -56.4982\n44     1263.5774    -9.9856  7237362.00 2690.23  2562.76  0.9436   -56.3706\n45     1262.0977    -9.9427  7209080.00 2684.97  2556.85  0.9414   -56.2513\n46     1259.8802    -9.8934  7176621.00 2678.92  2550.40  0.9391   -56.1209\n47     1260.4973    -9.8446  7144503.50 2672.92  2544.07  0.9367   -55.9930\n48     1248.0010    -9.7881  7107218.50 2665.94  2536.86  0.9341   -55.8474\n49     1250.6514    -9.7421  7076958.50 2660.26  2530.01  0.9316   -55.7091\n50     1245.8976    -9.6453  7013154.00 2648.24  2517.59  0.9270   -55.4581\n51     1239.5295    -9.5900  6976722.50 2641.35  2511.30  0.9247   -55.3311\n52     1235.6171    -9.5420  6945143.50 2635.36  2504.32  0.9221   -55.1899\n53     1234.2877    -9.4611  6891824.50 2625.23  2494.23  0.9184   -54.9861\n54     1230.2357    -9.4371  6875997.00 2622.21  2490.16  0.9169   -54.9039\n55     1228.4630    -9.3797  6838189.50 2614.99  2482.49  0.9141   -54.7490\n56     1225.4169    -9.3161  6796271.50 2606.97  2474.00  0.9109   -54.5775\n57     1220.3249    -9.2396  6745928.00 2597.29  2463.92  0.9072   -54.3737\n58     1212.7716    -9.1831  6708659.00 2590.11  2456.84  0.9046   -54.2308\n59     1213.5649    -9.0939  6649918.50 2578.74  2444.77  0.9002   -53.9870\n60     1201.1058    -9.0491  6620402.00 2573.01  2438.82  0.8980   -53.8666\n61     1199.0725    -9.0015  6589050.00 2566.91  2431.66  0.8953   -53.7221\n62     1195.4685    -8.9357  6545706.50 2558.46  2422.65  0.8920   -53.5400\n63     1192.0309    -8.8515  6490212.00 2547.59  2411.66  0.8880   -53.3180\n64     1187.6230    -8.7656  6433599.00 2536.45  2399.82  0.8836   -53.0787\n65     1185.9842    -8.6803  6377415.00 2525.35  2387.36  0.8790   -52.8271\n66     1178.0339    -8.6008  6325042.50 2514.96  2378.22  0.8757   -52.6424\n67     1173.1184    -8.6004  6324809.50 2514.92  2377.22  0.8753   -52.6222\n68     1168.5648    -8.5411  6285712.00 2507.13  2367.93  0.8719   -52.4345\n69     1166.3165    -8.4583  6231180.00 2496.23  2356.98  0.8678   -52.2132\n70     1159.1664    -8.4006  6193146.00 2488.60  2348.60  0.8648   -52.0439\n71     1154.6119    -8.3201  6140106.50 2477.92  2337.00  0.8605   -51.8097\n72     1147.5852    -8.2687  6106269.50 2471.09  2329.95  0.8579   -51.6672\n73     1144.9758    -8.1868  6052319.00 2460.15  2318.25  0.8536   -51.4309\n74     1135.9099    -8.1025  5996748.00 2448.83  2306.30  0.8492   -51.1893\n75     1137.1826    -8.0498  5962060.00 2441.73  2297.40  0.8459   -51.0095\n76     1131.8583    -7.9837  5918515.50 2432.80  2287.53  0.8423   -50.8101\n77     1129.1180    -7.9126  5871689.50 2423.16  2277.23  0.8385   -50.6021\n78     1117.7449    -7.8157  5807802.50 2409.94  2263.28  0.8333   -50.3203\n79     1113.7050    -7.7738  5780190.50 2404.20  2256.97  0.8310   -50.1927\n80     1110.1378    -7.7271  5749433.00 2397.80  2250.43  0.8286   -50.0607\n81     1102.4912    -7.6014  5666621.50 2380.47  2233.06  0.8222   -49.7098\n82     1100.7135    -7.5509  5633373.50 2373.47  2224.22  0.8190   -49.5310\n83     1089.5701    -7.4778  5585211.50 2363.31  2212.72  0.8147   -49.2988\n84     1090.6206    -7.4341  5556456.50 2357.21  2205.73  0.8121   -49.1575\n85     1086.5549    -7.3363  5491970.50 2343.50  2190.67  0.8066   -48.8532\n86     1074.7990    -7.2314  5422888.50 2328.71  2176.06  0.8012   -48.5582\n87     1069.7698    -7.1766  5386796.00 2320.95  2167.01  0.7979   -48.3753\n88     1062.1415    -7.1325  5357741.00 2314.68  2160.28  0.7954   -48.2393\n89     1062.8903    -7.0304  5290436.50 2300.09  2143.31  0.7892   -47.8965\n90     1049.3534    -6.9948  5266992.00 2294.99  2138.16  0.7873   -47.7924\n91     1044.8484    -6.9077  5209640.50 2282.46  2124.50  0.7822   -47.5165\n92     1038.4066    -6.8333  5160641.50 2271.70  2113.05  0.7780   -47.2852\n93     1036.3202    -6.7833  5127657.00 2264.43  2104.04  0.7747   -47.1032\n94     1029.4540    -6.6627  5048244.00 2246.83  2084.85  0.7676   -46.7154\n95     1023.5514    -6.6363  5030820.00 2242.95  2078.96  0.7655   -46.5965\n96     1019.5487    -6.5725  4988789.00 2233.56  2068.69  0.7617   -46.3890\n97     1012.4832    -6.4769  4925842.00 2219.42  2053.77  0.7562   -46.0875\n98     1003.7474    -6.3464  4839845.50 2199.96  2032.34  0.7483   -45.6545\n99     998.5907     -6.3352  4832467.00 2198.29  2029.18  0.7471   -45.5906\n100    989.0246     -6.2140  4752617.00 2180.05  2010.73  0.7404   -45.2179\n\n‚úÖ FOLD 5 FINAL METRICS:\n   R¬≤ = -6.2140\n   RMSE = 2180.05 mL\n   LLL = -45.2179\n\n======================================================================\nüèÜ FINAL ENSEMBLE RESULTS (5-FOLD AVERAGE)\n======================================================================\n\nüìä Out-of-Fold (OOF) Ensemble Performance:\n   R¬≤    = -5.6217\n   MSE   = 4589253.07\n   RMSE  = 2142.25 mL\n   MAE   = 1981.29 mL\n   RMAE  = 0.7364\n   LLL   = -44.6232\n\nüéØ Benchmark Comparison:\n   Target RMSE: < 170 mL  ‚Üí  ‚ùå NEEDS IMPROVEMENT\n   Target LLL:  > -6.64   ‚Üí  ‚ùå NEEDS IMPROVEMENT\n\nüìà Average Across Folds:\n   R2     = -5.6885\n   MSE    = 4589358.5000\n   RMSE   = 2141.6035\n   MAE    = 1981.3119\n   RMAE   = 0.7364\n   LLL    = -44.6236\n\n‚úÖ Training Complete! Models saved as 'best_model_fold{1-5}.pth'\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}